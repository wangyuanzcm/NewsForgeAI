This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.github/
  workflows/
    codeReview.yml
    dockerImageBuild.yml
    latest-changes.yml
    release-drafter.yml
  pull_request_template.md
  release-drafter.yml
app/
  config/
    __init__.py
    config.py
  controllers/
    manager/
      base_manager.py
      memory_manager.py
      redis_manager.py
    v1/
      base.py
      llm.py
      video.py
    v2/
      base.py
      script.py
    base.py
    ping.py
  models/
    const.py
    exception.py
    schema_v2.py
    schema.py
  services/
    audio_merger.py
    llm.py
    material.py
    script_service.py
    state.py
    subtitle.py
    task.py
    video_service.py
    video.py
    voice.py
    youtube_service.py
  test/
    test_gemini.py
    test_moviepy_merge.py
    test_moviepy_speed.py
    test_moviepy.py
    test_qwen.py
  utils/
    check_script.py
    gemini_analyzer.py
    qwenvl_analyzer.py
    script_generator.py
    utils.py
    video_processor_v2.py
    video_processor.py
  asgi.py
  router.py
docker/
  Dockerfile_MiniCPM
docs/
  voice-list.txt
resource/
  fonts/
    fonts_in_here.txt
  public/
    index.html
webui/
  components/
    __init__.py
    audio_settings.py
    basic_settings.py
    merge_settings.py
    review_settings.py
    script_settings.py
    subtitle_settings.py
    system_settings.py
    video_settings.py
  config/
    settings.py
  i18n/
    __init__.py
    en.json
    zh.json
  tools/
    base.py
    generate_script_docu.py
    generate_script_short.py
  utils/
    __init__.py
    cache.py
    file_utils.py
    merge_video.py
    performance.py
    vision_analyzer.py
  __init__.py
.dockerignore
.gitignore
changelog.py
config.example.toml
docker-compose.yml
docker-entrypoint.sh
Dockerfile
LICENSE
main.py
README-cn.md
README-ja.md
README.md
release-notes.md
requirements.txt
video_pipeline.py
webui.py
webui.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/codeReview.yml">
name: Code Review

permissions:
  contents: read
  pull-requests: write

on:
  # åœ¨æåˆå¹¶è¯·æ±‚çš„æ—¶å€™è§¦å‘
  pull_request:
    types: [opened, reopened]
  workflow_dispatch:

jobs:
  codeReview:
    runs-on: ubuntu-latest
    steps:
      - name: GPTä»£ç é€»è¾‘æ£€æŸ¥
        uses: anc95/ChatGPT-CodeReview@main
        env:
          GITHUB_TOKEN: ${{ secrets.GIT_TOKEN }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_API_ENDPOINT: https://api.groq.com/openai/v1
          MODEL: llama-3.1-70b-versatile
          LANGUAGE: Chinese
</file>

<file path=".github/workflows/dockerImageBuild.yml">
name: build_docker

on:
  release:
    types: [created] # è¡¨ç¤ºåœ¨åˆ›å»ºæ–°çš„ Release æ—¶è§¦å‘
  workflow_dispatch:

jobs:
  build_docker:
    name: Build docker
    runs-on: ubuntu-latest
    steps:
      - name: Remove unnecessary files
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf "$AGENT_TOOLSDIRECTORY"
      - name: Checkout
        uses: actions/checkout@v3

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Extract project version
        id: extract_version
        run: |
          project_version=$(grep 'project_version' config.example.toml | cut -d '"' -f 2)
          echo "PROJECT_VERSION=$project_version" >> $GITHUB_ENV

      - name: Build and push
        id: docker_build
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./Dockerfile
          push: true
          platforms: linux/amd64,linux/arm64
          tags: |
            ${{ secrets.DOCKERHUB_USERNAME }}/narratoai:${{ env.PROJECT_VERSION }}
            ${{ secrets.DOCKERHUB_USERNAME }}/narratoai:latest
</file>

<file path=".github/workflows/latest-changes.yml">
name: Latest Changes

on:
  pull_request_target:
    branches:
      - main
    types:
      - closed
  workflow_dispatch:
    inputs:
      number:
        description: PR number
        required: true
      debug_enabled:
        description: "åœ¨å¯ç”¨ tmate è°ƒè¯•çš„æƒ…å†µä¸‹è¿è¡Œæ„å»º (https://github.com/marketplace/actions/debugging-with-tmate)"
        required: false
        default: "false"

jobs:
  latest-changes:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: read
    steps:
      - name: Dump GitHub context
        env:
          GITHUB_CONTEXT: ${{ toJson(github) }}
        run: echo "$GITHUB_CONTEXT"
      - uses: actions/checkout@v4
        with:
          # å…è®¸å°†æœ€æ–°æ›´æ”¹æäº¤åˆ°ä¸»åˆ†æ”¯
          token: ${{ secrets.GIT_TOKEN }}
      - uses: tiangolo/latest-changes@0.3.2
        with:
          token: ${{ secrets.GIT_TOKEN }}
          latest_changes_file: ./release-notes.md
          latest_changes_header: "## Latest Changes"
          end_regex: "^## "
          debug_logs: true
          label_header_prefix: "### "
</file>

<file path=".github/workflows/release-drafter.yml">
name: Release Drafter

on:
  push:
    branches:
      - main
  pull_request:
    types: [opened, reopened, synchronize]

permissions:
  contents: read

jobs:
  update_release_draft:
    permissions:
      contents: write
      pull-requests: write
    runs-on: ubuntu-latest
    steps:
      - uses: release-drafter/release-drafter@v5
        env:
          GITHUB_TOKEN: ${{ secrets.GIT_TOKEN }}
</file>

<file path=".github/pull_request_template.md">
## PR ç±»å‹
è¯·é€‰æ‹©ä¸€ä¸ªé€‚å½“çš„æ ‡ç­¾ï¼ˆå¿…é€‰å…¶ä¸€ï¼‰ï¼š

- [ ] ç ´åæ€§å˜æ›´ (breaking)
- [ ] å®‰å…¨ä¿®å¤ (security)
- [ ] æ–°åŠŸèƒ½ (feature)
- [ ] Bugä¿®å¤ (bug)
- [ ] ä»£ç é‡æ„ (refactor)
- [ ] ä¾èµ–å‡çº§ (upgrade)
- [ ] æ–‡æ¡£æ›´æ–° (docs)
- [ ] ç¿»è¯‘ç›¸å…³ (lang-all)
- [ ] å†…éƒ¨æ”¹è¿› (internal)

## æè¿°
<!-- è¯·æä¾›å¯¹æ­¤æ¬¡æ›´æ”¹çš„æ¸…æ™°æè¿°ã€‚ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªæ›´æ”¹ï¼Ÿå®ƒè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ -->

## ç›¸å…³ Issue
<!-- è¯·é“¾æ¥ç›¸å…³çš„ issueï¼ˆå¦‚æœæœ‰ï¼‰ã€‚ä¾‹å¦‚ï¼šFixes #123 -->

## æ›´æ”¹å†…å®¹
<!-- è¯¦ç»†æè¿°å…·ä½“æ›´æ”¹äº†ä»€ä¹ˆ -->

- xxx
- xxx
- xxx

## æµ‹è¯•
<!-- æè¿°å¦‚ä½•æµ‹è¯•ä½ çš„æ›´æ”¹ -->

- [ ] å•å…ƒæµ‹è¯•
- [ ] é›†æˆæµ‹è¯•
- [ ] æ‰‹åŠ¨æµ‹è¯•

## æˆªå›¾ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
<!-- å¦‚æœæ˜¯UIç›¸å…³çš„æ›´æ”¹ï¼Œè¯·æä¾›æˆªå›¾ -->

## æ£€æŸ¥æ¸…å•

- [ ] æˆ‘çš„ä»£ç éµå¾ªé¡¹ç›®çš„ä»£ç é£æ ¼
- [ ] æˆ‘å·²ç»æ·»åŠ äº†å¿…è¦çš„æµ‹è¯•
- [ ] æˆ‘å·²ç»æ›´æ–°äº†ç›¸å…³æ–‡æ¡£
- [ ] æˆ‘çš„æ›´æ”¹ä¸ä¼šå¼•å…¥æ–°çš„è­¦å‘Š
- [ ] PR æ ‡é¢˜æ¸…æ™°æè¿°äº†æ›´æ”¹å†…å®¹

## è¡¥å……è¯´æ˜
<!-- ä»»ä½•å…¶ä»–ç›¸å…³ä¿¡æ¯ -->
</file>

<file path=".github/release-drafter.yml">
name-template: 'v$RESOLVED_VERSION'
tag-template: 'v$RESOLVED_VERSION'
categories:
  - title: 'ğŸš€ æ–°åŠŸèƒ½'
    labels:
      - 'feature'
      - 'enhancement'
  - title: 'ğŸ› Bug ä¿®å¤'
    labels:
      - 'fix'
      - 'bug'
  - title: 'ğŸ§° ç»´æŠ¤'
    labels:
      - 'chore'
      - 'maintenance'
  - title: 'ğŸ“š æ–‡æ¡£'
    labels:
      - 'docs'
      - 'documentation'

change-template: '- $TITLE @$AUTHOR (#$NUMBER)'

version-resolver:
  major:
    labels:
      - 'major'
      - 'breaking'
  minor:
    labels:
      - 'minor'
      - 'feature'
  patch:
    labels:
      - 'patch'
      - 'fix'
      - 'bug'
      - 'maintenance'
  default: patch

template: |
  ## æ›´æ–°å†…å®¹

  $CHANGES

  ## è´¡çŒ®è€…

  $CONTRIBUTORS
</file>

<file path="app/config/__init__.py">
import os
import sys

from loguru import logger

from app.config import config
from app.utils import utils


def __init_logger():
    # _log_file = utils.storage_dir("logs/server.log")
    _lvl = config.log_level
    root_dir = os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
    )

    def format_record(record):
        # è·å–æ—¥å¿—è®°å½•ä¸­çš„æ–‡ä»¶å…¨è·¯å¾„
        file_path = record["file"].path
        # å°†ç»å¯¹è·¯å¾„è½¬æ¢ä¸ºç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
        relative_path = os.path.relpath(file_path, root_dir)
        # æ›´æ–°è®°å½•ä¸­çš„æ–‡ä»¶è·¯å¾„
        record["file"].path = f"./{relative_path}"
        # è¿”å›ä¿®æ”¹åçš„æ ¼å¼å­—ç¬¦ä¸²
        # æ‚¨å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿™é‡Œçš„æ ¼å¼
        _format = (
            "<green>{time:%Y-%m-%d %H:%M:%S}</> | "
            + "<level>{level}</> | "
            + '"{file.path}:{line}":<blue> {function}</> '
            + "- <level>{message}</>"
            + "\n"
        )
        return _format

    logger.remove()

    logger.add(
        sys.stdout,
        level=_lvl,
        format=format_record,
        colorize=True,
    )

    # logger.add(
    #     _log_file,
    #     level=_lvl,
    #     format=format_record,
    #     rotation="00:00",
    #     retention="3 days",
    #     backtrace=True,
    #     diagnose=True,
    #     enqueue=True,
    # )


__init_logger()
</file>

<file path="app/config/config.py">
import os
import socket
import toml
import shutil
from loguru import logger

root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
config_file = f"{root_dir}/config.toml"


def load_config():
    # fix: IsADirectoryError: [Errno 21] Is a directory: '/NarratoAI/config.toml'
    if os.path.isdir(config_file):
        shutil.rmtree(config_file)

    if not os.path.isfile(config_file):
        example_file = f"{root_dir}/config.example.toml"
        if os.path.isfile(example_file):
            shutil.copyfile(example_file, config_file)
            logger.info(f"copy config.example.toml to config.toml")

    logger.info(f"load config from file: {config_file}")

    try:
        _config_ = toml.load(config_file)
    except Exception as e:
        logger.warning(f"load config failed: {str(e)}, try to load as utf-8-sig")
        with open(config_file, mode="r", encoding="utf-8-sig") as fp:
            _cfg_content = fp.read()
            _config_ = toml.loads(_cfg_content)
    return _config_


def save_config():
    with open(config_file, "w", encoding="utf-8") as f:
        _cfg["app"] = app
        _cfg["azure"] = azure
        _cfg["ui"] = ui
        f.write(toml.dumps(_cfg))


_cfg = load_config()
app = _cfg.get("app", {})
whisper = _cfg.get("whisper", {})
proxy = _cfg.get("proxy", {})
azure = _cfg.get("azure", {})
ui = _cfg.get("ui", {})
frames = _cfg.get("frames", {})

hostname = socket.gethostname()

log_level = _cfg.get("log_level", "DEBUG")
listen_host = _cfg.get("listen_host", "0.0.0.0")
listen_port = _cfg.get("listen_port", 8080)
project_name = _cfg.get("project_name", "NarratoAI")
project_description = _cfg.get(
    "project_description",
    "<a href='https://github.com/linyqh/NarratoAI'>https://github.com/linyqh/NarratoAI</a>",
)
project_version = _cfg.get("app", {}).get("project_version")
reload_debug = False

imagemagick_path = app.get("imagemagick_path", "")
if imagemagick_path and os.path.isfile(imagemagick_path):
    os.environ["IMAGEMAGICK_BINARY"] = imagemagick_path

ffmpeg_path = app.get("ffmpeg_path", "")
if ffmpeg_path and os.path.isfile(ffmpeg_path):
    os.environ["IMAGEIO_FFMPEG_EXE"] = ffmpeg_path

logger.info(f"{project_name} v{project_version}")
</file>

<file path="app/controllers/manager/base_manager.py">
import threading
from typing import Callable, Any, Dict


class TaskManager:
    def __init__(self, max_concurrent_tasks: int):
        self.max_concurrent_tasks = max_concurrent_tasks
        self.current_tasks = 0
        self.lock = threading.Lock()
        self.queue = self.create_queue()

    def create_queue(self):
        raise NotImplementedError()

    def add_task(self, func: Callable, *args: Any, **kwargs: Any):
        with self.lock:
            if self.current_tasks < self.max_concurrent_tasks:
                print(f"add task: {func.__name__}, current_tasks: {self.current_tasks}")
                self.execute_task(func, *args, **kwargs)
            else:
                print(
                    f"enqueue task: {func.__name__}, current_tasks: {self.current_tasks}"
                )
                self.enqueue({"func": func, "args": args, "kwargs": kwargs})

    def execute_task(self, func: Callable, *args: Any, **kwargs: Any):
        thread = threading.Thread(
            target=self.run_task, args=(func, *args), kwargs=kwargs
        )
        thread.start()

    def run_task(self, func: Callable, *args: Any, **kwargs: Any):
        try:
            with self.lock:
                self.current_tasks += 1
            func(*args, **kwargs)  # åœ¨è¿™é‡Œè°ƒç”¨å‡½æ•°ï¼Œä¼ é€’*argså’Œ**kwargs
        finally:
            self.task_done()

    def check_queue(self):
        with self.lock:
            if (
                self.current_tasks < self.max_concurrent_tasks
                and not self.is_queue_empty()
            ):
                task_info = self.dequeue()
                func = task_info["func"]
                args = task_info.get("args", ())
                kwargs = task_info.get("kwargs", {})
                self.execute_task(func, *args, **kwargs)

    def task_done(self):
        with self.lock:
            self.current_tasks -= 1
        self.check_queue()

    def enqueue(self, task: Dict):
        raise NotImplementedError()

    def dequeue(self):
        raise NotImplementedError()

    def is_queue_empty(self):
        raise NotImplementedError()
</file>

<file path="app/controllers/manager/memory_manager.py">
from queue import Queue
from typing import Dict

from app.controllers.manager.base_manager import TaskManager


class InMemoryTaskManager(TaskManager):
    def create_queue(self):
        return Queue()

    def enqueue(self, task: Dict):
        self.queue.put(task)

    def dequeue(self):
        return self.queue.get()

    def is_queue_empty(self):
        return self.queue.empty()
</file>

<file path="app/controllers/manager/redis_manager.py">
import json
from typing import Dict

import redis

from app.controllers.manager.base_manager import TaskManager
from app.models.schema import VideoParams
from app.services import task as tm

FUNC_MAP = {
    "start": tm.start,
    # 'start_test': tm.start_test
}


class RedisTaskManager(TaskManager):
    def __init__(self, max_concurrent_tasks: int, redis_url: str):
        self.redis_client = redis.Redis.from_url(redis_url)
        super().__init__(max_concurrent_tasks)

    def create_queue(self):
        return "task_queue"

    def enqueue(self, task: Dict):
        task_with_serializable_params = task.copy()

        if "params" in task["kwargs"] and isinstance(
            task["kwargs"]["params"], VideoParams
        ):
            task_with_serializable_params["kwargs"]["params"] = task["kwargs"][
                "params"
            ].dict()

        # å°†å‡½æ•°å¯¹è±¡è½¬æ¢ä¸ºå…¶åç§°
        task_with_serializable_params["func"] = task["func"].__name__
        self.redis_client.rpush(self.queue, json.dumps(task_with_serializable_params))

    def dequeue(self):
        task_json = self.redis_client.lpop(self.queue)
        if task_json:
            task_info = json.loads(task_json)
            # å°†å‡½æ•°åç§°è½¬æ¢å›å‡½æ•°å¯¹è±¡
            task_info["func"] = FUNC_MAP[task_info["func"]]

            if "params" in task_info["kwargs"] and isinstance(
                task_info["kwargs"]["params"], dict
            ):
                task_info["kwargs"]["params"] = VideoParams(
                    **task_info["kwargs"]["params"]
                )

            return task_info
        return None

    def is_queue_empty(self):
        return self.redis_client.llen(self.queue) == 0
</file>

<file path="app/controllers/v1/base.py">
from fastapi import APIRouter, Depends


def new_router(dependencies=None):
    router = APIRouter()
    router.tags = ["V1"]
    router.prefix = "/api/v1"
    # å°†è®¤è¯ä¾èµ–é¡¹åº”ç”¨äºæ‰€æœ‰è·¯ç”±
    if dependencies:
        router.dependencies = dependencies
    return router
</file>

<file path="app/controllers/v1/llm.py">
from fastapi import Request, File, UploadFile
import os
from app.controllers.v1.base import new_router
from app.models.schema import (
    VideoScriptResponse,
    VideoScriptRequest,
    VideoTermsResponse,
    VideoTermsRequest,
    VideoTranscriptionRequest,
    VideoTranscriptionResponse,
)
from app.services import llm
from app.utils import utils
from app.config import config

# è®¤è¯ä¾èµ–é¡¹
# router = new_router(dependencies=[Depends(base.verify_token)])
router = new_router()

# å®šä¹‰ä¸Šä¼ ç›®å½•
UPLOAD_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "uploads")

@router.post(
    "/scripts",
    response_model=VideoScriptResponse,
    summary="Create a script for the video",
)
def generate_video_script(request: Request, body: VideoScriptRequest):
    video_script = llm.generate_script(
        video_subject=body.video_subject,
        language=body.video_language,
        paragraph_number=body.paragraph_number,
    )
    response = {"video_script": video_script}
    return utils.get_response(200, response)


@router.post(
    "/terms",
    response_model=VideoTermsResponse,
    summary="Generate video terms based on the video script",
)
def generate_video_terms(request: Request, body: VideoTermsRequest):
    video_terms = llm.generate_terms(
        video_subject=body.video_subject,
        video_script=body.video_script,
        amount=body.amount,
    )
    response = {"video_terms": video_terms}
    return utils.get_response(200, response)


@router.post(
    "/transcription",
    response_model=VideoTranscriptionResponse, 
    summary="Transcribe video content using Gemini"
)
async def transcribe_video(
    request: Request,
    video_name: str,
    language: str = "zh-CN",
    video_file: UploadFile = File(...)
):
    """
    ä½¿ç”¨ Gemini è½¬å½•è§†é¢‘å†…å®¹,åŒ…æ‹¬æ—¶é—´æˆ³ã€ç”»é¢æè¿°å’Œè¯­éŸ³å†…å®¹
    
    Args:
        video_name: è§†é¢‘åç§°
        language: è¯­è¨€ä»£ç ,é»˜è®¤zh-CN
        video_file: ä¸Šä¼ çš„è§†é¢‘æ–‡ä»¶
    """
    # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºå­˜å‚¨ä¸Šä¼ çš„è§†é¢‘
    os.makedirs(UPLOAD_DIR, exist_ok=True)
    
    # ä¿å­˜ä¸Šä¼ çš„è§†é¢‘æ–‡ä»¶
    video_path = os.path.join(UPLOAD_DIR, video_file.filename)
    with open(video_path, "wb") as buffer:
        content = await video_file.read()
        buffer.write(content)
    
    try:
        transcription = llm.gemini_video_transcription(
            video_name=video_name,
            video_path=video_path,
            language=language,
            llm_provider_video=config.app.get("video_llm_provider", "gemini")
        )
        response = {"transcription": transcription}
        return utils.get_response(200, response)
    finally:
        # å¤„ç†å®Œæˆååˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(video_path):
            os.remove(video_path)
</file>

<file path="app/controllers/v1/video.py">
import glob
import os
import pathlib
import shutil
from typing import Union

from fastapi import BackgroundTasks, Depends, Path, Request, UploadFile
from fastapi.params import File
from fastapi.responses import FileResponse, StreamingResponse
from loguru import logger

from app.config import config
from app.controllers import base
from app.controllers.manager.memory_manager import InMemoryTaskManager
from app.controllers.manager.redis_manager import RedisTaskManager
from app.controllers.v1.base import new_router
from app.models.exception import HttpException
from app.models.schema import (
    AudioRequest,
    BgmRetrieveResponse,
    BgmUploadResponse,
    SubtitleRequest,
    TaskDeletionResponse,
    TaskQueryRequest,
    TaskQueryResponse,
    TaskResponse,
    TaskVideoRequest,
)
from app.services import state as sm
from app.services import task as tm
from app.utils import utils

# è®¤è¯ä¾èµ–é¡¹
# router = new_router(dependencies=[Depends(base.verify_token)])
router = new_router()

_enable_redis = config.app.get("enable_redis", False)
_redis_host = config.app.get("redis_host", "localhost")
_redis_port = config.app.get("redis_port", 6379)
_redis_db = config.app.get("redis_db", 0)
_redis_password = config.app.get("redis_password", None)
_max_concurrent_tasks = config.app.get("max_concurrent_tasks", 5)

redis_url = f"redis://:{_redis_password}@{_redis_host}:{_redis_port}/{_redis_db}"
# æ ¹æ®é…ç½®é€‰æ‹©åˆé€‚çš„ä»»åŠ¡ç®¡ç†å™¨
if _enable_redis:
    task_manager = RedisTaskManager(
        max_concurrent_tasks=_max_concurrent_tasks, redis_url=redis_url
    )
else:
    task_manager = InMemoryTaskManager(max_concurrent_tasks=_max_concurrent_tasks)


@router.post("/videos", response_model=TaskResponse, summary="Generate a short video")
def create_video(
    background_tasks: BackgroundTasks, request: Request, body: TaskVideoRequest
):
    return create_task(request, body, stop_at="video")


@router.post("/subtitle", response_model=TaskResponse, summary="Generate subtitle only")
def create_subtitle(
    background_tasks: BackgroundTasks, request: Request, body: SubtitleRequest
):
    return create_task(request, body, stop_at="subtitle")


@router.post("/audio", response_model=TaskResponse, summary="Generate audio only")
def create_audio(
    background_tasks: BackgroundTasks, request: Request, body: AudioRequest
):
    return create_task(request, body, stop_at="audio")


def create_task(
    request: Request,
    body: Union[TaskVideoRequest, SubtitleRequest, AudioRequest],
    stop_at: str,
):
    task_id = utils.get_uuid()
    request_id = base.get_task_id(request)
    try:
        task = {
            "task_id": task_id,
            "request_id": request_id,
            "params": body.model_dump(),
        }
        sm.state.update_task(task_id)
        task_manager.add_task(tm.start, task_id=task_id, params=body, stop_at=stop_at)
        logger.success(f"Task created: {utils.to_json(task)}")
        return utils.get_response(200, task)
    except ValueError as e:
        raise HttpException(
            task_id=task_id, status_code=400, message=f"{request_id}: {str(e)}"
        )


@router.get(
    "/tasks/{task_id}", response_model=TaskQueryResponse, summary="Query task status"
)
def get_task(
    request: Request,
    task_id: str = Path(..., description="Task ID"),
    query: TaskQueryRequest = Depends(),
):
    endpoint = config.app.get("endpoint", "")
    if not endpoint:
        endpoint = str(request.base_url)
    endpoint = endpoint.rstrip("/")

    request_id = base.get_task_id(request)
    task = sm.state.get_task(task_id)
    if task:
        task_dir = utils.task_dir()

        def file_to_uri(file):
            if not file.startswith(endpoint):
                _uri_path = v.replace(task_dir, "tasks").replace("\\", "/")
                _uri_path = f"{endpoint}/{_uri_path}"
            else:
                _uri_path = file
            return _uri_path

        if "videos" in task:
            videos = task["videos"]
            urls = []
            for v in videos:
                urls.append(file_to_uri(v))
            task["videos"] = urls
        if "combined_videos" in task:
            combined_videos = task["combined_videos"]
            urls = []
            for v in combined_videos:
                urls.append(file_to_uri(v))
            task["combined_videos"] = urls
        return utils.get_response(200, task)

    raise HttpException(
        task_id=task_id, status_code=404, message=f"{request_id}: task not found"
    )


@router.delete(
    "/tasks/{task_id}",
    response_model=TaskDeletionResponse,
    summary="Delete a generated short video task",
)
def delete_video(request: Request, task_id: str = Path(..., description="Task ID")):
    request_id = base.get_task_id(request)
    task = sm.state.get_task(task_id)
    if task:
        tasks_dir = utils.task_dir()
        current_task_dir = os.path.join(tasks_dir, task_id)
        if os.path.exists(current_task_dir):
            shutil.rmtree(current_task_dir)

        sm.state.delete_task(task_id)
        logger.success(f"video deleted: {utils.to_json(task)}")
        return utils.get_response(200)

    raise HttpException(
        task_id=task_id, status_code=404, message=f"{request_id}: task not found"
    )


# @router.get(
#     "/musics", response_model=BgmRetrieveResponse, summary="Retrieve local BGM files"
# )
# def get_bgm_list(request: Request):
#     suffix = "*.mp3"
#     song_dir = utils.song_dir()
#     files = glob.glob(os.path.join(song_dir, suffix))
#     bgm_list = []
#     for file in files:
#         bgm_list.append(
#             {
#                 "name": os.path.basename(file),
#                 "size": os.path.getsize(file),
#                 "file": file,
#             }
#         )
#     response = {"files": bgm_list}
#     return utils.get_response(200, response)
#

# @router.post(
#     "/musics",
#     response_model=BgmUploadResponse,
#     summary="Upload the BGM file to the songs directory",
# )
# def upload_bgm_file(request: Request, file: UploadFile = File(...)):
#     request_id = base.get_task_id(request)
#     # check file ext
#     if file.filename.endswith("mp3"):
#         song_dir = utils.song_dir()
#         save_path = os.path.join(song_dir, file.filename)
#         # save file
#         with open(save_path, "wb+") as buffer:
#             # If the file already exists, it will be overwritten
#             file.file.seek(0)
#             buffer.write(file.file.read())
#         response = {"file": save_path}
#         return utils.get_response(200, response)
#
#     raise HttpException(
#         "", status_code=400, message=f"{request_id}: Only *.mp3 files can be uploaded"
#     )
#
#
# @router.get("/stream/{file_path:path}")
# async def stream_video(request: Request, file_path: str):
#     tasks_dir = utils.task_dir()
#     video_path = os.path.join(tasks_dir, file_path)
#     range_header = request.headers.get("Range")
#     video_size = os.path.getsize(video_path)
#     start, end = 0, video_size - 1
#
#     length = video_size
#     if range_header:
#         range_ = range_header.split("bytes=")[1]
#         start, end = [int(part) if part else None for part in range_.split("-")]
#         if start is None:
#             start = video_size - end
#             end = video_size - 1
#         if end is None:
#             end = video_size - 1
#         length = end - start + 1
#
#     def file_iterator(file_path, offset=0, bytes_to_read=None):
#         with open(file_path, "rb") as f:
#             f.seek(offset, os.SEEK_SET)
#             remaining = bytes_to_read or video_size
#             while remaining > 0:
#                 bytes_to_read = min(4096, remaining)
#                 data = f.read(bytes_to_read)
#                 if not data:
#                     break
#                 remaining -= len(data)
#                 yield data
#
#     response = StreamingResponse(
#         file_iterator(video_path, start, length), media_type="video/mp4"
#     )
#     response.headers["Content-Range"] = f"bytes {start}-{end}/{video_size}"
#     response.headers["Accept-Ranges"] = "bytes"
#     response.headers["Content-Length"] = str(length)
#     response.status_code = 206  # Partial Content
#
#     return response
#
#
# @router.get("/download/{file_path:path}")
# async def download_video(_: Request, file_path: str):
#     """
#     download video
#     :param _: Request request
#     :param file_path: video file path, eg: /cd1727ed-3473-42a2-a7da-4faafafec72b/final-1.mp4
#     :return: video file
#     """
#     tasks_dir = utils.task_dir()
#     video_path = os.path.join(tasks_dir, file_path)
#     file_path = pathlib.Path(video_path)
#     filename = file_path.stem
#     extension = file_path.suffix
#     headers = {"Content-Disposition": f"attachment; filename={filename}{extension}"}
#     return FileResponse(
#         path=video_path,
#         headers=headers,
#         filename=f"{filename}{extension}",
#         media_type=f"video/{extension[1:]}",
#     )
</file>

<file path="app/controllers/v2/base.py">
from fastapi import APIRouter, Depends


def v2_router(dependencies=None):
    router = APIRouter()
    router.tags = ["V2"]
    router.prefix = "/api/v2"
    # å°†è®¤è¯ä¾èµ–é¡¹åº”ç”¨äºæ‰€æœ‰è·¯ç”±
    if dependencies:
        router.dependencies = dependencies
    return router
</file>

<file path="app/controllers/v2/script.py">
from fastapi import APIRouter, BackgroundTasks
from loguru import logger
import os

from app.models.schema_v2 import (
    GenerateScriptRequest, 
    GenerateScriptResponse,
    CropVideoRequest,
    CropVideoResponse,
    DownloadVideoRequest,
    DownloadVideoResponse,
    StartSubclipRequest,
    StartSubclipResponse
)
from app.models.schema import VideoClipParams
from app.services.script_service import ScriptGenerator
from app.services.video_service import VideoService
from app.utils import utils
from app.controllers.v2.base import v2_router
from app.models.schema import VideoClipParams
from app.services.youtube_service import YoutubeService
from app.services import task as task_service

router = v2_router()


@router.post(
    "/scripts/generate",
    response_model=GenerateScriptResponse,
    summary="åŒæ­¥è¯·æ±‚ï¼›ç”Ÿæˆè§†é¢‘è„šæœ¬ (V2)"
)
async def generate_script(
    request: GenerateScriptRequest,
    background_tasks: BackgroundTasks
):
    """
    ç”Ÿæˆè§†é¢‘è„šæœ¬çš„V2ç‰ˆæœ¬API
    """
    task_id = utils.get_uuid()
    
    try:
        generator = ScriptGenerator()
        script = await generator.generate_script(
            video_path=request.video_path,
            video_theme=request.video_theme,
            custom_prompt=request.custom_prompt,
            skip_seconds=request.skip_seconds,
            threshold=request.threshold,
            vision_batch_size=request.vision_batch_size,
            vision_llm_provider=request.vision_llm_provider
        )
        
        return {
            "task_id": task_id,
            "script": script
        }
        
    except Exception as e:
        logger.exception(f"Generate script failed: {str(e)}")
        raise


@router.post(
    "/scripts/crop",
    response_model=CropVideoResponse,
    summary="åŒæ­¥è¯·æ±‚ï¼›è£å‰ªè§†é¢‘ (V2)"
)
async def crop_video(
    request: CropVideoRequest,
    background_tasks: BackgroundTasks
):
    """
    æ ¹æ®è„šæœ¬è£å‰ªè§†é¢‘çš„V2ç‰ˆæœ¬API
    """
    try:
        # è°ƒç”¨è§†é¢‘è£å‰ªæœåŠ¡
        video_service = VideoService()
        task_id, subclip_videos = await video_service.crop_video(
            video_path=request.video_origin_path,
            video_script=request.video_script
        )
        logger.debug(f"è£å‰ªè§†é¢‘æˆåŠŸï¼Œè§†é¢‘ç‰‡æ®µè·¯å¾„: {subclip_videos}")
        logger.debug(type(subclip_videos))
        return {
            "task_id": task_id,
            "subclip_videos": subclip_videos
        }
        
    except Exception as e:
        logger.exception(f"Crop video failed: {str(e)}")
        raise


@router.post(
    "/youtube/download",
    response_model=DownloadVideoResponse,
    summary="åŒæ­¥è¯·æ±‚ï¼›ä¸‹è½½YouTubeè§†é¢‘ (V2)"
)
async def download_youtube_video(
    request: DownloadVideoRequest,
    background_tasks: BackgroundTasks
):
    """
    ä¸‹è½½æŒ‡å®šåˆ†è¾¨ç‡çš„YouTubeè§†é¢‘
    """
    try:
        youtube_service = YoutubeService()
        task_id, output_path, filename = await youtube_service.download_video(
            url=request.url,
            resolution=request.resolution,
            output_format=request.output_format,
            rename=request.rename
        )
        
        return {
            "task_id": task_id,
            "output_path": output_path,
            "resolution": request.resolution,
            "format": request.output_format,
            "filename": filename
        }
        
    except Exception as e:
        logger.exception(f"Download YouTube video failed: {str(e)}")
        raise


@router.post(
    "/scripts/start-subclip",
    response_model=StartSubclipResponse,
    summary="å¼‚æ­¥è¯·æ±‚ï¼›å¼€å§‹è§†é¢‘å‰ªè¾‘ä»»åŠ¡ (V2)"
)
async def start_subclip(
    request: VideoClipParams,
    task_id: str,
    subclip_videos: dict,
    background_tasks: BackgroundTasks
):
    """
    å¼€å§‹è§†é¢‘å‰ªè¾‘ä»»åŠ¡çš„V2ç‰ˆæœ¬API
    """
    try:
        # æ„å»ºå‚æ•°å¯¹è±¡
        params = VideoClipParams(
            video_origin_path=request.video_origin_path,
            video_clip_json_path=request.video_clip_json_path,
            voice_name=request.voice_name,
            voice_rate=request.voice_rate,
            voice_pitch=request.voice_pitch,
            subtitle_enabled=request.subtitle_enabled,
            video_aspect=request.video_aspect,
            n_threads=request.n_threads
        )
        
        # åœ¨åå°ä»»åŠ¡ä¸­æ‰§è¡Œè§†é¢‘å‰ªè¾‘
        background_tasks.add_task(
            task_service.start_subclip,
            task_id=task_id,
            params=params,
            subclip_path_videos=subclip_videos
        )
        
        return {
            "task_id": task_id,
            "state": "PROCESSING"  # åˆå§‹çŠ¶æ€
        }
        
    except Exception as e:
        logger.exception(f"Start subclip task failed: {str(e)}")
        raise
</file>

<file path="app/controllers/base.py">
from uuid import uuid4

from fastapi import Request

from app.config import config
from app.models.exception import HttpException


def get_task_id(request: Request):
    task_id = request.headers.get("x-task-id")
    if not task_id:
        task_id = uuid4()
    return str(task_id)


def get_api_key(request: Request):
    api_key = request.headers.get("x-api-key")
    return api_key


def verify_token(request: Request):
    token = get_api_key(request)
    if token != config.app.get("api_key", ""):
        request_id = get_task_id(request)
        request_url = request.url
        user_agent = request.headers.get("user-agent")
        raise HttpException(
            task_id=request_id,
            status_code=401,
            message=f"invalid token: {request_url}, {user_agent}",
        )
</file>

<file path="app/controllers/ping.py">
from fastapi import APIRouter
from fastapi import Request

router = APIRouter()


@router.get(
    "/ping",
    tags=["Health Check"],
    description="æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§",
    response_description="pong",
)
def ping(request: Request) -> str:
    return "pong"
</file>

<file path="app/models/const.py">
PUNCTUATIONS = [
    "?",
    ",",
    ".",
    "ã€",
    ";",
    ":",
    "!",
    "â€¦",
    "ï¼Ÿ",
    "ï¼Œ",
    "ã€‚",
    "ã€",
    "ï¼›",
    "ï¼š",
    "ï¼",
    "...",
]

TASK_STATE_FAILED = -1
TASK_STATE_COMPLETE = 1
TASK_STATE_PROCESSING = 4

FILE_TYPE_VIDEOS = ["mp4", "mov", "mkv", "webm"]
FILE_TYPE_IMAGES = ["jpg", "jpeg", "png", "bmp"]
</file>

<file path="app/models/exception.py">
import traceback
from typing import Any

from loguru import logger


class HttpException(Exception):
    def __init__(
        self, task_id: str, status_code: int, message: str = "", data: Any = None
    ):
        self.message = message
        self.status_code = status_code
        self.data = data
        # è·å–å¼‚å¸¸å †æ ˆä¿¡æ¯
        tb_str = traceback.format_exc().strip()
        if not tb_str or tb_str == "NoneType: None":
            msg = f"HttpException: {status_code}, {task_id}, {message}"
        else:
            msg = f"HttpException: {status_code}, {task_id}, {message}\n{tb_str}"

        if status_code == 400:
            logger.warning(msg)
        else:
            logger.error(msg)


class FileNotFoundException(Exception):
    pass
</file>

<file path="app/models/schema_v2.py">
from typing import Optional, List
from pydantic import BaseModel


class GenerateScriptRequest(BaseModel):
    video_path: str
    video_theme: Optional[str] = ""
    custom_prompt: Optional[str] = ""
    skip_seconds: Optional[int] = 0
    threshold: Optional[int] = 30
    vision_batch_size: Optional[int] = 5
    vision_llm_provider: Optional[str] = "gemini"


class GenerateScriptResponse(BaseModel):
    task_id: str
    script: List[dict]


class CropVideoRequest(BaseModel):
    video_origin_path: str
    video_script: List[dict]


class CropVideoResponse(BaseModel):
    task_id: str
    subclip_videos: dict


class DownloadVideoRequest(BaseModel):
    url: str
    resolution: str
    output_format: Optional[str] = "mp4"
    rename: Optional[str] = None


class DownloadVideoResponse(BaseModel):
    task_id: str
    output_path: str
    resolution: str
    format: str
    filename: str


class StartSubclipRequest(BaseModel):
    task_id: str
    video_origin_path: str
    video_clip_json_path: str
    voice_name: Optional[str] = None
    voice_rate: Optional[int] = 0
    voice_pitch: Optional[int] = 0
    subtitle_enabled: Optional[bool] = True
    video_aspect: Optional[str] = "16:9"
    n_threads: Optional[int] = 4
    subclip_videos: list  # ä»è£å‰ªè§†é¢‘æ¥å£è·å–çš„è§†é¢‘ç‰‡æ®µå­—å…¸


class StartSubclipResponse(BaseModel):
    task_id: str
    state: str
    videos: Optional[List[str]] = None
    combined_videos: Optional[List[str]] = None
</file>

<file path="app/models/schema.py">
import warnings
from enum import Enum
from typing import Any, List, Optional

import pydantic
from pydantic import BaseModel, Field

# å¿½ç•¥ Pydantic çš„ç‰¹å®šè­¦å‘Š
warnings.filterwarnings(
    "ignore",
    category=UserWarning,
    message="Field name.*shadows an attribute in parent.*",
)


class VideoConcatMode(str, Enum):
    random = "random"
    sequential = "sequential"


class VideoAspect(str, Enum):
    landscape = "16:9"
    portrait = "9:16"
    square = "1:1"

    def to_resolution(self):
        if self == VideoAspect.landscape.value:
            return 1920, 1080
        elif self == VideoAspect.portrait.value:
            return 1080, 1920
        elif self == VideoAspect.square.value:
            return 1080, 1080
        return 1080, 1920


class _Config:
    arbitrary_types_allowed = True


@pydantic.dataclasses.dataclass(config=_Config)
class MaterialInfo:
    provider: str = "pexels"
    url: str = ""
    duration: int = 0


# VoiceNames = [
#     # zh-CN
#     "female-zh-CN-XiaoxiaoNeural",
#     "female-zh-CN-XiaoyiNeural",
#     "female-zh-CN-liaoning-XiaobeiNeural",
#     "female-zh-CN-shaanxi-XiaoniNeural",
#
#     "male-zh-CN-YunjianNeural",
#     "male-zh-CN-YunxiNeural",
#     "male-zh-CN-YunxiaNeural",
#     "male-zh-CN-YunyangNeural",
#
#     # "female-zh-HK-HiuGaaiNeural",
#     # "female-zh-HK-HiuMaanNeural",
#     # "male-zh-HK-WanLungNeural",
#     #
#     # "female-zh-TW-HsiaoChenNeural",
#     # "female-zh-TW-HsiaoYuNeural",
#     # "male-zh-TW-YunJheNeural",
#
#     # en-US
#     "female-en-US-AnaNeural",
#     "female-en-US-AriaNeural",
#     "female-en-US-AvaNeural",
#     "female-en-US-EmmaNeural",
#     "female-en-US-JennyNeural",
#     "female-en-US-MichelleNeural",
#
#     "male-en-US-AndrewNeural",
#     "male-en-US-BrianNeural",
#     "male-en-US-ChristopherNeural",
#     "male-en-US-EricNeural",
#     "male-en-US-GuyNeural",
#     "male-en-US-RogerNeural",
#     "male-en-US-SteffanNeural",
# ]


class VideoParams(BaseModel):
    """
    {
      "video_subject": "",
      "video_aspect": "æ¨ªå± 16:9ï¼ˆè¥¿ç“œè§†é¢‘ï¼‰",
      "voice_name": "å¥³ç”Ÿ-æ™“æ™“",
      "bgm_name": "random",
      "font_name": "STHeitiMedium é»‘ä½“-ä¸­",
      "text_color": "#FFFFFF",
      "font_size": 60,
      "stroke_color": "#000000",
      "stroke_width": 1.5
    }
    """

    video_subject: str
    video_script: str = ""  # ç”¨äºç”Ÿæˆè§†é¢‘çš„è„šæœ¬
    video_terms: Optional[str | list] = None  # ç”¨äºç”Ÿæˆè§†é¢‘çš„å…³é”®è¯
    video_aspect: Optional[VideoAspect] = VideoAspect.portrait.value
    video_concat_mode: Optional[VideoConcatMode] = VideoConcatMode.random.value
    video_clip_duration: Optional[int] = 5
    video_count: Optional[int] = 1

    video_source: Optional[str] = "pexels"
    video_materials: Optional[List[MaterialInfo]] = None  # ç”¨äºç”Ÿæˆè§†é¢‘çš„ç´ æ

    video_language: Optional[str] = ""  # auto detect

    voice_name: Optional[str] = ""
    voice_volume: Optional[float] = 1.0
    voice_rate: Optional[float] = 1.0
    bgm_type: Optional[str] = "random"
    bgm_file: Optional[str] = ""
    bgm_volume: Optional[float] = 0.2

    subtitle_enabled: Optional[bool] = True
    subtitle_position: Optional[str] = "bottom"  # top, bottom, center
    custom_position: float = 70.0
    font_name: Optional[str] = "STHeitiMedium.ttc"
    text_fore_color: Optional[str] = "#FFFFFF"
    text_background_color: Optional[str] = "transparent"

    font_size: int = 60
    stroke_color: Optional[str] = "#000000"
    stroke_width: float = 1.5
    n_threads: Optional[int] = 2
    paragraph_number: Optional[int] = 1


class SubtitleRequest(BaseModel):
    video_script: str
    video_language: Optional[str] = ""
    voice_name: Optional[str] = "zh-CN-XiaoxiaoNeural-Female"
    voice_volume: Optional[float] = 1.0
    voice_rate: Optional[float] = 1.2
    bgm_type: Optional[str] = "random"
    bgm_file: Optional[str] = ""
    bgm_volume: Optional[float] = 0.2
    subtitle_position: Optional[str] = "bottom"
    font_name: Optional[str] = "STHeitiMedium.ttc"
    text_fore_color: Optional[str] = "#FFFFFF"
    text_background_color: Optional[str] = "transparent"
    font_size: int = 60
    stroke_color: Optional[str] = "#000000"
    stroke_width: float = 1.5
    video_source: Optional[str] = "local"
    subtitle_enabled: Optional[str] = "true"


class AudioRequest(BaseModel):
    video_script: str
    video_language: Optional[str] = ""
    voice_name: Optional[str] = "zh-CN-XiaoxiaoNeural-Female"
    voice_volume: Optional[float] = 1.0
    voice_rate: Optional[float] = 1.2
    bgm_type: Optional[str] = "random"
    bgm_file: Optional[str] = ""
    bgm_volume: Optional[float] = 0.2
    video_source: Optional[str] = "local"


class VideoScriptParams:
    """
    {
      "video_subject": "æ˜¥å¤©çš„èŠ±æµ·",
      "video_language": "",
      "paragraph_number": 1
    }
    """

    video_subject: Optional[str] = "æ˜¥å¤©çš„èŠ±æµ·"
    video_language: Optional[str] = ""
    paragraph_number: Optional[int] = 1


class VideoTermsParams:
    """
    {
      "video_subject": "",
      "video_script": "",
      "amount": 5
    }
    """

    video_subject: Optional[str] = "æ˜¥å¤©çš„èŠ±æµ·"
    video_script: Optional[str] = (
        "æ˜¥å¤©çš„èŠ±æµ·ï¼Œå¦‚è¯—å¦‚ç”»èˆ¬å±•ç°åœ¨çœ¼å‰ã€‚ä¸‡ç‰©å¤è‹çš„å­£èŠ‚é‡Œï¼Œå¤§åœ°æŠ«ä¸Šäº†ä¸€è¢­ç»šä¸½å¤šå½©çš„ç››è£…ã€‚é‡‘é»„çš„è¿æ˜¥ã€ç²‰å«©çš„æ¨±èŠ±ã€æ´ç™½çš„æ¢¨èŠ±ã€è‰³ä¸½çš„éƒé‡‘é¦™â€¦â€¦"
    )
    amount: Optional[int] = 5


class BaseResponse(BaseModel):
    status: int = 200
    message: Optional[str] = "success"
    data: Any = None


class TaskVideoRequest(VideoParams, BaseModel):
    pass


class TaskQueryRequest(BaseModel):
    pass


class VideoScriptRequest(VideoScriptParams, BaseModel):
    pass


class VideoTermsRequest(VideoTermsParams, BaseModel):
    pass


######################################################################################################
######################################################################################################
######################################################################################################
######################################################################################################
class TaskResponse(BaseResponse):
    class TaskResponseData(BaseModel):
        task_id: str

    data: TaskResponseData

    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {"task_id": "6c85c8cc-a77a-42b9-bc30-947815aa0558"},
            },
        }


class TaskQueryResponse(BaseResponse):
    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {
                    "state": 1,
                    "progress": 100,
                    "videos": [
                        "http://127.0.0.1:8080/tasks/6c85c8cc-a77a-42b9-bc30-947815aa0558/final-1.mp4"
                    ],
                    "combined_videos": [
                        "http://127.0.0.1:8080/tasks/6c85c8cc-a77a-42b9-bc30-947815aa0558/combined-1.mp4"
                    ],
                },
            },
        }


class TaskDeletionResponse(BaseResponse):
    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {
                    "state": 1,
                    "progress": 100,
                    "videos": [
                        "http://127.0.0.1:8080/tasks/6c85c8cc-a77a-42b9-bc30-947815aa0558/final-1.mp4"
                    ],
                    "combined_videos": [
                        "http://127.0.0.1:8080/tasks/6c85c8cc-a77a-42b9-bc30-947815aa0558/combined-1.mp4"
                    ],
                },
            },
        }


class VideoScriptResponse(BaseResponse):
    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {
                    "video_script": "æ˜¥å¤©çš„èŠ±æµ·ï¼Œæ˜¯å¤§è‡ªç„¶çš„ä¸€å¹…ç¾ä¸½ç”»å·ã€‚åœ¨è¿™ä¸ªå­£èŠ‚é‡Œï¼Œå¤§åœ°å¤è‹ï¼Œä¸‡ç‰©ç”Ÿé•¿ï¼ŒèŠ±æœµäº‰ç›¸ç»½æ”¾ï¼Œå½¢æˆäº†ä¸€ç‰‡äº”å½©æ–‘æ–“çš„èŠ±æµ·..."
                },
            },
        }


class VideoTermsResponse(BaseResponse):
    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {"video_terms": ["sky", "tree"]},
            },
        }


class BgmRetrieveResponse(BaseResponse):
    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {
                    "files": [
                        {
                            "name": "output013.mp3",
                            "size": 1891269,
                            "file": "/NarratoAI/resource/songs/output013.mp3",
                        }
                    ]
                },
            },
        }


class BgmUploadResponse(BaseResponse):
    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {"file": "/NarratoAI/resource/songs/example.mp3"},
            },
        }


class VideoClipParams(BaseModel):
    """
    NarratoAI æ•°æ®æ¨¡å‹
    """
    video_clip_json: Optional[list] = Field(default=[], description="LLM ç”Ÿæˆçš„è§†é¢‘å‰ªè¾‘è„šæœ¬å†…å®¹")
    video_clip_json_path: Optional[str] = Field(default="", description="LLM ç”Ÿæˆçš„è§†é¢‘å‰ªè¾‘è„šæœ¬è·¯å¾„")
    video_origin_path: Optional[str] = Field(default="", description="åŸè§†é¢‘è·¯å¾„")
    video_aspect: Optional[VideoAspect] = Field(default=VideoAspect.portrait.value, description="è§†é¢‘æ¯”ä¾‹")
    video_language: Optional[str] = Field(default="zh-CN", description="è§†é¢‘è¯­è¨€")

    # video_clip_duration: Optional[int] = 5      # è§†é¢‘ç‰‡æ®µæ—¶é•¿
    # video_count: Optional[int] = 1      # è§†é¢‘ç‰‡æ®µæ•°é‡
    # video_source: Optional[str] = "local"
    # video_concat_mode: Optional[VideoConcatMode] = VideoConcatMode.random.value

    voice_name: Optional[str] = Field(default="zh-CN-YunjianNeural", description="è¯­éŸ³åç§°")
    voice_volume: Optional[float] = Field(default=1.0, description="è§£è¯´è¯­éŸ³éŸ³é‡")
    voice_rate: Optional[float] = Field(default=1.0, description="è¯­é€Ÿ")
    voice_pitch: Optional[float] = Field(default=1.0, description="è¯­è°ƒ")

    bgm_name: Optional[str] = Field(default="random", description="èƒŒæ™¯éŸ³ä¹åç§°")
    bgm_type: Optional[str] = Field(default="random", description="èƒŒæ™¯éŸ³ä¹ç±»å‹")
    bgm_file: Optional[str] = Field(default="", description="èƒŒæ™¯éŸ³ä¹æ–‡ä»¶")

    subtitle_enabled: bool = True
    font_name: str = "SimHei"  # é»˜è®¤ä½¿ç”¨é»‘ä½“
    font_size: int = 36
    text_fore_color: str = "white"              # æ–‡æœ¬å‰æ™¯è‰²
    text_back_color: Optional[str] = None       # æ–‡æœ¬èƒŒæ™¯è‰²
    stroke_color: str = "black"                 # æè¾¹é¢œè‰²
    stroke_width: float = 1.5                   # æè¾¹å®½åº¦
    subtitle_position: str = "bottom"  # top, bottom, center, custom

    n_threads: Optional[int] = Field(default=16, description="è§£è¯´è¯­éŸ³éŸ³é‡")    # çº¿ç¨‹ï¿½ï¿½ï¿½ï¼Œæœ‰åŠ©äºæå‡è§†é¢‘å¤„ç†é€Ÿåº¦

    tts_volume: Optional[float] = Field(default=1.0, description="è§£è¯´è¯­éŸ³éŸ³é‡ï¼ˆåå¤„ç†ï¼‰")
    original_volume: Optional[float] = Field(default=1.0, description="è§†é¢‘åŸå£°éŸ³é‡")
    bgm_volume: Optional[float] = Field(default=0.6, description="èƒŒæ™¯éŸ³ä¹éŸ³é‡")


class VideoTranscriptionRequest(BaseModel):
    video_name: str
    language: str = "zh-CN"

    class Config:
        arbitrary_types_allowed = True


class VideoTranscriptionResponse(BaseModel):
    transcription: str


class SubtitlePosition(str, Enum):
    TOP = "top"
    CENTER = "center"
    BOTTOM = "bottom"
</file>

<file path="app/services/audio_merger.py">
import os
import json
import subprocess
import edge_tts
from edge_tts import submaker
from pydub import AudioSegment
from typing import List, Dict
from loguru import logger
from app.utils import utils


def check_ffmpeg():
    """æ£€æŸ¥FFmpegæ˜¯å¦å·²å®‰è£…"""
    try:
        subprocess.run(['ffmpeg', '-version'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        return True
    except FileNotFoundError:
        return False


def merge_audio_files(task_id: str, audio_files: list, total_duration: float, list_script: list):
    """
    åˆå¹¶éŸ³é¢‘æ–‡ä»¶ï¼Œæ ¹æ®OSTè®¾ç½®å¤„ç†ä¸åŒçš„éŸ³é¢‘è½¨é“
    
    Args:
        task_id: ä»»åŠ¡ID
        audio_files: TTSç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨
        total_duration: æ€»æ—¶é•¿
        list_script: å®Œæ•´è„šæœ¬ä¿¡æ¯ï¼ŒåŒ…å«OSTè®¾ç½®
    
    Returns:
        str: åˆå¹¶åçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    """
    # æ£€æŸ¥FFmpegæ˜¯å¦å®‰è£…
    if not check_ffmpeg():
        logger.error("FFmpegæœªå®‰è£…ï¼Œæ— æ³•åˆå¹¶éŸ³é¢‘æ–‡ä»¶")
        return None

    # åˆ›å»ºä¸€ä¸ªç©ºçš„éŸ³é¢‘ç‰‡æ®µ
    final_audio = AudioSegment.silent(duration=total_duration * 1000)  # æ€»æ—¶é•¿ä»¥æ¯«ç§’ä¸ºå•ä½

    # éå†è„šæœ¬ä¸­çš„æ¯ä¸ªç‰‡æ®µ
    for segment, audio_file in zip(list_script, audio_files):
        try:
            # åŠ è½½TTSéŸ³é¢‘æ–‡ä»¶
            tts_audio = AudioSegment.from_file(audio_file)

            # è·å–ç‰‡æ®µçš„å¼€å§‹å’Œç»“æŸæ—¶é—´
            start_time, end_time = segment['new_timestamp'].split('-')
            start_seconds = utils.time_to_seconds(start_time)
            end_seconds = utils.time_to_seconds(end_time)

            # æ ¹æ®OSTè®¾ç½®å¤„ç†éŸ³é¢‘
            if segment['OST'] == 0:
                # åªä½¿ç”¨TTSéŸ³é¢‘
                final_audio = final_audio.overlay(tts_audio, position=start_seconds * 1000)
            elif segment['OST'] == 1:
                # åªä½¿ç”¨åŸå£°ï¼ˆå‡è®¾åŸå£°å·²ç»åœ¨è§†é¢‘ä¸­ï¼‰
                continue
            elif segment['OST'] == 2:
                # æ··åˆTTSéŸ³é¢‘å’ŒåŸå£°
                original_audio = AudioSegment.silent(duration=(end_seconds - start_seconds) * 1000)
                mixed_audio = original_audio.overlay(tts_audio)
                final_audio = final_audio.overlay(mixed_audio, position=start_seconds * 1000)

        except Exception as e:
            logger.error(f"å¤„ç†éŸ³é¢‘æ–‡ä»¶ {audio_file} æ—¶å‡ºé”™: {str(e)}")
            continue

    # ä¿å­˜åˆå¹¶åçš„éŸ³é¢‘æ–‡ä»¶
    output_audio_path = os.path.join(utils.task_dir(task_id), "final_audio.mp3")
    final_audio.export(output_audio_path, format="mp3")
    logger.info(f"åˆå¹¶åçš„éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜: {output_audio_path}")

    return output_audio_path


def time_to_seconds(time_str):
    """
    å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
    1. 'HH:MM:SS,mmm' (æ—¶:åˆ†:ç§’,æ¯«ç§’)
    2. 'MM:SS,mmm' (åˆ†:ç§’,æ¯«ç§’)
    3. 'SS,mmm' (ç§’,æ¯«ç§’)
    """
    try:
        # å¤„ç†æ¯«ç§’éƒ¨åˆ†
        if ',' in time_str:
            time_part, ms_part = time_str.split(',')
            ms = float(ms_part) / 1000
        else:
            time_part = time_str
            ms = 0

        # åˆ†å‰²æ—¶é—´éƒ¨åˆ†
        parts = time_part.split(':')
        
        if len(parts) == 3:  # HH:MM:SS
            h, m, s = map(int, parts)
            seconds = h * 3600 + m * 60 + s
        elif len(parts) == 2:  # MM:SS
            m, s = map(int, parts)
            seconds = m * 60 + s
        else:  # SS
            seconds = int(parts[0])

        return seconds + ms
    except (ValueError, IndexError) as e:
        logger.error(f"Error parsing time {time_str}: {str(e)}")
        return 0.0


def extract_timestamp(filename):
    """
    ä»æ–‡ä»¶åä¸­æå–å¼€å§‹å’Œç»“æŸæ—¶é—´æˆ³
    ä¾‹å¦‚: "audio_00_06,500-00_24,800.mp3" -> (6.5, 24.8)
    """
    try:
        # ä»æ–‡ä»¶åä¸­æå–æ—¶é—´éƒ¨åˆ†
        time_part = filename.split('_', 1)[1].split('.')[0]  # è·å– "00_06,500-00_24,800" éƒ¨åˆ†
        start_time, end_time = time_part.split('-')  # åˆ†å‰²æˆå¼€å§‹å’Œç»“æŸæ—¶é—´
        
        # å°†ä¸‹åˆ’çº¿æ ¼å¼è½¬æ¢å›å†’å·æ ¼å¼
        start_time = start_time.replace('_', ':')
        end_time = end_time.replace('_', ':')
        
        # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºç§’
        start_seconds = time_to_seconds(start_time)
        end_seconds = time_to_seconds(end_time)

        return start_seconds, end_seconds
    except Exception as e:
        logger.error(f"Error extracting timestamp from {filename}: {str(e)}")
        return 0.0, 0.0


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    audio_files =[
        "/Users/apple/Desktop/home/NarratoAI/storage/tasks/test456/audio_00:06-00:24.mp3",
        "/Users/apple/Desktop/home/NarratoAI/storage/tasks/test456/audio_00:32-00:38.mp3",
        "/Users/apple/Desktop/home/NarratoAI/storage/tasks/test456/audio_00:43-00:52.mp3",
        "/Users/apple/Desktop/home/NarratoAI/storage/tasks/test456/audio_00:52-01:09.mp3",
        "/Users/apple/Desktop/home/NarratoAI/storage/tasks/test456/audio_01:13-01:15.mp3",
    ]
    total_duration = 38
    video_script_path = "/Users/apple/Desktop/home/NarratoAI/resource/scripts/test003.json"
    with open(video_script_path, "r", encoding="utf-8") as f:
        video_script = json.load(f)

    output_file = merge_audio_files("test456", audio_files, total_duration, video_script)
    print(output_file)
</file>

<file path="app/services/llm.py">
import os
import re
import json
import traceback
import streamlit as st
from typing import List
from loguru import logger
from openai import OpenAI
from openai import AzureOpenAI
from moviepy.editor import VideoFileClip
from openai.types.chat import ChatCompletion
import google.generativeai as gemini
from googleapiclient.errors import ResumableUploadError
from google.api_core.exceptions import *
from google.generativeai.types import *
import subprocess
from typing import Union, TextIO

from app.config import config
from app.utils.utils import clean_model_output

_max_retries = 5

Method = """
é‡è¦æç¤ºï¼šæ¯ä¸€éƒ¨å‰§çš„æ–‡æ¡ˆï¼Œå‰å‡ å¥å¿…é¡»å¸å¼•äºº
é¦–å…ˆæˆ‘ä»¬åœ¨çœ‹å®Œçœ‹æ‡‚ç”µå½±åï¼Œå¤§è„‘é‡Œé¢è¦å…ˆæœ‰ä¸€ä¸ªå¤§æ¦‚çš„è½®å»“ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªç±»ä¼¼äºä½œæ–‡çš„å¤§çº²ï¼Œç”µå½±ä¸»é¢˜çº¿åœ¨å“ªé‡Œï¼Œé¦–å…ˆè¦æ‰¾åˆ°ã€‚
ä¸€èˆ¬å°†æ–‡æ¡ˆåˆ†ä¸ºå¼€å¤´ã€å†…å®¹ã€ç»“å°¾
## å¼€å¤´éƒ¨åˆ†
æ–‡æ¡ˆå¼€å¤´ä¸‰å¥è¯ï¼Œæ˜¯ç•™ä½ç”¨æˆ·çš„å…³é”®ï¼

### æ–¹å¼ä¸€ï¼šå¼€å¤´æ¦‚æ‹¬æ€»ç»“
æ–‡æ¡ˆçš„å‰ä¸‰å¥ï¼Œæ˜¯æ•´éƒ¨ç”µå½±çš„æ¦‚æ‹¬æ€»ç»“ï¼Œ2-3å¥ä»‹ç»åï¼Œå¼€å§‹å™è¿°æ•…äº‹å‰§æƒ…ï¼
æ¨èæ–°æ‰‹ï¼ˆæ–°å·ï¼‰åšï¼šï¼ˆç›˜ç‚¹å‹ï¼‰
ç›˜ç‚¹å…¨çƒæœ€ææ€–çš„10éƒ¨ç”µå½±
ç›˜ï¿½ï¿½ï¿½å…¨çƒæœ€ç§‘å¹»çš„10éƒ¨ç”µå½±
ç›˜ç‚¹å…¨çƒæœ€æ‚²æƒ¨çš„10éƒ¨ç”µå½±
ç›˜å…¨çƒæœ€å€¼å¾—çœ‹çš„10éƒ¨ç¾éš¾ç”µå½±
ç›˜ç‚¹å…¨çƒæœ€å€¼å¾—çœ‹çš„10éƒ¨åŠ±å¿—ç”µå½±

ä¸‹é¢çš„ç¤ºä¾‹å°±æ˜¯æœ€ç®€å•çš„è§£è¯´æ–‡æ¡ˆå¼€å¤´ï¼š
1.è¿™æ˜¯XXXå›½20å¹´æ¥æœ€å¤§å°ºåº¦çš„ä¸€éƒ¨å‰§ï¼Œæåº¦çƒ§è„‘ï¼Œå´è®©99%çš„äººçœ‹å¾—å¿ƒæ½®æ¾æ¹ƒã€æ— æ³•è‡ªæ‹”ï¼Œæ•…äº‹å¼€å§‹â€¦â€¦
2.è¿™æ˜¯æœ‰å²ä»¥æ¥ç”µå½±é™¢å”¯ä¸€ä¸€éƒ¨å…¨ç¨‹å¼€ç¯æ”¾å®Œçš„ç”µå½±ï¼ŒæœŸé—´æ— æ•°äººå°–å«æ˜å¥ï¼Œä»–è¢«æˆä¸ºå‹‡æ•¢è€…çš„ä¸“å±ï¼Œå› ä¸º99%çš„äººéƒ½ä¸æ•¢çœ‹åˆ°ç»“å±€ï¼Œè®¸å¤šäººçœ‹å®Œå®ƒä»æ­¤ä¸æ„¿å†ç¢°æ‰‹æœºï¼Œä»–å°±æ˜¯å¤§åé¼é¼çš„æš—é»‘ç¥ä½œã€ŠXXXã€‹â€¦â€¦
3.è¿™åˆ°åº•æ˜¯ä¸€éƒ¨ä»€ä¹ˆæ ·çš„ç”µå½±ï¼Œèƒ½è¢«55ä¸ªå›½å®¶å…¬å¼€æŠµåˆ¶ï¼Œå®ƒç”šè‡³ä¸ºäº†ä¸Šæ˜ ï¼Œä¸æƒœåˆ å‡æ‰æ•´æ•´47åˆ†é’Ÿçš„å‰§æƒ…â€¦â€¦
4.æ˜¯ä»€ä¹ˆæ ·çš„ä¸€ä¸ªäººè¢«è±†ç“£ç½‘å‹ç§°ä¹‹ä¸ºå²ä¸Šæœ€ç‰›Pçš„è€å¤ªå¤ªï¼Œéƒ½70å²äº†è¿˜è¦å»è´©æ¯’â€¦â€¦
5.ä»–æ˜¯Må›½å†å²ä¸Šæœ€NB/æƒ¨/çŒ–ç‹‚/å†¤æ‰â€¦â€¦çš„å›šçŠ¯/æŠ¢åŠ«çŠ¯/â€¦â€¦
6.è¿™åˆ°åº•æ˜¯ä¸€éƒ¨ä»€ä¹ˆæ ·çš„å½±ç‰‡ï¼Œä»–ä¸€ä¸ªäººå°±æ‹¿äº†4ä¸ªé¡¶çº§å¥–é¡¹ï¼Œç¬¬ä¸€å­£8.7åˆ†ï¼Œç¬¬äºŒå­£ç›´æ¥å¹²åˆ°9.5åˆ†ï¼Œ11ä¸‡äººç»™å‡º5æ˜Ÿå¥½è¯„ï¼Œä¸€å…±ä¹Ÿå°±6é›†ï¼Œå´æ–©è·26é¡¹å›½é™…å¤§å¥–ï¼Œçœ‹è¿‡çš„äººéƒ½è¯´ï¼Œä»–æ˜¯è¿‘å¹´æ¥æœ€å¥½çš„xxxå‰§ï¼Œå‡ ä¹æˆä¸ºäº†è¿‘å¹´æ¥xxxå‰§çš„æ ‡æ†ã€‚æ•…äº‹å‘ç”Ÿåœ¨â€¦â€¦
7.ä»–æ˜¯å›½äº§ç”µå½±çš„å·…å³°ä½³ä½œï¼Œæ›´æ˜¯è®¸å¤š80-90åçš„é’æ˜¥å¯è’™ï¼Œæ›¾å…¥é€‰ã€Šï¿½ï¿½ä»£ã€‹å‘¨åˆŠï¼Œè·å¾—å¹´åº¦ä½³ç‰‡ç¬¬ä¸€ï¼Œå¯åœ¨å›½å†…å´è¢«å°˜å°å¤šå¹´ï¼Œè‡³ä»Šä¸ºæ­¢éƒ½æ— æ³•åœ¨å„å¤§è§†é¢‘ç½‘ç«™çœ‹åˆ°å®Œæ•´èµ„æºï¼Œä»–å°±æ˜¯ã€Šxxxxxxã€‹
8.è¿™æ˜¯ä¸€éƒ¨è®©æ‰€æœ‰äººçœ‹å¾—è·å°”è’™é£™å‡çš„çˆ½ç‰‡â€¦â€¦
9.ä»–è¢«æˆä¸ºä¸–ç•Œä¸Šæœ€è™å¿ƒç»æœ›çš„ç”µå½±ï¼Œè‡³ä»Šæ— äººæ•¢çœ‹ç¬¬äºŒéï¼Œå¾ˆéš¾æƒ³è±¡ï¼Œä»–æ˜¯æ ¹æ®çœŸå®äº‹ä»¶æ”¹ç¼–è€Œæ¥â€¦â€¦
10.è¿™å¤§æ¦‚æ˜¯æœ‰å²ä»¥æ¥æœ€ä»¤äººä¸å¯’è€Œæ —çš„ç”µå½±ï¼Œå½“å¹´ä¸€ç»æ”¾æ˜ ï¼Œå°±ç‚¹ç‡ƒäº†æ— æ•°äººçš„æ€’ç«ï¼Œä¸å°‘è§‚ä¼—ä¸ç­‰å½±ç‰‡æ”¾å®Œï¼Œå°±æ„¤ç„¶ç¦»åœºï¼Œå®ƒæ¯”ã€Šxxxã€‹æ›´è®©äººç»æœ›ï¼Œæ¯”æ¯”ã€Šxxxã€‹æ›´è®©äººxxxï¼Œèƒ½åšæŒçœ‹å®Œå…¨ç‰‡çš„äººï¼Œæ›´æ˜¯ä¸‡ä¸­æ— ä¸€ï¼ŒåŒ…æ‹¬æˆ‘ã€‚ç”šè‡³è§‚å½±ç»“æŸåï¼Œæœ‰æ— æ•°äººæŠµåˆ¶æŠ•è¯‰è¿™éƒ¨ç”µå½±ï¼Œè®¤ä¸ºå½±ç‰‡çš„å¯¼æ¼”ç©å¼„äº†ä»–ä»¬çš„æƒ…æ„Ÿï¼ä»–æ˜¯é¡¶çº§ç¥ä½œã€Šxxxxã€‹â€¦â€¦
11.è¿™æ˜¯Xå›½æœ‰å²ä»¥æ¥æœ€é«˜èµçš„ä¸€éƒ¨æ‚¬ç–‘ç”µå½±ï¼Œç„¶è€Œå´å› ä¸ºæŸäº›åŸå› ï¼Œå›½å†…90%çš„äººï¼Œæ²¡èƒ½çœ‹è¿‡è¿™éƒ¨ç‰‡å­ï¼Œä»–å°±æ˜¯ã€Šxxxã€‹â€¦â€¦
12.æœ‰è¿™æ ·ä¸€éƒ¨ç”µå½±ï¼Œè¿™è¾ˆå­ï¼Œä½ ç»å¯¹ä¸æƒ³å†çœ‹ç¬¬äºŒéï¼Œå¹¶ä¸æ˜¯å®ƒå‰§æƒ…çƒ‚ä¿—ï¼Œè€Œæ˜¯å®ƒçš„ç»“å±€ä½ æ ¹æœ¬æ‰¿å—ä¸èµ·/æƒ³è±¡ä¸åˆ°â€¦â€¦ç”šè‡³æœ‰80%çš„è§‚ä¼—åœ¨è§‚å½±é€”ä¸­æƒ…ç»ªå´©æºƒä¸­é€”ç¦»åœºï¼Œæ›´è®©è®¸å¤šåŒè¡Œéƒ½ä¸æƒ³è§£è¯´è¿™éƒ¨ç”µå½±ï¼Œä»–å°±æ˜¯å¤§åé¼é¼çš„æš—é»‘ç¥ä½œã€Šxxxã€‹â€¦
13.å®ƒè¢«èª‰ä¸ºå²ä¸Šæœ€ç‰›æ‚¬ç–‘ç‰‡æ— æ•°äººåœ¨çœ‹å®Œå®ƒæ—¶å€™ï¼Œä¸€ä¸ªæœˆä¸æ•¢ç…§é•œï¿½ï¿½ï¼Œè¿™æ ·ä¸€éƒ¨ä»…é€‚åˆéƒ¨åˆ†å¹´é¾„æ®µè§‚çœ‹çš„å½±ç‰‡ï¼Œç©¶ç«Ÿæœ‰ä»€ä¹ˆæ ·çš„é­…åŠ›ï¼Œç«Ÿç„¶è·å¾—æŸç“£8.2çš„é«˜åˆ†ï¼Œå¾ˆå¤šäººè¯´è¿™éƒ¨ç”µå½±åˆ°å¤„éƒ½æ˜¯çœ‹ç‚¹ï¼Œä»–å°±æ˜¯ã€Šxxxã€‹â€¦.
14.è¿™æ˜¯ä¸€éƒ¨åœ¨æŸç“£ä¸Šè¢«70ä¸‡äººæ‰“å‡º9.3åˆ†çš„é«˜åˆ†çš„ç”µå½±â€¦â€¦åˆ°åº•æ˜¯ä¸€éƒ¨ä»€ä¹ˆæ ·çš„ç”µå½±ï¼Œèƒ½å¤Ÿåœ¨æŸç“£ä¸Šè¢«70ä¸‡äººæ‰“å‡º9.3åˆ†çš„é«˜åˆ†â€¦â€¦
15.è¿™æ˜¯ä¸€éƒ¨ç»†æ€ææçš„ç§‘å¹»å¤§ç‰‡ï¼Œæ•´éƒ¨ç”µå½±é¢ è¦†ä½ çš„ä¸‰è§‚ï¼Œå®ƒçš„åå­—å«â€¦â€¦
16.å²ä¸Šæœ€éœ‡æ’¼çš„ç¾éš¾ç‰‡ï¼Œæ¯ä¸€ç‚¹éƒ½ä¸èˆå¾—å¿«è¿›çš„ç”µå½±ï¼Œä»–å«â€¦â€¦
17.ä»Šå¤©ç»™å¤§å®¶å¸¦æ¥ä¸€éƒ¨åŸºäºçœŸå®äº‹ä»¶æ”¹ç¼–çš„ï¼ˆä¸»é¢˜ä»‹ç»ä¸€å¥â€¦â€¦ï¼‰çš„æ•…äº‹ç‰‡ï¼Œè¿™æ˜¯ä¸€éƒ¨è¿ç¯æ‚¬ç–‘å‰§ï¼Œå¦‚æœä¸çœ‹åˆ°æœ€åç»å¯¹æƒ³ä¸åˆ°ç»“å±€ç«Ÿç„¶æ˜¯è¿™æ ·çš„åè½¬â€¦â€¦

### æ–¹å¼ï¼šæƒ…æ™¯å¼ã€å‡è®¾æ€§å¼€å¤´
1.ä»–å«â€¦â€¦ä½ ä»¥ä¸ºä»–æ˜¯â€¦â€¦çš„å—ï¼Ÿä¸ã€‚ä»–æ˜¯æ¥â€¦â€¦ç„¶åå¼€å§‹å™è¿°
2.ä½ çŸ¥é“â€¦â€¦å—ï¼ŸåŸæ¥â€¦â€¦ç„¶åå¼€å§‹å™è¿°
3.å¦‚æœç»™ä½ â€¦.ï¼Œä½ ä¼šæ€ä¹ˆæ ·ï¼Ÿ
4.å¦‚æœä½ æ˜¯â€¦.ï¼Œä½ ä¼šæ€ä¹ˆæ ·ï¼Ÿ

### æ–¹å¼ä¸‰ï¼šä»¥å›½å®¶ä¸ºå¼€å¤´ï¼ç®€å•æ˜äº†ã€‚è¯è¯­ä¸éœ€è¦å¤šï¼Œä½†æ˜¯éœ€è¦è®²è§£é€å½»ï¼
1.è¿™æ˜¯ä¸€éƒ¨éŸ©å›½æœ€æ–°ç¾éš¾ç‰‡ï¼Œä½ ä¸€å®šæ²¡æœ‰çœ‹è¿‡â€¦â€¦
2.è¿™æ˜¯ä¸€éƒ¨å°åº¦é«˜åˆ†æ‚¬ç–‘ç‰‡ï¼Œ
3.è¿™éƒ¨ç”µå½±åŸåœ¨æ—¥æœ¬å› ä¸ºâ€¦â€¦è€Œè¢«ä¸‹æ¶ï¼Œ
4.è¿™æ˜¯éŸ©å›½æœ€ææ€–çš„çŠ¯ç½ªç‰‡ï¼Œ
5.è¿™æ˜¯æœ€è¿‘å›½äº§ç‰‡è¯„åˆ†æœ€é«˜çš„æ‚¬ç–‘ï¿½ï¿½
ä»¥ä¸Šå‡æŒ‰ç…§å½±ç‰‡å›½å®¶æ¥åŒºåˆ†ï¼Œç„¶åç®€å•ä»‹ç»ä¸‹ä¸»é¢˜ã€‚å°±å¯ä»¥å¼€å§‹ç›´æ¥å™è¿°ä½œå“ã€‚ä¹Ÿæ˜¯ä¸€ä¸ªå¾ˆä¸é”™çš„æ–¹æ³•ï¼

### æ–¹å¼å››ï¼šå¦‚ä½•è‡ªç”±å‘æŒ¥
æ­£å¸¸æƒ…å†µä¸‹ï¼Œæ¯ä¸€éƒ¨ç”µå½±éƒ½æœ‰éå¸¸å…³é”®çš„ä¸€ä¸ªå¤§çº²ï¼Œè¿™éƒ¨ç”µå½±çš„ä¸»é¢˜å…¶å®æ˜¯å¯ä»¥ç”¨ä¸€å¥è¯ã€ä¸¤å¥è¯æ¦‚æ‹¬çš„ã€‚åªè¦çœ‹æ‡‚ç”µå½±ï¼Œå°±èƒ½æ‰¾åˆ°è¿™ä¸ªä¸»é¢˜å¤§çº²ã€‚
æˆ‘ä»¬æå‰æŠŠè¿™ä¸ªä¸»é¢˜å¤§çº²ç»™æ”¾åˆ°å½±è§†æœ€å‰é¢ï¼Œä½œä¸ºæˆ‘ä»¬çš„å‰ä¸‰å¥çš„æ–‡æ¡ˆï¼Œå°†ä¼šéå¸¸å¸å¼•äººï¼

ä¾‹å¦‚ï¼š
1.è¿™ä¸æ˜¯ç”µå½±ï¼Œè¿™æ˜¯çœŸå®æ•…äº‹ã€‚ä¸¤ä¸ªå¥³äººå’Œä¸€ä¸ªç”·äººè¢«å…³åœ¨å¯æ¡‘æ‹¿å®¤ã€‚å–Šç ´å–‰å’™ä¹Ÿæ²¡æœ‰ä¸€ä¸å›éŸ³ã€‚çª’æ¯æ„Ÿå’Œçƒ­åº¦è®©äººæŠ“ç‹‚ï¼Œæ•…äº‹å°±æ˜¯ä»è¿™é‡Œå¼€å§‹ï¼ 
2.å¦‚æœä½ ç”·æœ‹å‹å‡ºè½¨äº†ï¼Œä»–ä¸çˆ±ä½ äº†ï¼Œè¿˜ä½ å®¶æš´ï¼Œæ€ä¹ˆåŠï¼Ÿæ¥ä¸‹æ¥è¿™éƒ¨ç”µå½±å°±ä¼šæ•™ä½ å¦‚ä½•è®©è€å…¬æœæœå¸–å¸–çš„å‘†åœ¨ä½ èº«è¾¹ï¼å¥³ä¸»æ˜¯ä¸€ä¸ªâ€¦â€¦å¼€å§‹å™è¿°äº†ã€‚ 
3.ä»–åŠ›å¤§æ— ç©·ï¼ŒåŒçœ¼æ”¾å…‰ï¼Œè¿™ä¸æ˜¯æ‹¯æ•‘åœ°çƒçš„è¶…äººå—ï¼Ÿç„¶è€Œä¸æ˜¯ã€‚ä»Šå¤©ç»™å¤§å®¶æ¨èçš„è¿™éƒ¨ç”µå½±å«â€¦â€¦

ä»¥ä¸Šæ˜¯éœ€è¦çœ‹å®Œå½±ç‰‡ï¼Œçœ‹æ‡‚å½±ç‰‡ï¼Œç„¶åä»é‡Œé¢æç‚¼å‡ºç²¾å½©çš„å‡ å¥è¯,å½“ç„¶æ˜¯æ¯”è¾ƒéš¾çš„ï¼Œå½“ä½ ä¸ä¼šè‡ªå·±å»æ€»ç»“å‰ä¸‰å¥çš„ç»å…¸çš„è¯ã€‚å¯ä»¥ç”¨å‰é¢æ–¹å¼ä¸€äºŒä¸‰ï¼
å®åœ¨æƒ³ä¸å‡ºæ¥å¦‚ä½•å»æç‚¼ï¼Œå¯ä»¥å»æœç´¢è¿™éƒ¨å‰§ï¼Œå¯¹è¿™éƒ¨ç”µå½±çš„å½±è¯„ï¼Œä¹Ÿä¼šç»™ä½ å¸¦è¿‡æ¥å¾ˆå¤šçµæ„Ÿçš„ï¼


## å†…å®¹éƒ¨åˆ†
å¼€å¤´æœ‰äº†ï¼Œå‰©ä¸‹çš„å°±æ˜¯å¼€å§‹å™è¿°æ­£æ–‡äº†ã€‚ä¸»é¢˜ä»‹ç»æ˜¯æ ¹æ®å½±ç‰‡å†…å®¹æ¥ä»‹ç»ï¼Œå¦‚æœå®åœ¨è‡ªå·±æƒ³ä¸å‡ºæ¥ã€‚å¯ä»¥å‚è€ƒå…¶ä»–å¹³å°ä¸­å¯¹è¿™éƒ¨ç”µå½±çš„ç²¾å½©ä»‹ç»ï¼Œæå–2-3å¥ä¹Ÿå¯ä»¥ï¼
æ­£å¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å™è¿°çš„æ—¶å€™å…¶å®æ˜¯éå¸¸ç®€å•çš„ï¼ŒæŠŠæ•´éƒ¨ç”µå½±ä¸»é¢˜çº¿ï¼Œå™è¿°ä¸‹æ¥ï¼Œå…¶å®æ–‡æ¡ˆå°±æ˜¯åŠ äº›ä¿®é¥°è¯æŠŠç”µå½±é‡ç‚¹å†…å®¹å™è¿°ä¸‹æ¥ã€‚åŠ ä¸Šä¸€äº›ä¿®é¥°è¯ã€‚

ä»¥æ‚¬ç–‘å‰§ä¸ºä¾‹ï¼š
ç«Ÿç„¶ï¼Œçªç„¶ï¼ŒåŸæ¥ï¼Œä½†æ˜¯ï¼Œä½†ï¼Œå¯æ˜¯ï¼Œç»“æœï¼Œç›´åˆ°ï¼Œå¦‚æœï¼Œè€Œï¼Œæœç„¶ï¼Œå‘ç°ï¼Œåªæ˜¯ï¼Œå‡ºå¥‡ï¼Œä¹‹åï¼Œæ²¡é”™ï¼Œä¸æ­¢ï¼Œæ›´æ˜¯ï¼Œå½“ç„¶ï¼Œå› ä¸ºï¼Œæ‰€ä»¥â€¦â€¦ç­‰ï¼
ä»¥ä¸Šæ˜¯æ¯”è¾ƒå¸¸ç”¨çš„ï¼Œå½“ç„¶è¿˜æœ‰å¾ˆå¤šï¼Œéœ€è¦é å¹³æ—¶æ€è€ƒå’Œé˜…è¯»çš„ç§¯ç´¯ï¼å› æ‚¬ç–‘å‰§ä¼šæœ‰å¤šå¤„åè½¬å‰§æƒ…ã€‚æ‰€ä»¥éœ€è¦ç”¨åˆ°åè½¬çš„ä¿®é¥°è¯æ¯”è¾ƒå¤šï¼Œåªæœ‰ç”¨åˆ°è¿™äº›è¯ã€‚æ‰èƒ½ä½“ç°å‡ºå„ç§åè½¬å‰§æƒ…ï¼
å»ºè®®å¤§å®¶åœ¨åˆšå¼€å§‹åšçš„æ—¶å€™ï¼Œåš8åˆ†é’Ÿå†…çš„ï¼Œä¸è¦å¤ªé•¿ï¼Œåˆ†æˆä¸‰æ®µã€‚æ¯æ®µä¹Ÿæ˜¯ä¸è¶…è¿‡ä¸‰åˆ†é’Ÿï¼Œè¿™æ ·æ—¶é—´åˆšå¥½ã€‚å¯ä»¥æ¯”è¾ƒå¥½çš„å®Œæˆå®Œæ’­ç‡ï¼


## ç»“å°¾éƒ¨åˆ†
æœ€åæ•…äº‹çš„ç»“å±€ï¼Œé™¤äº†åè½¬ï¼Œå¯ä»¥æ¥ç‚¹äººç”Ÿçš„é“ç†ï¼å¦‚æœåˆšå¼€å§‹ä¸ä¼šï¼Œå¯ä»¥ä¸å†™ã€‚
åé¢æ°´å¹³è¶Šæ¥è¶Šé«˜çš„æ—¶å€™ï¼Œå¯ä»¥è¿›è¡Œäººç”Ÿé“ç†çš„è®²è¯„ã€‚

æ¯”å¦‚ï¼šè¿™éƒ¨ç”µå½±å‘Šè¯‰æˆ‘ä»¬â€¦â€¦
ç±»ä¼¼äºå“²ç†æ€§è´¨ï¿½ï¿½ä½œä¸ºä¸€ä¸ªæ€»ç»“ï¼
ä¹Ÿå¯ä»¥æŠŠæœ€åçš„å½±è§†åè½¬ï¼ŒåŸç”Ÿæ”¾å‡ºæ¥ï¼Œç•™ä¸‹æ‚¬å¿µã€‚

æ¯”å¦‚ï¼šä¹Ÿå¯ä»¥æ€»ç»“ä¸‹è¿™éƒ¨çŸ­ç‰‡å¦‚ä½•çš„å¥½ï¼Œæ¨è/å€¼å¾—å¤§å®¶å»è§‚çœ‹ä¹‹ç±»çš„è¯è¯­ã€‚
å…¶å®å°±æ˜¯ç»™æˆ‘ä»¬çš„ä½œå“æ¥ä¸€ä¸ªæ€»ç»“ï¼Œæ€»ç»“æˆ‘ä»¬æ‰€åšçš„ä¸‰ä¸ªè§†é¢‘ï¼Œæœ‰å¼€å§‹å°±è¦æœ‰ç»“æŸã€‚è¿™ä¸ªç»“æŸä¸ä¸€å®šæ˜¯å›ºå®šçš„æ¨¡ç‰ˆã€‚ä½†æ˜¯è§†é¢‘ä¸€å®šè¦æœ‰ç»“å°¾ã€‚è®©äººæ„Ÿè§‰æœ‰å¤´æœ‰å°¾æ‰æœ€èˆ’æœï¼
åšè§£è¯´ç¬¬ä¸€æ¬¡ï¼Œå¯èƒ½ä¼šåšä¸¤å¤©ã€‚ç¬¬äºŒæ¬¡å¯èƒ½å°±éœ€è¦ä¸€å¤©äº†ã€‚æ…¢æ…¢çš„ã€‚æ—¶é—´ç¼©çŸ­åˆ°8ä¸ªå°æ—¶ä¹‹å†…æ˜¯æˆ‘ä»¬å¹³çš„åˆ¶ä½œå…¨éƒ¨æ—¶é—´ï¼

"""


def handle_exception(err):
    if isinstance(err, PermissionDenied):
        raise Exception("403 ç”¨æˆ·æ²¡æœ‰æƒé™è®¿é—®è¯¥èµ„æº")
    elif isinstance(err, ResourceExhausted):
        raise Exception("429 æ‚¨çš„é…é¢å·²ç”¨å°½ã€‚è¯·ç¨åé‡è¯•ã€‚è¯·è€ƒè™‘è®¾ç½®è‡ªåŠ¨é‡è¯•æ¥å¤„ç†è¿™äº›é”™è¯¯")
    elif isinstance(err, InvalidArgument):
        raise Exception("400 å‚æ•°æ— æ•ˆã€‚ä¾‹å¦‚ï¼Œæ–‡ä»¶è¿‡å¤§ï¼Œè¶…å‡ºäº†è½½è·å¤§å°é™åˆ¶ã€‚å¦ä¸€ä¸ªäº‹ä»¶æä¾›äº†æ— æ•ˆçš„ API å¯†é’¥ã€‚")
    elif isinstance(err, AlreadyExists):
        raise Exception("409 å·²å­˜åœ¨å…·æœ‰ç›¸åŒ ID çš„å·²è°ƒå‚æ¨¡å‹ã€‚å¯¹æ–°æ¨¡å‹è¿›è¡Œè°ƒå‚æ—¶ï¼Œè¯·æŒ‡å®šå”¯ä¸€çš„æ¨¡å‹ IDã€‚")
    elif isinstance(err, RetryError):
        raise Exception("ä½¿ç”¨ä¸æ”¯æŒ gRPC çš„ä»£ç†æ—¶å¯èƒ½ä¼šå¼•èµ·æ­¤é”™è¯¯ã€‚è¯·å°è¯•å°† REST ä¼ è¾“ä¸ genai.configure(..., transport=rest) æ­é…ä½¿ç”¨ã€‚")
    elif isinstance(err, BlockedPromptException):
        raise Exception("400 å‡ºäºå®‰å…¨åŸå› ï¼Œè¯¥æç¤ºå·²è¢«å±è”½ã€‚")
    elif isinstance(err, BrokenResponseError):
        raise Exception("500 æµå¼ä¼ è¾“å“åº”å·²æŸåã€‚åœ¨è®¿é—®éœ€è¦å®Œæ•´å“åº”çš„å†…å®¹ï¼ˆä¾‹å¦‚èŠå¤©è®°å½•ï¼‰æ—¶å¼•å‘ã€‚æŸ¥çœ‹å †æ ˆè½¨è¿¹ä¸­æä¾›çš„é”™è¯¯è¯¦æƒ…ã€‚")
    elif isinstance(err, IncompleteIterationError):
        raise Exception("500 è®¿é—®éœ€è¦å®Œæ•´ API å“åº”ä½†æµå¼å“åº”å°šæœªå®Œå…¨è¿­ä»£çš„å†…å®¹æ—¶å¼•å‘ã€‚å¯¹å“åº”å¯¹è±¡è°ƒç”¨ resolve() ä»¥ä½¿ç”¨è¿­ä»£å™¨ã€‚")
    elif isinstance(err, ConnectionError):
        raise Exception("ç½‘ç»œè¿æ¥é”™è¯¯, è¯·æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥(å»ºè®®ä½¿ç”¨ NarratoAI å®˜æ–¹æä¾›çš„ url)")
    else:
        raise Exception(f"å¤§æ¨¡å‹è¯·æ±‚å¤±è´¥, ä¸‹é¢æ˜¯å…·ä½“æŠ¥é”™ä¿¡æ¯: \n\n{traceback.format_exc()}")


def _generate_response(prompt: str, llm_provider: str = None) -> str:
    """
    è°ƒç”¨å¤§æ¨¡å‹é€šç”¨æ–¹æ³•
        promptï¼š
        llm_providerï¼š
    """
    content = ""
    if not llm_provider:
        llm_provider = config.app.get("llm_provider", "openai")
    logger.info(f"llm provider: {llm_provider}")
    if llm_provider == "g4f":
        model_name = config.app.get("g4f_model_name", "")
        if not model_name:
            model_name = "gpt-3.5-turbo-16k-0613"
        import g4f

        content = g4f.ChatCompletion.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
        )
    else:
        api_version = ""  # for azure
        if llm_provider == "moonshot":
            api_key = config.app.get("moonshot_api_key")
            model_name = config.app.get("moonshot_model_name")
            base_url = "https://api.moonshot.cn/v1"
        elif llm_provider == "ollama":
            # api_key = config.app.get("openai_api_key")
            api_key = "ollama"  # any string works but you are required to have one
            model_name = config.app.get("ollama_model_name")
            base_url = config.app.get("ollama_base_url", "")
            if not base_url:
                base_url = "http://localhost:11434/v1"
        elif llm_provider == "openai":
            api_key = config.app.get("openai_api_key")
            model_name = config.app.get("openai_model_name")
            base_url = config.app.get("openai_base_url", "")
            if not base_url:
                base_url = "https://api.openai.com/v1"
        elif llm_provider == "oneapi":
            api_key = config.app.get("oneapi_api_key")
            model_name = config.app.get("oneapi_model_name")
            base_url = config.app.get("oneapi_base_url", "")
        elif llm_provider == "azure":
            api_key = config.app.get("azure_api_key")
            model_name = config.app.get("azure_model_name")
            base_url = config.app.get("azure_base_url", "")
            api_version = config.app.get("azure_api_version", "2024-02-15-preview")
        elif llm_provider == "gemini":
            api_key = config.app.get("gemini_api_key")
            model_name = config.app.get("gemini_model_name")
            base_url = "***"
        elif llm_provider == "qwen":
            api_key = config.app.get("qwen_api_key")
            model_name = config.app.get("qwen_model_name")
            base_url = "***"
        elif llm_provider == "cloudflare":
            api_key = config.app.get("cloudflare_api_key")
            model_name = config.app.get("cloudflare_model_name")
            account_id = config.app.get("cloudflare_account_id")
            base_url = "***"
        elif llm_provider == "deepseek":
            api_key = config.app.get("deepseek_api_key")
            model_name = config.app.get("deepseek_model_name")
            base_url = config.app.get("deepseek_base_url")
            if not base_url:
                base_url = "https://api.deepseek.com"
        elif llm_provider == "ernie":
            api_key = config.app.get("ernie_api_key")
            secret_key = config.app.get("ernie_secret_key")
            base_url = config.app.get("ernie_base_url")
            model_name = "***"
            if not secret_key:
                raise ValueError(
                    f"{llm_provider}: secret_key is not set, please set it in the config.toml file."
                )
        else:
            raise ValueError(
                "llm_provider is not set, please set it in the config.toml file."
            )

        if not api_key:
            raise ValueError(
                f"{llm_provider}: api_key is not set, please set it in the config.toml file."
            )
        if not model_name:
            raise ValueError(
                f"{llm_provider}: model_name is not set, please set it in the config.toml file."
            )
        if not base_url:
            raise ValueError(
                f"{llm_provider}: base_url is not set, please set it in the config.toml file."
            )

        if llm_provider == "qwen":
            import dashscope
            from dashscope.api_entities.dashscope_response import GenerationResponse

            dashscope.api_key = api_key
            response = dashscope.Generation.call(
                model=model_name, messages=[{"role": "user", "content": prompt}]
            )
            if response:
                if isinstance(response, GenerationResponse):
                    status_code = response.status_code
                    if status_code != 200:
                        raise Exception(
                            f'[{llm_provider}] returned an error response: "{response}"'
                        )

                    content = response["output"]["text"]
                    return content.replace("\n", "")
                else:
                    raise Exception(
                        f'[{llm_provider}] returned an invalid response: "{response}"'
                    )
            else:
                raise Exception(f"[{llm_provider}] returned an empty response")

        if llm_provider == "gemini":
            import google.generativeai as genai

            genai.configure(api_key=api_key, transport="rest")

            safety_settings = {
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            }

            model = genai.GenerativeModel(
                model_name=model_name,
                safety_settings=safety_settings,
            )

            try:
                response = model.generate_content(prompt)
                return response.text
            except Exception as err:
                return handle_exception(err)

        if llm_provider == "cloudflare":
            import requests

            response = requests.post(
                f"https://api.cloudflare.com/client/v4/accounts/{account_id}/ai/run/{model_name}",
                headers={"Authorization": f"Bearer {api_key}"},
                json={
                    "messages": [
                        {"role": "system", "content": "You are a friendly assistant"},
                        {"role": "user", "content": prompt},
                    ]
                },
            )
            result = response.json()
            logger.info(result)
            return result["result"]["response"]

        if llm_provider == "ernie":
            import requests

            params = {
                "grant_type": "client_credentials",
                "client_id": api_key,
                "client_secret": secret_key,
            }
            access_token = (
                requests.post("https://aip.baidubce.com/oauth/2.0/token", params=params)
                .json()
                .get("access_token")
            )
            url = f"{base_url}?access_token={access_token}"

            payload = json.dumps(
                {
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.5,
                    "top_p": 0.8,
                    "penalty_score": 1,
                    "disable_search": False,
                    "enable_citation": False,
                    "response_format": "text",
                }
            )
            headers = {"Content-Type": "application/json"}

            response = requests.request(
                "POST", url, headers=headers, data=payload
            ).json()
            return response.get("result")

        if llm_provider == "azure":
            client = AzureOpenAI(
                api_key=api_key,
                api_version=api_version,
                azure_endpoint=base_url,
            )
        else:
            client = OpenAI(
                api_key=api_key,
                base_url=base_url,
            )

        response = client.chat.completions.create(
            model=model_name, messages=[{"role": "user", "content": prompt}]
        )
        if response:
            if isinstance(response, ChatCompletion):
                content = response.choices[0].message.content
            else:
                raise Exception(
                    f'[{llm_provider}] returned an invalid response: "{response}", please check your network '
                    f"connection and try again."
                )
        else:
            raise Exception(
                f"[{llm_provider}] returned an empty response, please check your network connection and try again."
            )

    return content.replace("\n", "")


def _generate_response_video(prompt: str, llm_provider_video: str, video_file: Union[str, TextIO]) -> str:
    """
    å¤šæ¨¡æ€èƒ½åŠ›å¤§æ¨¡å‹
    """
    if llm_provider_video == "gemini":
        api_key = config.app.get("gemini_api_key")
        model_name = config.app.get("gemini_model_name")
        base_url = "***"
    else:
        raise ValueError(
            "llm_provider æœªè®¾ç½®ï¼Œè¯·åœ¨ config.toml æ–‡ä»¶ä¸­è¿›è¡Œè®¾ç½®ã€‚"
        )

    if llm_provider_video == "gemini":
        import google.generativeai as genai

        genai.configure(api_key=api_key, transport="rest")

        safety_settings = {
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        }

        model = genai.GenerativeModel(
            model_name=model_name,
            safety_settings=safety_settings,
        )

        try:
            response = model.generate_content([prompt, video_file])
            return response.text
        except Exception as err:
            return handle_exception(err)


def compress_video(input_path: str, output_path: str):
    """
    å‹ç¼©è§†é¢‘æ–‡ä»¶
    Args:
        input_path: è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºå‹ç¼©åçš„è§†é¢‘æ–‡ä»¶è·¯å¾„
    """
    # å¦‚æœå‹ç¼©åçš„è§†é¢‘æ–‡ä»¶å·²ç»å­˜åœ¨ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
    if os.path.exists(output_path):
        logger.info(f"å‹ç¼©è§†é¢‘æ–‡ä»¶å·²å­˜åœ¨: {output_path}")
        return

    try:
        clip = VideoFileClip(input_path)
        clip.write_videofile(output_path, codec='libx264', audio_codec='aac', bitrate="500k", audio_bitrate="128k")
    except subprocess.CalledProcessError as e:
        logger.error(f"è§†é¢‘å‹ç¼©å¤±è´¥: {e}")
        raise


def generate_script(
    video_path: str, video_plot: str, video_name: str, language: str = "zh-CN", progress_callback=None
) -> str:
    """
    ç”Ÿæˆè§†é¢‘å‰ªè¾‘è„šæœ¬
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        video_plot: è§†é¢‘å‰§æƒ…å†…å®¹
        video_name: è§†é¢‘åç§°
        language: è¯­è¨€
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        str: ç”Ÿæˆçš„è„šæœ¬
    """
    try:
        # 1. å‹ç¼©è§†é¢‘
        compressed_video_path = f"{os.path.splitext(video_path)[0]}_compressed.mp4"
        compress_video(video_path, compressed_video_path)

        # åœ¨å…³é”®æ­¥éª¤æ›´æ–°è¿›åº¦
        if progress_callback:
            progress_callback(15, "å‹ç¼©å®Œæˆ")  # ä¾‹å¦‚,åœ¨å‹ç¼©è§†é¢‘å

        # 2. è½¬å½•è§†é¢‘
        transcription = gemini_video_transcription(
            video_name=video_name,
            video_path=compressed_video_path,
            language=language,
            llm_provider_video=config.app["video_llm_provider"],
            progress_callback=progress_callback
        )
        if progress_callback:
            progress_callback(60, "ç”Ÿæˆè§£è¯´æ–‡æ¡ˆ...")  # ä¾‹å¦‚,åœ¨è½¬å½•è§†é¢‘å

        # 3. ç¼–å†™è§£è¯´æ–‡æ¡ˆ
        script = writing_short_play(video_plot, video_name, config.app["llm_provider"], count=300)

        # åœ¨å…³é”®æ­¥éª¤æ›´æ–°è¿›åº¦
        if progress_callback:
            progress_callback(70, "åŒ¹é…ç”»é¢...")  # ä¾‹å¦‚,åœ¨ç”Ÿæˆè„šæœ¬å

        # 4. æ–‡æ¡ˆåŒ¹é…ç”»é¢
        if transcription != "":
            matched_script = screen_matching(huamian=transcription, wenan=script, llm_provider=config.app["video_llm_provider"])
            # åœ¨å…³é”®æ­¥éª¤æ›´æ–°è¿›åº¦
            if progress_callback:
                progress_callback(80, "åŒ¹é…æˆåŠŸ")
            return matched_script
        else:
            return ""
    except Exception as e:
        handle_exception(e)
        raise


def gemini_video_transcription(video_name: str, video_path: str, language: str, llm_provider_video: str, progress_callback=None):
    '''
    ä½¿ç”¨ gemini-1.5-xxx è¿›è¡Œè§†é¢‘ç”»é¢è½¬å½•
    '''
    api_key = config.app.get("gemini_api_key")
    gemini.configure(api_key=api_key)

    prompt = """
    è¯·è½¬å½•éŸ³é¢‘ï¼ŒåŒ…æ‹¬æ—¶é—´æˆ³ï¼Œå¹¶æä¾›è§†è§‰æè¿°ï¼Œç„¶åä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œå½“å‰è§†é¢‘ä¸­ä½¿ç”¨çš„è¯­è¨€ä¸º %sã€‚
    
    åœ¨è½¬å½•è§†é¢‘æ—¶ï¼Œè¯·é€šè¿‡ç¡®ä¿ä»¥ä¸‹æ¡ä»¶æ¥å®Œæˆè½¬å½•ï¼š
    1. ç”»é¢æè¿°ä½¿ç”¨è¯­è¨€: %s è¿›è¡Œè¾“å‡ºã€‚
    2. åŒä¸€ä¸ªç”»é¢åˆå¹¶ä¸ºä¸€ä¸ªè½¬å½•è®°å½•ã€‚
    3. ä½¿ç”¨ä»¥ä¸‹ JSON schema:    
        Graphics = {"timestamp": "MM:SS-MM:SS"(æ—¶é—´æˆ³æ ¼å¼), "picture": "str"(ç”»é¢æè¿°), "speech": "str"(å°è¯ï¼Œå¦‚æœæ²¡æœ‰äººè¯´è¯ï¼Œåˆ™ä½¿ç”¨ç©ºå­—ç¬¦ä¸²ã€‚)}
        Return: list[Graphics]
    4. è¯·ä»¥ä¸¥æ ¼çš„ JSON æ ¼å¼è¿”å›æ•°æ®ï¼Œä¸è¦åŒ…å«ä»»ä½•æ³¨é‡Šã€æ ‡è®°æˆ–å…¶ä»–å­—ç¬¦ã€‚æ•°æ®åº”ç¬¦åˆ JSON è¯­æ³•ï¼Œå¯ä»¥è¢« json.loads() å‡½æ•°ç›´æ¥è§£æï¼Œ ä¸è¦æ·»åŠ  ```json æˆ–å…¶ä»–æ ‡è®°ã€‚
    """ % (language, language)

    logger.debug(f"è§†é¢‘åç§°: {video_name}")
    try:
        if progress_callback:
            progress_callback(20, "ä¸Šä¼ è§†é¢‘è‡³ Google cloud")
        gemini_video_file = gemini.upload_file(video_path)
        logger.debug(f"è§†é¢‘ {gemini_video_file.name} ä¸Šä¼ è‡³ Google cloud æˆåŠŸ, å¼€å§‹è§£æ...")
        while gemini_video_file.state.name == "PROCESSING":
            gemini_video_file = gemini.get_file(gemini_video_file.name)
            if progress_callback:
                progress_callback(30, "ä¸Šä¼ æˆåŠŸ, å¼€å§‹è§£æ")  # æ›´æ–°è¿›åº¦ä¸º20%
        if gemini_video_file.state.name == "FAILED":
            raise ValueError(gemini_video_file.state.name)
        elif gemini_video_file.state.name == "ACTIVE":
            if progress_callback:
                progress_callback(40, "è§£æå®Œæˆ, å¼€å§‹è½¬å½•...")  # æ›´æ–°è¿›åº¦ä¸º30%
            logger.debug("è§£æå®Œæˆ, å¼€å§‹è½¬å½•...")
    except ResumableUploadError as err:
        logger.error(f"ä¸Šä¼ è§†é¢‘è‡³ Google cloud å¤±è´¥, ç”¨æˆ·çš„ä½ç½®ä¿¡æ¯ä¸æ”¯æŒç”¨äºè¯¥API; \n{traceback.format_exc()}")
        return False
    except FailedPrecondition as err:
        logger.error(f"400 ç”¨æˆ·ä½ç½®ä¸æ”¯æŒ Google API ä½¿ç”¨ã€‚\n{traceback.format_exc()}")
        return False

    if progress_callback:
        progress_callback(50, "å¼€å§‹è½¬å½•")
    try:
        response = _generate_response_video(prompt=prompt, llm_provider_video=llm_provider_video, video_file=gemini_video_file)
        logger.success("è§†é¢‘è½¬å½•æˆåŠŸ")
        logger.debug(response)
        print(type(response))
        return response
    except Exception as err:
        return handle_exception(err)


def generate_terms(video_subject: str, video_script: str, amount: int = 5) -> List[str]:
    prompt = f"""
# Role: Video Search Terms Generator

## Goals:
Generate {amount} search terms for stock videos, depending on the subject of a video.

## Constrains:
1. the search terms are to be returned as a json-array of strings.
2. each search term should consist of 1-3 words, always add the main subject of the video.
3. you must only return the json-array of strings. you must not return anything else. you must not return the script.
4. the search terms must be related to the subject of the video.
5. reply with english search terms only.

## Output Example:
["search term 1", "search term 2", "search term 3","search term 4","search term 5"]

## Context:
### Video Subject
{video_subject}

### Video Script
{video_script}

Please note that you must use English for generating video search terms; Chinese is not accepted.
""".strip()

    logger.info(f"subject: {video_subject}")

    search_terms = []
    response = ""
    for i in range(_max_retries):
        try:
            response = _generate_response(prompt)
            search_terms = json.loads(response)
            if not isinstance(search_terms, list) or not all(
                isinstance(term, str) for term in search_terms
            ):
                logger.error("response is not a list of strings.")
                continue

        except Exception as e:
            logger.warning(f"failed to generate video terms: {str(e)}")
            if response:
                match = re.search(r"\[.*]", response)
                if match:
                    try:
                        search_terms = json.loads(match.group())
                    except Exception as e:
                        logger.warning(f"failed to generate video terms: {str(e)}")
                        pass

        if search_terms and len(search_terms) > 0:
            break
        if i < _max_retries:
            logger.warning(f"failed to generate video terms, trying again... {i + 1}")

    logger.success(f"completed: \n{search_terms}")
    return search_terms


def gemini_video2json(video_origin_name: str, video_origin_path: str, video_plot: str, language: str) -> str:
    '''
    ä½¿ç”¨ gemini-1.5-pro è¿›è¡Œå½±è§†è§£æ
    Args:
        video_origin_name: str - å½±è§†ä½œå“çš„åŸå§‹åç§°
        video_origin_path: str - å½±è§†ä½œå“çš„åŸå§‹è·¯å¾„
        video_plot: str - å½±è§†ä½œå“çš„ç®€ä»‹æˆ–å‰§æƒ…æ¦‚è¿°

    Return:
        str - è§£æåçš„ JSON æ ¼å¼å­—ç¬¦ä¸²
    '''
    api_key = config.app.get("gemini_api_key")
    model_name = config.app.get("gemini_model_name")

    gemini.configure(api_key=api_key)
    model = gemini.GenerativeModel(model_name=model_name)

    prompt = """
**è§’è‰²è®¾å®šï¼š**  
ä½ æ˜¯ä¸€ä½å½±è§†è§£è¯´ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®å‰§æƒ…ç”Ÿæˆå¼•äººå…¥èƒœçš„çŸ­è§†é¢‘è§£è¯´æ–‡æ¡ˆï¼Œç‰¹åˆ«ç†Ÿæ‚‰é€‚ç”¨äºTikTok/æŠ–éŸ³é£æ ¼çš„å¿«é€Ÿã€æŠ“äººè§†é¢‘è§£è¯´ã€‚

**ä»»åŠ¡ç›®æ ‡ï¼š**  
1. æ ¹æ®ç»™å®šå‰§æƒ…ï¼Œè¯¦ç»†æè¿°ç”»é¢ï¼Œé‡ç‚¹çªå‡ºé‡è¦åœºæ™¯å’Œæƒ…èŠ‚ã€‚  
2. ç”Ÿæˆç¬¦åˆTikTok/æŠ–éŸ³é£æ ¼çš„è§£è¯´ï¼ŒèŠ‚å¥ç´§å‡‘ï¼Œè¯­è¨€ç®€æ´ï¼Œå¸å¼•è§‚ä¼—ã€‚  
3. è§£è¯´çš„æ—¶å€™éœ€è¦è§£è¯´ä¸€æ®µæ’­æ”¾ä¸€æ®µåŸè§†é¢‘ï¼ŒåŸè§†é¢‘ä¸€èˆ¬ä¸ºæœ‰å°è¯çš„ç‰‡æ®µï¼ŒåŸè§†é¢‘çš„æ§åˆ¶æœ‰ OST å­—æ®µæ§åˆ¶ã€‚
4. ç»“æœè¾“å‡ºä¸ºJSONæ ¼å¼ï¼ŒåŒ…å«å­—æ®µï¼š  
   - "picture"ï¼šç”»é¢æè¿°  
   - "timestamp"ï¼šç”»é¢å‡ºç°çš„æ—¶é—´èŒƒå›´  
   - "narration"ï¼šè§£è¯´å†…å®¹
   - "OST": æ˜¯å¦å¼€å¯åŸå£°ï¼ˆtrue / falseï¼‰

**è¾“å…¥ç¤ºä¾‹ï¼š**  
```text  
åœ¨ä¸€ä¸ªï¿½ï¿½ï¿½æš—çš„å°å··ä¸­ï¼Œä¸»è§’ç¼“æ…¢èµ°è¿›ï¼Œå››å‘¨é™è°§æ— å£°ï¼Œåªæœ‰è¿œå¤„éšéšä¼ æ¥çŒ«çš„å«å£°ã€‚çªç„¶ï¼ŒèƒŒåå‡ºç°ä¸€ä¸ªç¥ç§˜çš„èº«å½±ã€‚  
```  

**è¾“å‡ºæ ¼å¼ï¼š**  
```json  
[  
    {  
        "picture": "é»‘æš—çš„å°å··ï¼Œä¸»è§’ç¼“æ…¢èµ°å…¥ï¼Œå››å‘¨å®‰é™ï¼Œè¿œå¤„ä¼ æ¥çŒ«å«å£°ã€‚",  
        "timestamp": "00:00-00:17",  
        "narration": "é™è°§çš„å°å··é‡Œï¼Œä¸»è§’æ­¥æ­¥å‰è¡Œï¼Œæ°”æ°›æ¸æ¸å˜å¾—å‹æŠ‘ã€‚"  
        "OST": False  
    },  
    {  
        "picture": "ç¥ç§˜èº«å½±çªç„¶å‡ºç°ï¼Œç´§å¼ æ°”æ°›åŠ å‰§ã€‚",  
        "timestamp": "00:17-00:39",  
        "narration": "åŸå£°æ’­æ”¾"  
        "OST": True  
    }  
]  
```  

**æç¤ºï¼š**  
- æ–‡æ¡ˆè¦ç®€çŸ­æœ‰åŠ›ï¼Œå¥‘åˆçŸ­è§†é¢‘å¹³å°ç”¨æˆ·çš„è§‚èµä¹ æƒ¯ã€‚  
- ä¿æŒå¼ºçƒˆçš„æ‚¬å¿µå’Œæƒ…æ„Ÿä»£å…¥ï¼Œå¸å¼•è§‚ä¼—ç»§ç»­è§‚çœ‹ã€‚  
- è§£è¯´ä¸€æ®µåæ’­æ”¾ä¸€æ®µåŸå£°ï¼ŒåŸå£°å†…å®¹å°½é‡å’Œè§£è¯´åŒ¹é…ã€‚
- æ–‡æ¡ˆè¯­è¨€ä¸ºï¼š%s  
- å‰§æƒ…å†…å®¹ï¼š%s (ä¸ºç©ºåˆ™å¿½ç•¥)  

""" % (language, video_plot)

    logger.debug(f"è§†é¢‘åç§°: {video_origin_name}")
    # try:
    gemini_video_file = gemini.upload_file(video_origin_path)
    logger.debug(f"ä¸Šä¼ è§†é¢‘è‡³ Google cloud æˆåŠŸ: {gemini_video_file.name}")
    while gemini_video_file.state.name == "PROCESSING":
        import time
        time.sleep(1)
        gemini_video_file = gemini.get_file(gemini_video_file.name)
        logger.debug(f"è§†é¢‘å½“å‰çŠ¶æ€(ACTIVEæ‰å¯ç”¨): {gemini_video_file.state.name}")
    if gemini_video_file.state.name == "FAILED":
        raise ValueError(gemini_video_file.state.name)
    # except Exception as err:
    #     logger.error(f"ä¸Šä¼ è§†é¢‘è‡³ Google cloud å¤±è´¥, è¯·æ£€æŸ¥ VPN é…ç½®å’Œ APIKey æ˜¯å¦æ­£ç¡® \n{traceback.format_exc()}")
    #     raise TimeoutError(f"ä¸Šä¼ è§†é¢‘è‡³ Google cloud å¤±è´¥, è¯·æ£€æŸ¥ VPN é…ç½®å’Œ APIKey æ˜¯å¦æ­£ç¡®; {err}")

    streams = model.generate_content([prompt, gemini_video_file], stream=True)
    response = []
    for chunk in streams:
        response.append(chunk.text)

    response = "".join(response)
    logger.success(f"llm response: \n{response}")

    return response


def writing_movie(video_plot, video_name, llm_provider):
    """
    å½±è§†è§£è¯´ï¼ˆç”µå½±è§£è¯´ï¼‰
    """
    prompt = f"""
    **è§’è‰²è®¾å®šï¼š**  
    ä½ æ˜¯ä¸€åæœ‰10å¹´ç»éªŒçš„å½±è§†è§£è¯´æ–‡æ¡ˆçš„åˆ›ä½œè€…ï¼Œ
    ä¸‹é¢æ˜¯å…³äºå¦‚ä½•å†™è§£è¯´æ–‡æ¡ˆçš„æ–¹æ³• {Method}ï¼Œè¯·è®¤çœŸé˜…è¯»å®ƒï¼Œä¹‹åæˆ‘ä¼šç»™ä½ ä¸€éƒ¨å½±è§†ä½œå“çš„åç§°ï¼Œç„¶åè®©ä½ å†™ä¸€ç¯‡æ–‡æ¡ˆ
    è¯·æ ¹æ®æ–¹æ³•æ’°å†™ ã€Š{video_name}ã€‹çš„å½±è§†è§£è¯´æ–‡æ¡ˆï¼Œã€Š{video_name}ã€‹çš„å¤§è‡´å‰§æƒ…å¦‚ä¸‹: {video_plot}
    æ–‡æ¡ˆè¦ç¬¦åˆä»¥ä¸‹è¦æ±‚:
    
    **ä»»åŠ¡ç›®æ ‡ï¼š**  
    1. æ–‡æ¡ˆå­—æ•°åœ¨ 1500å­—å·¦å³ï¼Œä¸¥æ ¼è¦æ±‚å­—æ•°ï¼Œæœ€ä½ä¸å¾—å°‘äº 1000å­—ã€‚
    2. é¿å…ä½¿ç”¨ markdown æ ¼å¼è¾“å‡ºæ–‡æ¡ˆã€‚  
    3. ä»…è¾“å‡ºè§£è¯´æ–‡æ¡ˆï¼Œä¸è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚
    4. ä¸è¦åŒ…å«å°æ ‡é¢˜ï¼Œæ¯ä¸ªæ®µè½ä»¥ \n è¿›è¡Œåˆ†éš”ã€‚
    """
    try:
        response = _generate_response(prompt, llm_provider)
        logger.success("è§£è¯´æ–‡æ¡ˆç”ŸæˆæˆåŠŸ")
        return response
    except Exception as err:
        return handle_exception(err)


def writing_short_play(video_plot: str, video_name: str, llm_provider: str, count: int = 500):
    """
    å½±è§†è§£è¯´ï¼ˆçŸ­å‰§è§£è¯´ï¼‰
    """
    if not video_plot:
        raise ValueError("çŸ­å‰§çš„ç®€ä»‹ä¸èƒ½ä¸ºç©º")
    if not video_name:
        raise ValueError("çŸ­å‰§åç§°ä¸èƒ½ä¸ºç©º")

    prompt = f"""
    **è§’è‰²è®¾å®šï¼š**  
    ä½ æ˜¯ä¸€åæœ‰10å¹´ç»éªŒçš„çŸ­å‰§è§£è¯´æ–‡æ¡ˆçš„åˆ›ä½œè€…ï¼Œ
    ä¸‹é¢æ˜¯å…³äºå¦‚ä½•å†™è§£è¯´æ–‡æ¡ˆçš„æ–¹æ³• {Method}ï¼Œè¯·è®¤çœŸé˜…è¯»å®ƒï¼Œä¹‹åæˆ‘ä¼šç»™ä½ ä¸€éƒ¨çŸ­å‰§ä½œå“çš„ç®€ä»‹ï¼Œç„¶åè®©ä½ å†™ä¸€ç¯‡è§£è¯´æ–‡æ¡ˆ
    è¯·æ ¹æ®æ–¹æ³•æ’°å†™ ã€Š{video_name}ã€‹çš„è§£è¯´æ–‡æ¡ˆï¼Œã€Š{video_name}ã€‹çš„å¤§è‡´å‰§æƒ…å¦‚ä¸‹: {video_plot}
    æ–‡æ¡ˆè¦ç¬¦åˆä»¥ä¸‹è¦æ±‚:

    **ä»»åŠ¡ç›®æ ‡ï¼š**  
    1. è¯·ä¸¥æ ¼è¦æ±‚æ–‡æ¡ˆå­—æ•°, å­—æ•°æ§åˆ¶åœ¨ {count} å­—å·¦å³ã€‚
    2. é¿å…ä½¿ç”¨ markdown æ ¼å¼è¾“å‡ºæ–‡æ¡ˆã€‚
    3. ä»…è¾“å‡ºè§£è¯´æ–‡æ¡ˆï¼Œä¸è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚
    4. ä¸è¦åŒ…å«å°æ ‡é¢˜ï¼Œæ¯ä¸ªæ®µè½ä»¥ \\n è¿›è¡Œåˆ†éš”ã€‚
    """
    try:
        response = _generate_response(prompt, llm_provider)
        logger.success("è§£è¯´æ–‡æ¡ˆç”ŸæˆæˆåŠŸ")
        logger.debug(response)
        return response
    except Exception as err:
        return handle_exception(err)


def screen_matching(huamian: str, wenan: str, llm_provider: str):
    """
    ç”»é¢åŒ¹é…ï¼ˆä¸€æ¬¡æ€§åŒ¹é…ï¼‰
    """
    if not huamian:
        raise ValueError("ç”»é¢ä¸èƒ½ä¸ºç©º")
    if not wenan:
        raise ValueError("æ–‡æ¡ˆä¸èƒ½ä¸ºç©º")

    prompt = """
    ä½ æ˜¯ä¸€åæœ‰10å¹´ç»éªŒçš„å½±è§†è§£è¯´åˆ›ä½œè€…ï¼Œ
    ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®è§†é¢‘è½¬å½•è„šæœ¬å’Œè§£è¯´æ–‡æ¡ˆï¼ŒåŒ¹é…å‡ºæ¯æ®µè§£è¯´æ–‡æ¡ˆå¯¹åº”çš„ç”»é¢æ—¶é—´æˆ³, ç»“æœä»¥ json æ ¼å¼è¾“å‡ºã€‚
    
    æ³¨æ„ï¼š
    è½¬å½•è„šæœ¬ä¸­ 
        - timestamp: è¡¨ç¤ºè§†é¢‘æ—¶é—´æˆ³
        - picture: è¡¨ç¤ºå½“å‰ç”»é¢æè¿°
        - speech": è¡¨ç¤ºå½“å‰è§†é¢‘ä¸­äººç‰©çš„å°è¯
    
    è½¬å½•è„šæœ¬å’Œæ–‡æ¡ˆï¼ˆç”± XML æ ‡è®°<PICTURE></PICTURE>å’Œ <COPYWRITER></COPYWRITER>åˆ†éš”ï¼‰å¦‚ä¸‹æ‰€ç¤ºï¼š
    <PICTURE>
    %s
    </PICTURE>
    
    <COPYWRITER>
    %s
    </COPYWRITER>

    åœ¨åŒ¹é…çš„è¿‡ç¨‹ä¸­ï¼Œè¯·é€šè¿‡ç¡®ä¿ä»¥ä¸‹æ¡ä»¶æ¥å®ŒæˆåŒ¹é…ï¼š
    - ä½¿ç”¨ä»¥ä¸‹ JSON schema:    
        script = {'picture': str, 'timestamp': str(æ—¶é—´æˆ³), "narration": str, "OST": bool(æ˜¯å¦å¼€å¯åŸå£°)}
        Return: list[script]
    - picture: å­—æ®µè¡¨ç¤ºå½“å‰ç”»é¢æè¿°ï¼Œä¸è½¬å½•è„šæœ¬ä¿æŒä¸€è‡´
    - timestamp: å­—æ®µè¡¨ç¤ºæŸä¸€æ®µæ–‡æ¡ˆå¯¹åº”çš„ç”»é¢çš„æ—¶é—´æˆ³ï¼Œä¸å¿…å’Œè½¬å½•è„šæœ¬çš„æ—¶é—´æˆ³ä¸€è‡´ï¼Œåº”è¯¥å……åˆ†è€ƒè™‘æ–‡æ¡ˆå†…å®¹ï¼ŒåŒ¹é…å‡ºä¸å…¶æè¿°æœ€åŒ¹é…çš„æ—¶é—´æˆ³
        - è¯·æ³¨æ„ï¼Œè¯·ä¸¥æ ¼çš„æ‰§è¡Œå·²ç»å‡ºç°çš„ç”»é¢ä¸èƒ½é‡å¤å‡ºç°ï¼Œå³ç”Ÿæˆçš„è„šæœ¬ä¸­ timestamp ä¸èƒ½æœ‰é‡å çš„éƒ¨åˆ†ã€‚
    - narration: å­—æ®µè¡¨ç¤ºéœ€è¦è§£è¯´æ–‡æ¡ˆï¼Œæ¯æ®µè§£è¯´æ–‡æ¡ˆå°½é‡ä¸è¦è¶…è¿‡30å­—
    - OST: å­—æ®µè¡¨ç¤ºæ˜¯å¦å¼€å¯åŸå£°ï¼Œå³å½“ OST å­—æ®µä¸º true æ—¶ï¼Œnarration å­—æ®µä¸ºç©ºå­—ç¬¦ä¸²ï¼Œå½“ OST ä¸º false æ—¶ï¼Œnarration å­—æ®µä¸ºå¯¹åº”çš„è§£è¯´æ–‡æ¡ˆ
    - æ³¨æ„ï¼Œåœ¨ç”»é¢åŒ¹é…çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦é€‚å½“çš„åŠ å…¥åŸå£°æ’­æ”¾ï¼Œä½¿å¾—è§£è¯´å’Œç”»é¢æ›´åŠ åŒ¹é…ï¼Œè¯·æŒ‰ç…§ 1:1 çš„æ¯”ä¾‹ï¼Œç”ŸæˆåŸå£°å’Œè§£è¯´çš„è„šæœ¬å†…å®¹ã€‚
    - æ³¨æ„ï¼Œåœ¨æ—¶é—´æˆ³åŒ¹é…ä¸Šï¼Œä¸€å®šä¸èƒ½åŸæ ·ç…§æ¬â€œè½¬å½•è„šæœ¬â€ï¼Œåº”å½“é€‚å½“çš„åˆå¹¶æˆ–è€…åˆ å‡ä¸€äº›ç‰‡æ®µã€‚
    - æ³¨æ„ï¼Œç¬¬ä¸€ä¸ªç”»é¢ä¸€å®šæ˜¯åŸå£°æ’­æ”¾å¹¶ä¸”æ—¶é•¿ä¸å°‘äº 20 sï¼Œä¸ºäº†å¸å¼•è§‚ä¼—ï¼Œç¬¬ä¸€æ®µä¸€å®šæ˜¯æ•´ä¸ªè½¬å½•è„šæœ¬ä¸­æœ€ç²¾å½©çš„ç‰‡æ®µã€‚
    - è¯·ä»¥ä¸¥æ ¼çš„ JSON æ ¼å¼è¿”å›æ•°æ®ï¼Œä¸è¦åŒ…å«ä»»ä½•æ³¨é‡Šã€æ ‡è®°æˆ–å…¶ä»–å­—ç¬¦ã€‚æ•°æ®åº”ç¬¦åˆ JSON è¯­æ³•ï¼Œå¯ä»¥è¢« json.loads() å‡½æ•°ç›´æ¥è§£æï¼Œ ä¸è¦æ·»åŠ  ```json æˆ–å…¶ä»–æ ‡è®°ã€‚
    """ % (huamian, wenan)

    try:
        response = _generate_response(prompt, llm_provider)
        logger.success("åŒ¹é…æˆåŠŸ")
        logger.debug(response)
        return response
    except Exception as err:
        return handle_exception(err)


if __name__ == "__main__":
    # 1. è§†é¢‘è½¬å½•
    video_subject = "ç¬¬äºŒåæ¡ä¹‹æ— ç½ªé‡Šæ”¾"
    video_path = "/Users/apple/Desktop/home/pipedream_project/downloads/jianzao.mp4"
    language = "zh-CN"
    gemini_video_transcription(
        video_name=video_subject,
        video_path=video_path,
        language=language,
        progress_callback=print,
        llm_provider_video="gemini"
    )

    # # 2. è§£è¯´æ–‡æ¡ˆ
    # video_path = "/Users/apple/Desktop/home/NarratoAI/resource/videos/1.mp4"
    # # video_path = "E:\\projects\\NarratoAI\\resource\\videos\\1.mp4"
    # video_plot = """
    #     æè‡ªå¿ æ‹¿ç€å„¿å­æç‰§åä¸‹çš„å­˜æŠ˜ï¼Œå»é“¶è¡Œå–é’±ç»™å„¿å­æ•‘å‘½ï¼Œå´è¢«è¦æ±‚è¯æ˜"ä½ å„¿å­æ˜¯ä½ å„¿å­"ã€‚
    # èµ°æŠ•æ— è·¯æ—¶ç¢°åˆ°é“¶è¡Œè¢«æŠ¢åŠ«ï¼ŒåŠ«åŒªç»™äº†ä»–ä¸¤æ²“é’±æ•‘å‘½ï¼Œæè‡ªå¿ å´å› æ­¤è¢«é“¶è¡Œä»¥æŠ¢åŠ«ç½ªèµ·è¯‰ï¼Œå¹¶é¡¶æ ¼åˆ¤å¤„20å¹´æœ‰æœŸå¾’åˆ‘ã€‚
    # è‹é†’åçš„æç‰§åšå†³ä¸ºçˆ¶äº²åšæ— ç½ªè¾©æŠ¤ï¼Œé¢å¯¹é“¶è¡Œçš„é¡¶çº§å¾‹å¸ˆå›¢é˜Ÿï¼Œä»–ä¸€ä¸ªæ³•å­¦é™¢å¤§ä¸€å­¦ç”Ÿï¼Œèƒ½å¦åŠ›æŒ½ç‹‚æ¾œï¼Œåˆ›ä½œå¥‡è¿¹ï¼ŸæŒ¥æ³•å¾‹ä¹‹åˆ©å‰‘ ï¼ŒæŒæ­£ä¹‰ä¹‹å¤©å¹³ï¼
    # """
    # res = generate_script(video_path, video_plot, video_name="ç¬¬äºŒåæ¡ä¹‹æ— ç½ªé‡Šæ”¾")
    # # res = generate_script(video_path, video_plot, video_name="æµ·å²¸")
    # print("è„šæœ¬ç”ŸæˆæˆåŠŸ:\n", res)
    # res = clean_model_output(res)
    # aaa = json.loads(res)
    # print(json.dumps(aaa, indent=2, ensure_ascii=False))
</file>

<file path="app/services/material.py">
import os
import subprocess
import random
import traceback
from urllib.parse import urlencode
from datetime import datetime

import requests
from typing import List
from loguru import logger
from moviepy.video.io.VideoFileClip import VideoFileClip

from app.config import config
from app.models.schema import VideoAspect, VideoConcatMode, MaterialInfo
from app.utils import utils

requested_count = 0


def get_api_key(cfg_key: str):
    api_keys = config.app.get(cfg_key)
    if not api_keys:
        raise ValueError(
            f"\n\n##### {cfg_key} is not set #####\n\nPlease set it in the config.toml file: {config.config_file}\n\n"
            f"{utils.to_json(config.app)}"
        )

    # if only one key is provided, return it
    if isinstance(api_keys, str):
        return api_keys

    global requested_count
    requested_count += 1
    return api_keys[requested_count % len(api_keys)]


def search_videos_pexels(
    search_term: str,
    minimum_duration: int,
    video_aspect: VideoAspect = VideoAspect.portrait,
) -> List[MaterialInfo]:
    aspect = VideoAspect(video_aspect)
    video_orientation = aspect.name
    video_width, video_height = aspect.to_resolution()
    api_key = get_api_key("pexels_api_keys")
    headers = {"Authorization": api_key}
    # Build URL
    params = {"query": search_term, "per_page": 20, "orientation": video_orientation}
    query_url = f"https://api.pexels.com/videos/search?{urlencode(params)}"
    logger.info(f"searching videos: {query_url}, with proxies: {config.proxy}")

    try:
        r = requests.get(
            query_url,
            headers=headers,
            proxies=config.proxy,
            verify=False,
            timeout=(30, 60),
        )
        response = r.json()
        video_items = []
        if "videos" not in response:
            logger.error(f"search videos failed: {response}")
            return video_items
        videos = response["videos"]
        # loop through each video in the result
        for v in videos:
            duration = v["duration"]
            # check if video has desired minimum duration
            if duration < minimum_duration:
                continue
            video_files = v["video_files"]
            # loop through each url to determine the best quality
            for video in video_files:
                w = int(video["width"])
                h = int(video["height"])
                if w == video_width and h == video_height:
                    item = MaterialInfo()
                    item.provider = "pexels"
                    item.url = video["link"]
                    item.duration = duration
                    video_items.append(item)
                    break
        return video_items
    except Exception as e:
        logger.error(f"search videos failed: {str(e)}")

    return []


def search_videos_pixabay(
    search_term: str,
    minimum_duration: int,
    video_aspect: VideoAspect = VideoAspect.portrait,
) -> List[MaterialInfo]:
    aspect = VideoAspect(video_aspect)

    video_width, video_height = aspect.to_resolution()

    api_key = get_api_key("pixabay_api_keys")
    # Build URL
    params = {
        "q": search_term,
        "video_type": "all",  # Accepted values: "all", "film", "animation"
        "per_page": 50,
        "key": api_key,
    }
    query_url = f"https://pixabay.com/api/videos/?{urlencode(params)}"
    logger.info(f"searching videos: {query_url}, with proxies: {config.proxy}")

    try:
        r = requests.get(
            query_url, proxies=config.proxy, verify=False, timeout=(30, 60)
        )
        response = r.json()
        video_items = []
        if "hits" not in response:
            logger.error(f"search videos failed: {response}")
            return video_items
        videos = response["hits"]
        # loop through each video in the result
        for v in videos:
            duration = v["duration"]
            # check if video has desired minimum duration
            if duration < minimum_duration:
                continue
            video_files = v["videos"]
            # loop through each url to determine the best quality
            for video_type in video_files:
                video = video_files[video_type]
                w = int(video["width"])
                h = int(video["height"])
                if w >= video_width:
                    item = MaterialInfo()
                    item.provider = "pixabay"
                    item.url = video["url"]
                    item.duration = duration
                    video_items.append(item)
                    break
        return video_items
    except Exception as e:
        logger.error(f"search videos failed: {str(e)}")

    return []


def save_video(video_url: str, save_dir: str = "") -> str:
    if not save_dir:
        save_dir = utils.storage_dir("cache_videos")

    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    url_without_query = video_url.split("?")[0]
    url_hash = utils.md5(url_without_query)
    video_id = f"vid-{url_hash}"
    video_path = f"{save_dir}/{video_id}.mp4"

    # if video already exists, return the path
    if os.path.exists(video_path) and os.path.getsize(video_path) > 0:
        logger.info(f"video already exists: {video_path}")
        return video_path

    # if video does not exist, download it
    with open(video_path, "wb") as f:
        f.write(
            requests.get(
                video_url, proxies=config.proxy, verify=False, timeout=(60, 240)
            ).content
        )

    if os.path.exists(video_path) and os.path.getsize(video_path) > 0:
        try:
            clip = VideoFileClip(video_path)
            duration = clip.duration
            fps = clip.fps
            clip.close()
            if duration > 0 and fps > 0:
                return video_path
        except Exception as e:
            try:
                os.remove(video_path)
            except Exception as e:
                logger.warning(f"æ— æ•ˆçš„è§†é¢‘æ–‡ä»¶: {video_path} => {str(e)}")
    return ""


def download_videos(
    task_id: str,
    search_terms: List[str],
    source: str = "pexels",
    video_aspect: VideoAspect = VideoAspect.portrait,
    video_contact_mode: VideoConcatMode = VideoConcatMode.random,
    audio_duration: float = 0.0,
    max_clip_duration: int = 5,
) -> List[str]:
    valid_video_items = []
    valid_video_urls = []
    found_duration = 0.0
    search_videos = search_videos_pexels
    if source == "pixabay":
        search_videos = search_videos_pixabay

    for search_term in search_terms:
        video_items = search_videos(
            search_term=search_term,
            minimum_duration=max_clip_duration,
            video_aspect=video_aspect,
        )
        logger.info(f"found {len(video_items)} videos for '{search_term}'")

        for item in video_items:
            if item.url not in valid_video_urls:
                valid_video_items.append(item)
                valid_video_urls.append(item.url)
                found_duration += item.duration

    logger.info(
        f"found total videos: {len(valid_video_items)}, required duration: {audio_duration} seconds, found duration: {found_duration} seconds"
    )
    video_paths = []

    material_directory = config.app.get("material_directory", "").strip()
    if material_directory == "task":
        material_directory = utils.task_dir(task_id)
    elif material_directory and not os.path.isdir(material_directory):
        material_directory = ""

    if video_contact_mode.value == VideoConcatMode.random.value:
        random.shuffle(valid_video_items)

    total_duration = 0.0
    for item in valid_video_items:
        try:
            logger.info(f"downloading video: {item.url}")
            saved_video_path = save_video(
                video_url=item.url, save_dir=material_directory
            )
            if saved_video_path:
                logger.info(f"video saved: {saved_video_path}")
                video_paths.append(saved_video_path)
                seconds = min(max_clip_duration, item.duration)
                total_duration += seconds
                if total_duration > audio_duration:
                    logger.info(
                        f"total duration of downloaded videos: {total_duration} seconds, skip downloading more"
                    )
                    break
        except Exception as e:
            logger.error(f"failed to download video: {utils.to_json(item)} => {str(e)}")
    logger.success(f"downloaded {len(video_paths)} videos")
    return video_paths


def time_to_seconds(time_str: str) -> float:
    """
    å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°
    æ”¯æŒæ ¼å¼: 'HH:MM:SS,mmm' (æ—¶:åˆ†:ç§’,æ¯«ç§’)
    
    Args:
        time_str: æ—¶é—´å­—ç¬¦ä¸²,å¦‚ "00:00:20,100"
        
    Returns:
        float: è½¬æ¢åçš„ç§’æ•°(åŒ…å«æ¯«ç§’)
    """
    try:
        # å¤„ç†æ¯«ç§’éƒ¨åˆ†
        if ',' in time_str:
            time_part, ms_part = time_str.split(',')
            ms = int(ms_part) / 1000
        else:
            time_part = time_str
            ms = 0

        # å¤„ç†æ—¶åˆ†ç§’
        parts = time_part.split(':')
        if len(parts) == 3:  # HH:MM:SS
            h, m, s = map(int, parts)
            seconds = h * 3600 + m * 60 + s
        else:
            raise ValueError("æ—¶é—´æ ¼å¼å¿…é¡»ä¸º HH:MM:SS,mmm")

        return seconds + ms
        
    except ValueError as e:
        logger.error(f"æ—¶é—´æ ¼å¼é”™è¯¯: {time_str}")
        raise ValueError(f"æ—¶é—´æ ¼å¼é”™è¯¯: å¿…é¡»ä¸º HH:MM:SS,mmm æ ¼å¼") from e


def format_timestamp(seconds: float) -> str:
    """
    å°†ç§’æ•°è½¬æ¢ä¸ºå¯è¯»çš„æ—¶é—´æ ¼å¼ (HH:MM:SS,mmm)
    
    Args:
        seconds: ç§’æ•°(å¯åŒ…å«æ¯«ç§’)
        
    Returns:
        str: æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²,å¦‚ "00:00:20,100"
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    seconds_remain = seconds % 60
    whole_seconds = int(seconds_remain)
    milliseconds = int((seconds_remain - whole_seconds) * 1000)
    
    return f"{hours:02d}:{minutes:02d}:{whole_seconds:02d},{milliseconds:03d}"


def save_clip_video(timestamp: str, origin_video: str, save_dir: str = "") -> dict:
    """
    ä¿å­˜å‰ªè¾‘åçš„è§†é¢‘
    
    Args:
        timestamp: éœ€è¦è£å‰ªçš„æ—¶é—´æˆ³,æ ¼å¼ä¸º 'HH:MM:SS,mmm-HH:MM:SS,mmm'
                  ä¾‹å¦‚: '00:00:00,000-00:00:20,100'
        origin_video: åŸè§†é¢‘è·¯å¾„
        save_dir: å­˜å‚¨ç›®å½•

    Returns:
        dict: è£å‰ªåçš„è§†é¢‘è·¯å¾„,æ ¼å¼ä¸º {timestamp: video_path}
    """
    # ä½¿ç”¨æ–°çš„è·¯å¾„ç»“æ„
    if not save_dir:
        base_dir = os.path.join(utils.temp_dir(), "clip_video")
        video_hash = utils.md5(origin_video)
        save_dir = os.path.join(base_dir, video_hash)

    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # ç”Ÿæˆæ›´è§„èŒƒçš„è§†é¢‘æ–‡ä»¶å
    video_id = f"vid-{timestamp.replace(':', '-').replace(',', '_')}"
    video_path = os.path.join(save_dir, f"{video_id}.mp4")

    if os.path.exists(video_path) and os.path.getsize(video_path) > 0:
        logger.info(f"video already exists: {video_path}")
        return {timestamp: video_path}

    try:
        # åŠ è½½è§†é¢‘è·å–æ€»æ—¶é•¿
        video = VideoFileClip(origin_video)
        total_duration = video.duration
        
        # è§£ææ—¶é—´æˆ³
        start_str, end_str = timestamp.split('-')
        start = time_to_seconds(start_str)
        end = time_to_seconds(end_str)
        
        # éªŒè¯æ—¶é—´æ®µ
        if start >= total_duration:
            logger.warning(f"èµ·å§‹æ—¶é—´ {format_timestamp(start)} ({start:.3f}ç§’) è¶…å‡ºè§†é¢‘æ€»æ—¶é•¿ {format_timestamp(total_duration)} ({total_duration:.3f}ç§’)")
            video.close()
            return {}
            
        if end > total_duration:
            logger.warning(f"ç»“æŸæ—¶é—´ {format_timestamp(end)} ({end:.3f}ç§’) è¶…å‡ºè§†é¢‘æ€»æ—¶é•¿ {format_timestamp(total_duration)} ({total_duration:.3f}ç§’)ï¼Œå°†è‡ªåŠ¨è°ƒæ•´ä¸ºè§†é¢‘ç»“å°¾")
            end = total_duration
            
        if end <= start:
            logger.warning(f"ç»“æŸæ—¶é—´ {format_timestamp(end)} å¿…é¡»å¤§äºèµ·å§‹æ—¶é—´ {format_timestamp(start)}")
            video.close()
            return {}
            
        # å‰ªè¾‘è§†é¢‘
        duration = end - start
        logger.info(f"å¼€å§‹å‰ªè¾‘è§†é¢‘: {format_timestamp(start)} - {format_timestamp(end)}ï¼Œæ—¶é•¿ {format_timestamp(duration)}")
        
        # å‰ªè¾‘è§†é¢‘
        subclip = video.subclip(start, end)
        
        try:
            # æ£€æŸ¥è§†é¢‘æ˜¯å¦æœ‰éŸ³é¢‘è½¨é“å¹¶å†™å…¥æ–‡ä»¶
            subclip.write_videofile(
                video_path,
                codec='libx264',
                audio_codec='aac',
                temp_audiofile='temp-audio.m4a',
                remove_temp=True,
                audio=(subclip.audio is not None),
                logger=None
            )
            
            # éªŒè¯ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶
            if os.path.exists(video_path) and os.path.getsize(video_path) > 0:
                with VideoFileClip(video_path) as clip:
                    if clip.duration > 0 and clip.fps > 0:
                        return {timestamp: video_path}
                    
            raise ValueError("è§†é¢‘æ–‡ä»¶éªŒè¯å¤±è´¥")
            
        except Exception as e:
            logger.warning(f"è§†é¢‘æ–‡ä»¶å¤„ç†å¤±è´¥: {video_path} => {str(e)}")
            if os.path.exists(video_path):
                os.remove(video_path)
                
    except Exception as e:
        logger.warning(f"è§†é¢‘å‰ªè¾‘å¤±è´¥: \n{str(traceback.format_exc())}")
        if os.path.exists(video_path):
            os.remove(video_path)
    finally:
        # ç¡®ä¿è§†é¢‘å¯¹è±¡è¢«æ­£ç¡®å…³é—­
        try:
            video.close()
            if 'subclip' in locals():
                subclip.close()
        except:
            pass
    
    return {}


def clip_videos(task_id: str, timestamp_terms: List[str], origin_video: str, progress_callback=None) -> dict:
    """
    å‰ªè¾‘è§†é¢‘
    Args:
        task_id: ä»»åŠ¡id
        timestamp_terms: éœ€è¦å‰ªè¾‘çš„æ—¶é—´æˆ³åˆ—è¡¨ï¼Œå¦‚:['00:00:00,000-00:00:20,100', '00:00:43,039-00:00:46,959']
        origin_video: åŸè§†é¢‘è·¯å¾„
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        å‰ªè¾‘åçš„è§†é¢‘è·¯å¾„
    """
    video_paths = {}
    total_items = len(timestamp_terms)
    for index, item in enumerate(timestamp_terms):
        material_directory = config.app.get("material_directory", "").strip()
        try:
            saved_video_path = save_clip_video(timestamp=item, origin_video=origin_video, save_dir=material_directory)
            if saved_video_path:
                logger.info(f"video saved: {saved_video_path}")
                video_paths.update(saved_video_path)
            
            # æ›´æ–°è¿›åº¦
            if progress_callback:
                progress_callback(index + 1, total_items)
        except Exception as e:
            logger.error(f"è§†é¢‘è£å‰ªå¤±è´¥: {utils.to_json(item)} =>\n{str(traceback.format_exc())}")
            return {}
            
    logger.success(f"è£å‰ª {len(video_paths)} videos")
    return video_paths


def merge_videos(video_paths, ost_list):
    """
    åˆå¹¶å¤šä¸ªè§†é¢‘ä¸ºä¸€ä¸ªè§†é¢‘ï¼Œå¯é€‰æ‹©æ˜¯å¦ä¿ç•™æ¯ä¸ªè§†é¢‘çš„åŸå£°ã€‚

    :param video_paths: è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    :param ost_list: æ˜¯å¦ä¿ç•™åŸå£°çš„å¸ƒå°”å€¼åˆ—è¡¨
    :return: åˆå¹¶åçš„è§†é¢‘æ–‡ä»¶è·¯å¾„
    """
    if len(video_paths) != len(ost_list):
        raise ValueError("è§†é¢‘è·¯å¾„åˆ—è¡¨å’Œä¿ç•™åŸå£°åˆ—è¡¨é•¿åº¦å¿…é¡»ç›¸åŒ")

    if not video_paths:
        raise ValueError("è§†é¢‘è·¯å¾„åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

    # å‡†å¤‡ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨
    temp_file = "temp_file_list.txt"
    with open(temp_file, "w") as f:
        for video_path, keep_ost in zip(video_paths, ost_list):
            if keep_ost:
                f.write(f"file '{video_path}'\n")
            else:
                # å¦‚æœä¸ä¿ç•™åŸå£°ï¼Œåˆ›å»ºä¸€ä¸ªæ— å£°çš„ä¸´æ—¶è§†é¢‘
                silent_video = f"silent_{os.path.basename(video_path)}"
                subprocess.run(["ffmpeg", "-i", video_path, "-c:v", "copy", "-an", silent_video], check=True)
                f.write(f"file '{silent_video}'\n")

    # åˆå¹¶è§†é¢‘
    output_file = "combined.mp4"
    ffmpeg_cmd = [
        "ffmpeg",
        "-f", "concat",
        "-safe", "0",
        "-i", temp_file,
        "-c:v", "copy",
        "-c:a", "aac",
        "-strict", "experimental",
        output_file
    ]

    try:
        subprocess.run(ffmpeg_cmd, check=True)
        print(f"è§†é¢‘åˆå¹¶æˆåŠŸï¼š{output_file}")
    except subprocess.CalledProcessError as e:
        print(f"è§†é¢‘åˆå¹¶å¤±è´¥ï¼š{e}")
        return None
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_file)
        for video_path, keep_ost in zip(video_paths, ost_list):
            if not keep_ost:
                silent_video = f"silent_{os.path.basename(video_path)}"
                if os.path.exists(silent_video):
                    os.remove(silent_video)

    return output_file
</file>

<file path="app/services/script_service.py">
import os
import json
import time
import asyncio
import requests
from loguru import logger
from typing import List, Dict, Any, Callable

from app.utils import utils, gemini_analyzer, video_processor, video_processor_v2
from app.utils.script_generator import ScriptProcessor
from app.config import config


class ScriptGenerator:
    def __init__(self):
        self.temp_dir = utils.temp_dir()
        self.keyframes_dir = os.path.join(self.temp_dir, "keyframes")
        
    async def generate_script(
        self,
        video_path: str,
        video_theme: str = "",
        custom_prompt: str = "",
        skip_seconds: int = 0,
        threshold: int = 30,
        vision_batch_size: int = 5,
        vision_llm_provider: str = "gemini",
        progress_callback: Callable[[float, str], None] = None
    ) -> List[Dict[Any, Any]]:
        """
        ç”Ÿæˆè§†é¢‘è„šæœ¬çš„æ ¸å¿ƒé€»è¾‘
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            video_theme: è§†é¢‘ä¸»é¢˜
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯
            skip_seconds: è·³è¿‡å¼€å§‹çš„ç§’æ•°
            threshold: å·®å¼‚ï¿½ï¿½ï¿½å€¼
            vision_batch_size: è§†è§‰å¤„ç†æ‰¹æ¬¡å¤§å°
            vision_llm_provider: è§†è§‰æ¨¡å‹æä¾›å•†
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            List[Dict]: ç”Ÿæˆçš„è§†é¢‘è„šæœ¬
        """
        if progress_callback is None:
            progress_callback = lambda p, m: None
            
        try:
            # æå–å…³é”®å¸§
            progress_callback(10, "æ­£åœ¨æå–å…³é”®å¸§...")
            keyframe_files = await self._extract_keyframes(
                video_path, 
                skip_seconds,
                threshold
            )
            
            if vision_llm_provider == "gemini":
                script = await self._process_with_gemini(
                    keyframe_files,
                    video_theme,
                    custom_prompt,
                    vision_batch_size,
                    progress_callback
                )
            elif vision_llm_provider == "narratoapi":
                script = await self._process_with_narrato(
                    keyframe_files,
                    video_theme,
                    custom_prompt,
                    vision_batch_size,
                    progress_callback
                )
            else:
                raise ValueError(f"Unsupported vision provider: {vision_llm_provider}")
                
            return json.loads(script) if isinstance(script, str) else script
            
        except Exception as e:
            logger.exception("Generate script failed")
            raise
            
    async def _extract_keyframes(
        self,
        video_path: str,
        skip_seconds: int,
        threshold: int
    ) -> List[str]:
        """æå–è§†é¢‘å…³é”®å¸§"""
        video_hash = utils.md5(video_path + str(os.path.getmtime(video_path)))
        video_keyframes_dir = os.path.join(self.keyframes_dir, video_hash)
        
        # æ£€æŸ¥ç¼“å­˜
        keyframe_files = []
        if os.path.exists(video_keyframes_dir):
            for filename in sorted(os.listdir(video_keyframes_dir)):
                if filename.endswith('.jpg'):
                    keyframe_files.append(os.path.join(video_keyframes_dir, filename))
                    
            if keyframe_files:
                logger.info(f"Using cached keyframes: {video_keyframes_dir}")
                return keyframe_files
                
        # æå–æ–°çš„å…³é”®å¸§
        os.makedirs(video_keyframes_dir, exist_ok=True)
        
        try:
            if config.frames.get("version") == "v2":
                processor = video_processor_v2.VideoProcessor(video_path)
                processor.process_video_pipeline(
                    output_dir=video_keyframes_dir,
                    skip_seconds=skip_seconds,
                    threshold=threshold
                )
            else:
                processor = video_processor.VideoProcessor(video_path)
                processor.process_video(
                    output_dir=video_keyframes_dir,
                    skip_seconds=skip_seconds
                )
                
            for filename in sorted(os.listdir(video_keyframes_dir)):
                if filename.endswith('.jpg'):
                    keyframe_files.append(os.path.join(video_keyframes_dir, filename))
                    
            return keyframe_files
            
        except Exception as e:
            if os.path.exists(video_keyframes_dir):
                import shutil
                shutil.rmtree(video_keyframes_dir)
            raise
            
    async def _process_with_gemini(
        self,
        keyframe_files: List[str],
        video_theme: str,
        custom_prompt: str,
        vision_batch_size: int,
        progress_callback: Callable[[float, str], None]
    ) -> str:
        """ä½¿ç”¨Geminiå¤„ç†è§†é¢‘å¸§"""
        progress_callback(30, "æ­£åœ¨åˆå§‹åŒ–è§†è§‰åˆ†æå™¨...")
        
        # è·å–Geminié…ç½®
        vision_api_key = config.app.get("vision_gemini_api_key")
        vision_model = config.app.get("vision_gemini_model_name")
        
        if not vision_api_key or not vision_model:
            raise ValueError("æœªé…ç½® Gemini API Key æˆ–è€…æ¨¡å‹")

        analyzer = gemini_analyzer.VisionAnalyzer(
            model_name=vision_model,
            api_key=vision_api_key,
        )

        progress_callback(40, "æ­£åœ¨åˆ†æå…³é”®å¸§...")

        # æ‰§è¡Œå¼‚æ­¥åˆ†æ
        results = await analyzer.analyze_images(
            images=keyframe_files,
            prompt=config.app.get('vision_analysis_prompt'),
            batch_size=vision_batch_size
        )

        progress_callback(60, "æ­£åœ¨æ•´ç†åˆ†æç»“æœ...")
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„åˆ†æç»“æœ
        frame_analysis = ""
        prev_batch_files = None

        for result in results:
            if 'error' in result:
                logger.warning(f"æ‰¹æ¬¡ {result['batch_index']} å¤„ç†å‡ºç°è­¦å‘Š: {result['error']}")
                continue
                
            batch_files = self._get_batch_files(keyframe_files, result, vision_batch_size)
            first_timestamp, last_timestamp, _ = self._get_batch_timestamps(batch_files, prev_batch_files)
            
            # æ·»åŠ å¸¦æ—¶é—´æˆ³çš„åˆ†ï¿½ï¿½ç»“æœ
            frame_analysis += f"\n=== {first_timestamp}-{last_timestamp} ===\n"
            frame_analysis += result['response']
            frame_analysis += "\n"
            
            prev_batch_files = batch_files
        
        if not frame_analysis.strip():
            raise Exception("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å¸§åˆ†æç»“æœ")
        
        progress_callback(70, "æ­£åœ¨ç”Ÿæˆè„šæœ¬...")

        # æ„å»ºå¸§å†…å®¹åˆ—è¡¨
        frame_content_list = []
        prev_batch_files = None

        for result in results:
            if 'error' in result:
                continue
            
            batch_files = self._get_batch_files(keyframe_files, result, vision_batch_size)
            _, _, timestamp_range = self._get_batch_timestamps(batch_files, prev_batch_files)
            
            frame_content = {
                "timestamp": timestamp_range,
                "picture": result['response'],
                "narration": "",
                "OST": 2
            }
            frame_content_list.append(frame_content)
            prev_batch_files = batch_files

        if not frame_content_list:
            raise Exception("æ²¡æœ‰æœ‰æ•ˆçš„å¸§å†…å®¹å¯ä»¥å¤„ç†")

        progress_callback(90, "æ­£åœ¨ç”Ÿæˆæ–‡æ¡ˆ...")
        
        # è·å–æ–‡æœ¬ç”Ÿï¿½ï¿½é…ç½®
        text_provider = config.app.get('text_llm_provider', 'gemini').lower()
        text_api_key = config.app.get(f'text_{text_provider}_api_key')
        text_model = config.app.get(f'text_{text_provider}_model_name')

        processor = ScriptProcessor(
            model_name=text_model,
            api_key=text_api_key,
            prompt=custom_prompt,
            video_theme=video_theme
        )

        return processor.process_frames(frame_content_list)

    async def _process_with_narrato(
        self,
        keyframe_files: List[str],
        video_theme: str,
        custom_prompt: str,
        vision_batch_size: int,
        progress_callback: Callable[[float, str], None]
    ) -> str:
        """ä½¿ç”¨NarratoAPIå¤„ç†è§†é¢‘å¸§"""
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = utils.temp_dir("narrato")
        
        # æ‰“åŒ…å…³é”®å¸§
        progress_callback(30, "æ­£åœ¨æ‰“åŒ…å…³é”®å¸§...")
        zip_path = os.path.join(temp_dir, f"keyframes_{int(time.time())}.zip")
        
        try:
            if not utils.create_zip(keyframe_files, zip_path):
                raise Exception("æ‰“åŒ…å…³é”®å¸§å¤±è´¥")
            
            # è·å–APIé…ç½®
            api_url = config.app.get("narrato_api_url")
            api_key = config.app.get("narrato_api_key")
            
            if not api_key:
                raise ValueError("æœªé…ç½® Narrato API Key")
            
            headers = {
                'X-API-Key': api_key,
                'accept': 'application/json'
            }
            
            api_params = {
                'batch_size': vision_batch_size,
                'use_ai': False,
                'start_offset': 0,
                'vision_model': config.app.get('narrato_vision_model', 'gemini-1.5-flash'),
                'vision_api_key': config.app.get('narrato_vision_key'),
                'llm_model': config.app.get('narrato_llm_model', 'qwen-plus'),
                'llm_api_key': config.app.get('narrato_llm_key'),
                'custom_prompt': custom_prompt
            }
            
            progress_callback(40, "æ­£åœ¨ä¸Šä¼ æ–‡ä»¶...")
            with open(zip_path, 'rb') as f:
                files = {'file': (os.path.basename(zip_path), f, 'application/x-zip-compressed')}
                response = requests.post(
                    f"{api_url}/video/analyze",
                    headers=headers, 
                    params=api_params, 
                    files=files,
                    timeout=30
                )
                response.raise_for_status()
            
            task_data = response.json()
            task_id = task_data["data"].get('task_id')
            if not task_id:
                raise Exception(f"æ— æ•ˆçš„APIï¿½ï¿½åº”: {response.text}")
            
            progress_callback(50, "æ­£åœ¨ç­‰å¾…åˆ†æç»“æœ...")
            retry_count = 0
            max_retries = 60
            
            while retry_count < max_retries:
                try:
                    status_response = requests.get(
                        f"{api_url}/video/tasks/{task_id}",
                        headers=headers,
                        timeout=10
                    )
                    status_response.raise_for_status()
                    task_status = status_response.json()['data']
                    
                    if task_status['status'] == 'SUCCESS':
                        return task_status['result']['data']
                    elif task_status['status'] in ['FAILURE', 'RETRY']:
                        raise Exception(f"ä»»åŠ¡å¤±è´¥: {task_status.get('error')}")
                    
                    retry_count += 1
                    time.sleep(2)
                    
                except requests.RequestException as e:
                    logger.warning(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥ï¼Œé‡è¯•ä¸­: {str(e)}")
                    retry_count += 1
                    time.sleep(2)
                    continue
            
            raise Exception("ä»»åŠ¡æ‰§è¡Œè¶…æ—¶")
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                if os.path.exists(zip_path):
                    os.remove(zip_path)
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")

    def _get_batch_files(
        self, 
        keyframe_files: List[str], 
        result: Dict[str, Any], 
        batch_size: int
    ) -> List[str]:
        """è·å–å½“å‰æ‰¹æ¬¡çš„å›¾ç‰‡æ–‡ä»¶"""
        batch_start = result['batch_index'] * batch_size
        batch_end = min(batch_start + batch_size, len(keyframe_files))
        return keyframe_files[batch_start:batch_end]

    def _get_batch_timestamps(
        self, 
        batch_files: List[str], 
        prev_batch_files: List[str] = None
    ) -> tuple[str, str, str]:
        """è·å–ä¸€æ‰¹æ–‡ä»¶çš„æ—¶é—´æˆ³èŒƒå›´ï¼Œæ”¯æŒæ¯«ç§’çº§ç²¾åº¦"""
        if not batch_files:
            logger.warning("Empty batch files")
            return "00:00:00,000", "00:00:00,000", "00:00:00,000-00:00:00,000"
            
        if len(batch_files) == 1 and prev_batch_files and len(prev_batch_files) > 0:
            first_frame = os.path.basename(prev_batch_files[-1])
            last_frame = os.path.basename(batch_files[0])
        else:
            first_frame = os.path.basename(batch_files[0])
            last_frame = os.path.basename(batch_files[-1])
        
        first_time = first_frame.split('_')[2].replace('.jpg', '')
        last_time = last_frame.split('_')[2].replace('.jpg', '')
        
        def format_timestamp(time_str: str) -> str:
            """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸º HH:MM:SS,mmm æ ¼å¼"""
            try:
                if len(time_str) < 4:
                    logger.warning(f"Invalid timestamp format: {time_str}")
                    return "00:00:00,000"
                
                # å¤„ç†æ¯«ç§’éƒ¨åˆ†
                if ',' in time_str:
                    time_part, ms_part = time_str.split(',')
                    ms = int(ms_part)
                else:
                    time_part = time_str
                    ms = 0
                
                # å¤„ç†æ—¶åˆ†ç§’
                parts = time_part.split(':')
                if len(parts) == 3:  # HH:MM:SS
                    h, m, s = map(int, parts)
                elif len(parts) == 2:  # MM:SS
                    h = 0
                    m, s = map(int, parts)
                else:  # SS
                    h = 0
                    m = 0
                    s = int(parts[0])
                    
                # å¤„ç†è¿›ä½
                if s >= 60:
                    m += s // 60
                    s = s % 60
                if m >= 60:
                    h += m // 60
                    m = m % 60
                    
                return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"
                
            except Exception as e:
                logger.error(f"æ—¶é—´æˆ³æ ¼å¼è½¬æ¢é”™è¯¯ {time_str}: {str(e)}")
                return "00:00:00,000"
        
        first_timestamp = format_timestamp(first_time)
        last_timestamp = format_timestamp(last_time)
        timestamp_range = f"{first_timestamp}-{last_timestamp}"
        
        return first_timestamp, last_timestamp, timestamp_range
</file>

<file path="app/services/state.py">
import ast
from abc import ABC, abstractmethod
from app.config import config
from app.models import const


# Base class for state management
class BaseState(ABC):
    @abstractmethod
    def update_task(self, task_id: str, state: int, progress: int = 0, **kwargs):
        pass

    @abstractmethod
    def get_task(self, task_id: str):
        pass


# Memory state management
class MemoryState(BaseState):
    def __init__(self):
        self._tasks = {}

    def update_task(
        self,
        task_id: str,
        state: int = const.TASK_STATE_PROCESSING,
        progress: int = 0,
        **kwargs,
    ):
        progress = int(progress)
        if progress > 100:
            progress = 100

        self._tasks[task_id] = {
            "state": state,
            "progress": progress,
            **kwargs,
        }

    def get_task(self, task_id: str):
        return self._tasks.get(task_id, None)

    def delete_task(self, task_id: str):
        if task_id in self._tasks:
            del self._tasks[task_id]


# Redis state management
class RedisState(BaseState):
    def __init__(self, host="localhost", port=6379, db=0, password=None):
        import redis

        self._redis = redis.StrictRedis(host=host, port=port, db=db, password=password)

    def update_task(
        self,
        task_id: str,
        state: int = const.TASK_STATE_PROCESSING,
        progress: int = 0,
        **kwargs,
    ):
        progress = int(progress)
        if progress > 100:
            progress = 100

        fields = {
            "state": state,
            "progress": progress,
            **kwargs,
        }

        for field, value in fields.items():
            self._redis.hset(task_id, field, str(value))

    def get_task(self, task_id: str):
        task_data = self._redis.hgetall(task_id)
        if not task_data:
            return None

        task = {
            key.decode("utf-8"): self._convert_to_original_type(value)
            for key, value in task_data.items()
        }
        return task

    def delete_task(self, task_id: str):
        self._redis.delete(task_id)

    @staticmethod
    def _convert_to_original_type(value):
        """
        Convert the value from byte string to its original data type.
        You can extend this method to handle other data types as needed.
        """
        value_str = value.decode("utf-8")

        try:
            # try to convert byte string array to list
            return ast.literal_eval(value_str)
        except (ValueError, SyntaxError):
            pass

        if value_str.isdigit():
            return int(value_str)
        # Add more conversions here if needed
        return value_str


# Global state
_enable_redis = config.app.get("enable_redis", False)
_redis_host = config.app.get("redis_host", "localhost")
_redis_port = config.app.get("redis_port", 6379)
_redis_db = config.app.get("redis_db", 0)
_redis_password = config.app.get("redis_password", None)

state = (
    RedisState(
        host=_redis_host, port=_redis_port, db=_redis_db, password=_redis_password
    )
    if _enable_redis
    else MemoryState()
)
</file>

<file path="app/services/subtitle.py">
import json
import os.path
import re
import traceback
from typing import Optional

from faster_whisper import WhisperModel
from timeit import default_timer as timer
from loguru import logger
import google.generativeai as genai
from moviepy.editor import VideoFileClip
import os

from app.config import config
from app.utils import utils

model_size = config.whisper.get("model_size", "faster-whisper-large-v2")
device = config.whisper.get("device", "cpu")
compute_type = config.whisper.get("compute_type", "int8")
model = None


def create(audio_file, subtitle_file: str = ""):
    """
    ä¸ºç»™å®šçš„éŸ³é¢‘æ–‡ä»¶åˆ›å»ºå­—å¹•æ–‡ä»¶ã€‚

    å‚æ•°:
    - audio_file: éŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„ã€‚
    - subtitle_file: å­—å¹•æ–‡ä»¶çš„è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œå°†æ ¹æ®éŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„ç”Ÿæˆå­—å¹•æ–‡ä»¶ã€‚

    è¿”å›:
    æ— è¿”å›å€¼ï¼Œä½†ä¼šåœ¨æŒ‡å®šè·¯å¾„ç”Ÿæˆå­—å¹•æ–‡ä»¶ã€‚
    """
    global model, device, compute_type
    if not model:
        model_path = f"{utils.root_dir()}/app/models/faster-whisper-large-v2"
        model_bin_file = f"{model_path}/model.bin"
        if not os.path.isdir(model_path) or not os.path.isfile(model_bin_file):
            logger.error(
                "è¯·å…ˆä¸‹è½½ whisper æ¨¡å‹\n\n"
                "********************************************\n"
                "ä¸‹è½½åœ°å€ï¼šhttps://huggingface.co/guillaumekln/faster-whisper-large-v2\n"
                "å­˜æ”¾è·¯å¾„ï¼šapp/models \n"
                "********************************************\n"
            )
            return None

        # å°è¯•ä½¿ç”¨ CUDAï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ° CPU
        try:
            import torch
            if torch.cuda.is_available():
                try:
                    logger.info(f"å°è¯•ä½¿ç”¨ CUDA åŠ è½½æ¨¡å‹: {model_path}")
                    model = WhisperModel(
                        model_size_or_path=model_path,
                        device="cuda",
                        compute_type="float16",
                        local_files_only=True
                    )
                    device = "cuda"
                    compute_type = "float16"
                    logger.info("æˆåŠŸä½¿ç”¨ CUDA åŠ è½½æ¨¡å‹")
                except Exception as e:
                    logger.warning(f"CUDA åŠ è½½å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {str(e)}")
                    logger.warning("å›é€€åˆ° CPU æ¨¡å¼")
                    device = "cpu"
                    compute_type = "int8"
            else:
                logger.info("æœªæ£€æµ‹åˆ° CUDAï¼Œä½¿ç”¨ CPU æ¨¡å¼")
                device = "cpu"
                compute_type = "int8"
        except ImportError:
            logger.warning("æœªå®‰è£… torchï¼Œä½¿ç”¨ CPU æ¨¡å¼")
            device = "cpu"
            compute_type = "int8"

        if device == "cpu":
            logger.info(f"ä½¿ç”¨ CPU åŠ è½½æ¨¡å‹: {model_path}")
            model = WhisperModel(
                model_size_or_path=model_path,
                device=device,
                compute_type=compute_type,
                local_files_only=True
            )

        logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}, è®¡ç®—ç±»å‹: {compute_type}")

    logger.info(f"start, output file: {subtitle_file}")
    if not subtitle_file:
        subtitle_file = f"{audio_file}.srt"

    segments, info = model.transcribe(
        audio_file,
        beam_size=5,
        word_timestamps=True,
        vad_filter=True,
        vad_parameters=dict(min_silence_duration_ms=500),
        initial_prompt="ä»¥ä¸‹æ˜¯æ™®é€šè¯çš„å¥å­"
    )

    logger.info(
        f"æ£€æµ‹åˆ°çš„è¯­è¨€: '{info.language}', probability: {info.language_probability:.2f}"
    )

    start = timer()
    subtitles = []

    def recognized(seg_text, seg_start, seg_end):
        seg_text = seg_text.strip()
        if not seg_text:
            return

        msg = "[%.2fs -> %.2fs] %s" % (seg_start, seg_end, seg_text)
        logger.debug(msg)

        subtitles.append(
            {"msg": seg_text, "start_time": seg_start, "end_time": seg_end}
        )

    for segment in segments:
        words_idx = 0
        words_len = len(segment.words)

        seg_start = 0
        seg_end = 0
        seg_text = ""

        if segment.words:
            is_segmented = False
            for word in segment.words:
                if not is_segmented:
                    seg_start = word.start
                    is_segmented = True

                seg_end = word.end
                # å¦‚æœåŒ…å«æ ‡ç‚¹,åˆ™æ–­å¥
                seg_text += word.word

                if utils.str_contains_punctuation(word.word):
                    # remove last char
                    seg_text = seg_text[:-1]
                    if not seg_text:
                        continue

                    recognized(seg_text, seg_start, seg_end)

                    is_segmented = False
                    seg_text = ""

                if words_idx == 0 and segment.start < word.start:
                    seg_start = word.start
                if words_idx == (words_len - 1) and segment.end > word.end:
                    seg_end = word.end
                words_idx += 1

        if not seg_text:
            continue

        recognized(seg_text, seg_start, seg_end)

    end = timer()

    diff = end - start
    logger.info(f"complete, elapsed: {diff:.2f} s")

    idx = 1
    lines = []
    for subtitle in subtitles:
        text = subtitle.get("msg")
        if text:
            lines.append(
                utils.text_to_srt(
                    idx, text, subtitle.get("start_time"), subtitle.get("end_time")
                )
            )
            idx += 1

    sub = "\n".join(lines) + "\n"
    with open(subtitle_file, "w", encoding="utf-8") as f:
        f.write(sub)
    logger.info(f"subtitle file created: {subtitle_file}")


def file_to_subtitles(filename):
    """
    å°†å­—å¹•æ–‡ä»¶è½¬æ¢ä¸ºå­—å¹•åˆ—è¡¨ã€‚

    å‚æ•°:
    filename (str): å­—å¹•æ–‡ä»¶çš„è·¯å¾„ã€‚

    è¿”å›:
    list: åŒ…å«å­—å¹•åºå·ã€å‡ºç°æ—¶é—´ã€å’Œå­—å¹•æ–‡æœ¬çš„å…ƒç»„åˆ—è¡¨ã€‚
    """
    if not filename or not os.path.isfile(filename):
        return []

    times_texts = []
    current_times = None
    current_text = ""
    index = 0
    with open(filename, "r", encoding="utf-8") as f:
        for line in f:
            times = re.findall("([0-9]*:[0-9]*:[0-9]*,[0-9]*)", line)
            if times:
                current_times = line
            elif line.strip() == "" and current_times:
                index += 1
                times_texts.append((index, current_times.strip(), current_text.strip()))
                current_times, current_text = None, ""
            elif current_times:
                current_text += line
    return times_texts


def levenshtein_distance(s1, s2):
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)

    if len(s2) == 0:
        return len(s1)

    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row

    return previous_row[-1]


def similarity(a, b):
    distance = levenshtein_distance(a.lower(), b.lower())
    max_length = max(len(a), len(b))
    return 1 - (distance / max_length)


def correct(subtitle_file, video_script):
    subtitle_items = file_to_subtitles(subtitle_file)
    script_lines = utils.split_string_by_punctuations(video_script)

    corrected = False
    new_subtitle_items = []
    script_index = 0
    subtitle_index = 0

    while script_index < len(script_lines) and subtitle_index < len(subtitle_items):
        script_line = script_lines[script_index].strip()
        subtitle_line = subtitle_items[subtitle_index][2].strip()

        if script_line == subtitle_line:
            new_subtitle_items.append(subtitle_items[subtitle_index])
            script_index += 1
            subtitle_index += 1
        else:
            combined_subtitle = subtitle_line
            start_time = subtitle_items[subtitle_index][1].split(" --> ")[0]
            end_time = subtitle_items[subtitle_index][1].split(" --> ")[1]
            next_subtitle_index = subtitle_index + 1

            while next_subtitle_index < len(subtitle_items):
                next_subtitle = subtitle_items[next_subtitle_index][2].strip()
                if similarity(
                    script_line, combined_subtitle + " " + next_subtitle
                ) > similarity(script_line, combined_subtitle):
                    combined_subtitle += " " + next_subtitle
                    end_time = subtitle_items[next_subtitle_index][1].split(" --> ")[1]
                    next_subtitle_index += 1
                else:
                    break

            if similarity(script_line, combined_subtitle) > 0.8:
                logger.warning(
                    f"Merged/Corrected - Script: {script_line}, Subtitle: {combined_subtitle}"
                )
                new_subtitle_items.append(
                    (
                        len(new_subtitle_items) + 1,
                        f"{start_time} --> {end_time}",
                        script_line,
                    )
                )
                corrected = True
            else:
                logger.warning(
                    f"Mismatch - Script: {script_line}, Subtitle: {combined_subtitle}"
                )
                new_subtitle_items.append(
                    (
                        len(new_subtitle_items) + 1,
                        f"{start_time} --> {end_time}",
                        script_line,
                    )
                )
                corrected = True

            script_index += 1
            subtitle_index = next_subtitle_index

    # å¤„ç†å‰©ä½™çš„è„šæœ¬è¡Œ
    while script_index < len(script_lines):
        logger.warning(f"Extra script line: {script_lines[script_index]}")
        if subtitle_index < len(subtitle_items):
            new_subtitle_items.append(
                (
                    len(new_subtitle_items) + 1,
                    subtitle_items[subtitle_index][1],
                    script_lines[script_index],
                )
            )
            subtitle_index += 1
        else:
            new_subtitle_items.append(
                (
                    len(new_subtitle_items) + 1,
                    "00:00:00,000 --> 00:00:00,000",
                    script_lines[script_index],
                )
            )
        script_index += 1
        corrected = True

    if corrected:
        with open(subtitle_file, "w", encoding="utf-8") as fd:
            for i, item in enumerate(new_subtitle_items):
                fd.write(f"{i + 1}\n{item[1]}\n{item[2]}\n\n")
        logger.info("Subtitle corrected")
    else:
        logger.success("Subtitle is correct")


def create_with_gemini(audio_file: str, subtitle_file: str = "", api_key: Optional[str] = None) -> Optional[str]:
    if not api_key:
        logger.error("Gemini API key is not provided")
        return None

    genai.configure(api_key=api_key)

    logger.info(f"å¼€å§‹ä½¿ç”¨Geminiæ¨¡å‹å¤„ç†éŸ³é¢‘æ–‡ä»¶: {audio_file}")
    
    model = genai.GenerativeModel(model_name="gemini-1.5-flash")
    prompt = "ç”Ÿæˆè¿™æ®µè¯­éŸ³çš„è½¬å½•æ–‡æœ¬ã€‚è¯·ä»¥SRTæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«æ—¶é—´æˆ³ã€‚"

    try:
        with open(audio_file, "rb") as f:
            audio_data = f.read()
        
        response = model.generate_content([prompt, audio_data])
        transcript = response.text

        if not subtitle_file:
            subtitle_file = f"{audio_file}.srt"

        with open(subtitle_file, "w", encoding="utf-8") as f:
            f.write(transcript)

        logger.info(f"Geminiç”Ÿæˆçš„å­—å¹•æ–‡ä»¶å·²ä¿å­˜: {subtitle_file}")
        return subtitle_file
    except Exception as e:
        logger.error(f"ä½¿ç”¨Geminiå¤„ç†éŸ³é¢‘æ—¶å‡ºé”™: {e}")
        return None


def extract_audio_and_create_subtitle(video_file: str, subtitle_file: str = "") -> Optional[str]:
    """
    ä»è§†é¢‘æ–‡ä»¶ä¸­æå–éŸ³é¢‘å¹¶ç”Ÿæˆå­—å¹•æ–‡ä»¶ã€‚

    å‚æ•°:
    - video_file: MP4è§†é¢‘æ–‡ä»¶çš„è·¯å¾„
    - subtitle_file: è¾“å‡ºå­—å¹•æ–‡ä»¶çš„è·¯å¾„ï¼ˆå¯é€‰ï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œå°†æ ¹æ®è§†é¢‘æ–‡ä»¶åè‡ªåŠ¨ç”Ÿæˆã€‚

    è¿”å›:
    - str: ç”Ÿæˆçš„å­—å¹•æ–‡ä»¶è·¯å¾„
    - None: å¦‚æœå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯
    """
    try:
        # è·å–è§†é¢‘æ–‡ä»¶æ‰€åœ¨ç›®å½•
        video_dir = os.path.dirname(video_file)
        video_name = os.path.splitext(os.path.basename(video_file))[0]
        
        # è®¾ç½®éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        audio_file = os.path.join(video_dir, f"{video_name}_audio.wav")
        
        # å¦‚æœæœªæŒ‡å®šå­—å¹•æ–‡ä»¶è·¯å¾„ï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆ
        if not subtitle_file:
            subtitle_file = os.path.join(video_dir, f"{video_name}.srt")
        
        logger.info(f"å¼€å§‹ä»è§†é¢‘æå–éŸ³é¢‘: {video_file}")
        
        # åŠ è½½è§†é¢‘æ–‡ä»¶
        video = VideoFileClip(video_file)
        
        # æå–éŸ³é¢‘å¹¶ä¿å­˜ä¸ºWAVæ ¼å¼
        logger.info(f"æ­£åœ¨æå–éŸ³é¢‘åˆ°: {audio_file}")
        video.audio.write_audiofile(audio_file, codec='pcm_s16le')
        
        # å…³é—­è§†é¢‘æ–‡ä»¶
        video.close()
        
        logger.info("éŸ³é¢‘æå–å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆå­—å¹•")
        
        # ä½¿ç”¨createå‡½æ•°ç”Ÿæˆå­—å¹•
        create(audio_file, subtitle_file)
        
        # åˆ é™¤ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
        if os.path.exists(audio_file):
            os.remove(audio_file)
            logger.info("å·²æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶")
        
        return subtitle_file
        
    except Exception as e:
        logger.error(f"å¤„ç†è§†é¢‘æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        logger.error(traceback.format_exc())
        return None


if __name__ == "__main__":
    task_id = "123456"
    task_dir = utils.task_dir(task_id)
    subtitle_file = f"{task_dir}/subtitle_123456.srt"
    audio_file = f"{task_dir}/audio.wav"
    video_file = "/Users/apple/Desktop/home/NarratoAI/resource/videos/merged_video_1702.mp4"

    extract_audio_and_create_subtitle(video_file, subtitle_file)

    # subtitles = file_to_subtitles(subtitle_file)
    # print(subtitles)

    # # script_file = f"{task_dir}/script.json"
    # # with open(script_file, "r") as f:
    # #     script_content = f.read()
    # # s = json.loads(script_content)
    # # script = s.get("script")
    # #
    # # correct(subtitle_file, script)

    # subtitle_file = f"{task_dir}/subtitle111.srt"
    # create(audio_file, subtitle_file)

    # # # ä½¿ç”¨Geminiæ¨¡å‹å¤„ç†éŸ³é¢‘
    # # gemini_api_key = config.app.get("gemini_api_key")  # è¯·æ›¿æ¢ä¸ºå®é™…çš„APIå¯†é’¥
    # # gemini_subtitle_file = create_with_gemini(audio_file, api_key=gemini_api_key)
    # #
    # # if gemini_subtitle_file:
    # #     print(f"Geminiç”Ÿæˆçš„å­—å¹•æ–‡ä»¶: {gemini_subtitle_file}")
</file>

<file path="app/services/task.py">
import math
import json
import os.path
import re
import traceback
from os import path
from loguru import logger

from app.config import config
from app.models import const
from app.models.schema import VideoConcatMode, VideoParams, VideoClipParams
from app.services import llm, material, subtitle, video, voice, audio_merger
from app.services import state as sm
from app.utils import utils


def generate_script(task_id, params):
    logger.info("\n\n## generating video script")
    video_script = params.video_script.strip()
    if not video_script:
        video_script = llm.generate_script(
            video_subject=params.video_subject,
            language=params.video_language,
            paragraph_number=params.paragraph_number,
        )
    else:
        logger.debug(f"video script: \n{video_script}")

    if not video_script:
        sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
        logger.error("failed to generate video script.")
        return None

    return video_script


def generate_terms(task_id, params, video_script):
    logger.info("\n\n## generating video terms")
    video_terms = params.video_terms
    if not video_terms:
        video_terms = llm.generate_terms(
            video_subject=params.video_subject, video_script=video_script, amount=5
        )
    else:
        if isinstance(video_terms, str):
            video_terms = [term.strip() for term in re.split(r"[,ï¼Œ]", video_terms)]
        elif isinstance(video_terms, list):
            video_terms = [term.strip() for term in video_terms]
        else:
            raise ValueError("video_terms must be a string or a list of strings.")

        logger.debug(f"video terms: {utils.to_json(video_terms)}")

    if not video_terms:
        sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
        logger.error("failed to generate video terms.")
        return None

    return video_terms


def save_script_data(task_id, video_script, video_terms, params):
    script_file = path.join(utils.task_dir(task_id), "script.json")
    script_data = {
        "script": video_script,
        "search_terms": video_terms,
        "params": params,
    }

    with open(script_file, "w", encoding="utf-8") as f:
        f.write(utils.to_json(script_data))


def generate_audio(task_id, params, video_script):
    logger.info("\n\n## generating audio")
    audio_file = path.join(utils.task_dir(task_id), "audio.mp3")
    sub_maker = voice.tts(
        text=video_script,
        voice_name=voice.parse_voice_name(params.voice_name),
        voice_rate=params.voice_rate,
        voice_file=audio_file,
    )
    if sub_maker is None:
        sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
        logger.error(
            """failed to generate audio:
1. check if the language of the voice matches the language of the video script.
2. check if the network is available. If you are in China, it is recommended to use a VPN and enable the global traffic mode.
        """.strip()
        )
        return None, None, None

    audio_duration = math.ceil(voice.get_audio_duration(sub_maker))
    return audio_file, audio_duration, sub_maker


def generate_subtitle(task_id, params, video_script, sub_maker, audio_file):
    if not params.subtitle_enabled:
        return ""

    subtitle_path = path.join(utils.task_dir(task_id), "subtitle111.srt")
    subtitle_provider = config.app.get("subtitle_provider", "").strip().lower()
    logger.info(f"\n\n## generating subtitle, provider: {subtitle_provider}")

    subtitle_fallback = False
    if subtitle_provider == "edge":
        voice.create_subtitle(
            text=video_script, sub_maker=sub_maker, subtitle_file=subtitle_path
        )
        if not os.path.exists(subtitle_path):
            subtitle_fallback = True
            logger.warning("subtitle file not found, fallback to whisper")

    if subtitle_provider == "whisper" or subtitle_fallback:
        subtitle.create(audio_file=audio_file, subtitle_file=subtitle_path)
        logger.info("\n\n## correcting subtitle")
        subtitle.correct(subtitle_file=subtitle_path, video_script=video_script)

    subtitle_lines = subtitle.file_to_subtitles(subtitle_path)
    if not subtitle_lines:
        logger.warning(f"subtitle file is invalid: {subtitle_path}")
        return ""

    return subtitle_path


def get_video_materials(task_id, params, video_terms, audio_duration):
    if params.video_source == "local":
        logger.info("\n\n## preprocess local materials")
        materials = video.preprocess_video(
            materials=params.video_materials, clip_duration=params.video_clip_duration
        )
        if not materials:
            sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
            logger.error(
                "no valid materials found, please check the materials and try again."
            )
            return None
        return [material_info.url for material_info in materials]
    else:
        logger.info(f"\n\n## downloading videos from {params.video_source}")
        downloaded_videos = material.download_videos(
            task_id=task_id,
            search_terms=video_terms,
            source=params.video_source,
            video_aspect=params.video_aspect,
            video_contact_mode=params.video_concat_mode,
            audio_duration=audio_duration * params.video_count,
            max_clip_duration=params.video_clip_duration,
        )
        if not downloaded_videos:
            sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
            logger.error(
                "failed to download videos, maybe the network is not available. if you are in China, please use a VPN."
            )
            return None
        return downloaded_videos


def start_subclip(task_id: str, params: VideoClipParams, subclip_path_videos: dict):
    """åå°ä»»åŠ¡ï¼ˆè‡ªåŠ¨å‰ªè¾‘è§†é¢‘è¿›è¡Œå‰ªè¾‘ï¼‰"""
    logger.info(f"\n\n## å¼€å§‹ä»»åŠ¡: {task_id}")
    
    # åˆå§‹åŒ– ImageMagick
    if not utils.init_imagemagick():
        logger.warning("ImageMagick åˆå§‹åŒ–å¤±è´¥ï¼Œå­—å¹•å¯èƒ½æ— æ³•æ­£å¸¸æ˜¾ç¤º")
    
    sm.state.update_task(task_id, state=const.TASK_STATE_PROCESSING, progress=5)

    # tts è§’è‰²åç§°
    voice_name = voice.parse_voice_name(params.voice_name)

    logger.info("\n\n## 1. åŠ è½½è§†é¢‘è„šæœ¬")
    video_script_path = path.join(params.video_clip_json_path)
    
    if path.exists(video_script_path):
        try:
            with open(video_script_path, "r", encoding="utf-8") as f:
                list_script = json.load(f)
                video_list = [i['narration'] for i in list_script]
                video_ost = [i['OST'] for i in list_script]
                time_list = [i['timestamp'] for i in list_script]

                video_script = " ".join(video_list)
                logger.debug(f"è§£è¯´å®Œæ•´è„šæœ¬: \n{video_script}")
                logger.debug(f"è§£è¯´ OST åˆ—è¡¨: \n{video_ost}")
                logger.debug(f"è§£è¯´æ—¶é—´æˆ³åˆ—è¡¨: \n{time_list}")
                
                # è·å–è§†é¢‘æ€»æ—¶é•¿(å•ä½ s)
                last_timestamp = list_script[-1]['new_timestamp']
                end_time = last_timestamp.split("-")[1]
                total_duration = utils.time_to_seconds(end_time)
                
        except Exception as e:
            logger.error(f"æ— æ³•è¯»å–è§†é¢‘jsonè„šæœ¬ï¼Œè¯·æ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®ã€‚{e}")
            raise ValueError("æ— æ³•è¯»å–è§†é¢‘jsonè„šæœ¬ï¼Œè¯·æ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®")
    else:
        logger.error(f"video_script_path: {video_script_path} \n\n", traceback.format_exc())
        raise ValueError("è§£è¯´è„šæœ¬ä¸å­˜åœ¨ï¼è¯·æ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®ã€‚")

    logger.info("\n\n## 2. æ ¹æ®OSTè®¾ç½®ç”ŸæˆéŸ³é¢‘åˆ—è¡¨")
    # åªä¸ºOST=0æˆ–2çš„ç‰‡æ®µç”ŸæˆTTSéŸ³é¢‘
    tts_segments = [
        segment for segment in list_script 
        if segment['OST'] in [0, 2]
    ]
    logger.debug(f"éœ€è¦ç”ŸæˆTTSçš„ç‰‡æ®µæ•°: {len(tts_segments)}")
    
    # åˆå§‹åŒ–éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    audio_files = []
    final_audio = ""
    
    if tts_segments:
        audio_files, sub_maker_list = voice.tts_multiple(
            task_id=task_id,
            list_script=tts_segments,  # åªä¼ å…¥éœ€è¦TTSçš„ç‰‡æ®µ
            voice_name=voice_name,
            voice_rate=params.voice_rate,
            voice_pitch=params.voice_pitch,
            force_regenerate=True
        )
        if audio_files is None:
            sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
            logger.error("TTSè½¬æ¢éŸ³é¢‘å¤±è´¥, å¯èƒ½æ˜¯ç½‘ç»œä¸å¯ç”¨! å¦‚æœæ‚¨åœ¨ä¸­å›½, è¯·ä½¿ç”¨VPN.")
            return

        if audio_files:
            logger.info(f"åˆå¹¶éŸ³é¢‘æ–‡ä»¶: {audio_files}")
            try:
                # ä¼ å…¥OSTä¿¡æ¯ä»¥ä¾¿æ­£ç¡®å¤„ç†éŸ³é¢‘
                final_audio = audio_merger.merge_audio_files(
                    task_id=task_id,
                    audio_files=audio_files,
                    total_duration=total_duration,
                    list_script=list_script  # ä¼ å…¥å®Œæ•´è„šæœ¬ä»¥ä¾¿å¤„ç†OST
                )
                logger.info("éŸ³é¢‘æ–‡ä»¶åˆå¹¶æˆåŠŸ")
            except Exception as e:
                logger.error(f"åˆå¹¶éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {str(e)}")
                final_audio = ""
    else:
        # å¦‚æœæ²¡æœ‰éœ€è¦ç”ŸæˆTTSçš„ç‰‡æ®µï¼Œåˆ›å»ºä¸€ä¸ªç©ºç™½éŸ³é¢‘æ–‡ä»¶
        # è¿™æ ·å¯ä»¥ç¡®ä¿åç»­çš„éŸ³é¢‘å¤„ç†èƒ½æ­£ç¡®è¿›è¡Œ
        logger.info("æ²¡æœ‰éœ€è¦ç”ŸæˆTTSçš„ç‰‡æ®µï¼Œå°†ä¿ç•™åŸå£°å’ŒèƒŒæ™¯éŸ³ä¹")
        final_audio = path.join(utils.task_dir(task_id), "empty.mp3")
        try:
            from moviepy.editor import AudioClip
            # åˆ›å»ºä¸€ä¸ªä¸è§†é¢‘ç­‰é•¿çš„ç©ºç™½éŸ³é¢‘
            empty_audio = AudioClip(make_frame=lambda t: 0, duration=total_duration)
            empty_audio.write_audiofile(final_audio, fps=44100)
            logger.info(f"å·²åˆ›å»ºç©ºç™½éŸ³é¢‘æ–‡ä»¶: {final_audio}")
        except Exception as e:
            logger.error(f"åˆ›å»ºç©ºç™½éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {str(e)}")
            final_audio = ""

    sm.state.update_task(task_id, state=const.TASK_STATE_PROCESSING, progress=30)

    subtitle_path = ""
    if params.subtitle_enabled:
        if audio_files:
            subtitle_path = path.join(utils.task_dir(task_id), f"subtitle.srt")
            subtitle_provider = config.app.get("subtitle_provider", "").strip().lower()
            logger.info(f"\n\n## 3. ç”Ÿæˆå­—å¹•ã€æä¾›ç¨‹åºæ˜¯: {subtitle_provider}")

            subtitle.create(
                audio_file=final_audio,
                subtitle_file=subtitle_path,
            )

            subtitle_lines = subtitle.file_to_subtitles(subtitle_path)
            if not subtitle_lines:
                logger.warning(f"å­—å¹•æ–‡ä»¶æ— æ•ˆ: {subtitle_path}")
                subtitle_path = ""

    sm.state.update_task(task_id, state=const.TASK_STATE_PROCESSING, progress=40)

    logger.info("\n\n## 4. è£å‰ªè§†é¢‘")
    subclip_videos = [x for x in subclip_path_videos.values()]
    # logger.debug(f"\n\n## è£å‰ªåçš„è§†é¢‘æ–‡ä»¶åˆ—è¡¨: \n{subclip_videos}")

    if not subclip_videos:
        sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
        logger.error(
            "è£å‰ªè§†é¢‘å¤±è´¥ï¼Œå¯èƒ½æ˜¯ ImageMagick ä¸å¯ç”¨")
        return

    sm.state.update_task(task_id, state=const.TASK_STATE_PROCESSING, progress=50)

    final_video_paths = []
    combined_video_paths = []

    _progress = 50
    index = 1
    combined_video_path = path.join(utils.task_dir(task_id), f"combined.mp4")
    logger.info(f"\n\n## 5. åˆå¹¶è§†é¢‘: => {combined_video_path}")

    video.combine_clip_videos(
        combined_video_path=combined_video_path,
        video_paths=subclip_videos,
        video_ost_list=video_ost,
        list_script=list_script,
        video_aspect=params.video_aspect,
        threads=params.n_threads  # å¤šçº¿ç¨‹
    )

    _progress += 50 / 2
    sm.state.update_task(task_id, progress=_progress)

    final_video_path = path.join(utils.task_dir(task_id), f"final-{index}.mp4")

    logger.info(f"\n\n## 6. æœ€ååˆæˆ: {index} => {final_video_path}")
    
    # è·å–èƒŒæ™¯éŸ³ä¹
    bgm_path = None
    if params.bgm_type or params.bgm_file:
        try:
            bgm_path = utils.get_bgm_file(bgm_type=params.bgm_type, bgm_file=params.bgm_file)
            if bgm_path:
                logger.info(f"ä½¿ç”¨èƒŒæ™¯éŸ³ä¹: {bgm_path}")
        except Exception as e:
            logger.error(f"è·å–èƒŒæ™¯éŸ³ä¹å¤±è´¥: {str(e)}")

    # ç¤ºä¾‹ï¼šè‡ªå®šä¹‰å­—å¹•æ ·å¼
    subtitle_style = {
        'fontsize': params.font_size,  # å­—ä½“å¤§å°
        'color': params.text_fore_color,  # å­—ä½“é¢œè‰²
        'stroke_color': params.stroke_color,  # æè¾¹é¢œè‰²
        'stroke_width': params.stroke_width,  # æè¾¹å®½åº¦, èŒƒå›´0-10
        'bg_color': params.text_back_color,   # åŠé€æ˜é»‘è‰²èƒŒæ™¯
        'position': (params.subtitle_position, 0.2),  # è·ç¦»é¡¶éƒ¨60%çš„ä½ç½®
        'method': 'caption'  # æ¸²æŸ“æ–¹æ³•
    }

    # ç¤ºä¾‹ï¼šè‡ªå®šä¹‰éŸ³é‡é…ç½®
    volume_config = {
        'original': params.original_volume,  # åŸå£°éŸ³é‡80%
        'bgm': params.bgm_volume,  # BGMéŸ³é‡20%
        'narration': params.tts_volume or params.voice_volume,  # è§£è¯´éŸ³é‡100%
    }
    font_path = utils.font_dir(params.font_name)
    video.generate_video_v3(
        video_path=combined_video_path,
        subtitle_path=subtitle_path,
        bgm_path=bgm_path,
        narration_path=final_audio,
        output_path=final_video_path,
        volume_config=volume_config,  # æ·»åŠ éŸ³é‡é…ç½®
        subtitle_style=subtitle_style,
        font_path=font_path
    )

    _progress += 50 / 2
    sm.state.update_task(task_id, progress=_progress)

    final_video_paths.append(final_video_path)
    combined_video_paths.append(combined_video_path)

    logger.success(f"ä»»åŠ¡ {task_id} å·²å®Œæˆ, ç”Ÿæˆ {len(final_video_paths)} ä¸ªè§†é¢‘.")

    kwargs = {
        "videos": final_video_paths,
        "combined_videos": combined_video_paths
    }
    sm.state.update_task(task_id, state=const.TASK_STATE_COMPLETE, progress=100, **kwargs)
    return kwargs


def validate_params(video_path, audio_path, output_file, params):
    """
    éªŒè¯è¾“å…¥å‚æ•°
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        params: è§†é¢‘å‚æ•°

    Raises:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨æ—¶æŠ›å‡º
        ValueError: å‚æ•°æ— æ•ˆæ—¶æŠ›å‡º
    """
    if not video_path:
        raise ValueError("è§†é¢‘è·¯å¾„ä¸èƒ½ä¸ºç©º")
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
    # å¦‚æœæä¾›äº†éŸ³é¢‘è·¯å¾„ï¼Œåˆ™éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if audio_path and not os.path.exists(audio_path):
        raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
        
    if not output_file:
        raise ValueError("è¾“å‡ºæ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_file)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    if not params:
        raise ValueError("è§†é¢‘å‚æ•°ä¸èƒ½ä¸ºç©º")


if __name__ == "__main__":
    # task_id = "test123"
    # subclip_path_videos = {'00:41-01:58': 'E:\\projects\\NarratoAI\\storage\\cache_videos/vid-00_41-01_58.mp4',
    #                        '00:06-00:15': 'E:\\projects\\NarratoAI\\storage\\cache_videos/vid-00_06-00_15.mp4',
    #                        '01:10-01:17': 'E:\\projects\\NarratoAI\\storage\\cache_videos/vid-01_10-01_17.mp4',
    #                        '00:47-01:03': 'E:\\projects\\NarratoAI\\storage\\cache_videos/vid-00_47-01_03.mp4',
    #                        '01:03-01:10': 'E:\\projects\\NarratoAI\\storage\\cache_videos/vid-01_03-01_10.mp4',
    #                        '02:40-03:08': 'E:\\projects\\NarratoAI\\storage\\cache_videos/vid-02_40-03_08.mp4',
    #                        '03:02-03:20': 'E:\\projects\\NarratoAI\\storage\\cache_videos/vid-03_02-03_20.mp4',
    #                        '03:18-03:20': 'E:\\projects\\NarratoAI\\storage\\cache_videos/vid-03_18-03_20.mp4'}
    #
    # params = VideoClipParams(
    #     video_clip_json_path="E:\\projects\\NarratoAI\\resource/scripts/test003.json",
    #     video_origin_path="E:\\projects\\NarratoAI\\resource/videos/1.mp4",
    # )
    # start_subclip(task_id, params, subclip_path_videos=subclip_path_videos)

    task_id = "test456"
    subclip_path_videos = {'01:10-01:17': './storage/cache_videos/vid-01_10-01_17.mp4',
                           '01:58-02:04': './storage/cache_videos/vid-01_58-02_04.mp4',
                           '02:25-02:31': './storage/cache_videos/vid-02_25-02_31.mp4',
                           '01:28-01:33': './storage/cache_videos/vid-01_28-01_33.mp4',
                           '03:14-03:18': './storage/cache_videos/vid-03_14-03_18.mp4',
                           '00:24-00:28': './storage/cache_videos/vid-00_24-00_28.mp4',
                           '03:02-03:08': './storage/cache_videos/vid-03_02-03_08.mp4',
                           '00:41-00:44': './storage/cache_videos/vid-00_41-00_44.mp4',
                           '02:12-02:25': './storage/cache_videos/vid-02_12-02_25.mp4'}

    params = VideoClipParams(
        video_clip_json_path="/Users/apple/Desktop/home/NarratoAI/resource/scripts/test004.json",
        video_origin_path="/Users/apple/Desktop/home/NarratoAI/resource/videos/1.mp4",
    )
    start_subclip(task_id, params, subclip_path_videos=subclip_path_videos)
</file>

<file path="app/services/video_service.py">
import os
from uuid import uuid4
from loguru import logger
from typing import Dict, List, Optional, Tuple

from app.services import material
from app.models.schema import VideoClipParams
from app.utils import utils


class VideoService:
    @staticmethod
    async def crop_video(
        video_path: str,
        video_script: List[dict]
    ) -> Tuple[str, Dict[str, str]]:
        """
        è£å‰ªè§†é¢‘æœåŠ¡
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            video_script: è§†é¢‘è„šæœ¬åˆ—è¡¨
            
        Returns:
            Tuple[str, Dict[str, str]]: (task_id, è£å‰ªåçš„è§†é¢‘ç‰‡æ®µå­—å…¸)
            è§†é¢‘ç‰‡æ®µå­—å…¸æ ¼å¼: {timestamp: video_path}
        """
        try:
            task_id = str(uuid4())
            
            # ä»è„šæœ¬ä¸­æå–æ—¶é—´æˆ³åˆ—è¡¨
            time_list = [scene['timestamp'] for scene in video_script]
            
            # è°ƒç”¨è£å‰ªæœåŠ¡
            subclip_videos = material.clip_videos(
                task_id=task_id,
                timestamp_terms=time_list,
                origin_video=video_path
            )
            
            if subclip_videos is None:
                raise ValueError("è£å‰ªè§†é¢‘å¤±è´¥")
                
            # æ›´æ–°è„šæœ¬ä¸­çš„è§†é¢‘è·¯å¾„
            for scene in video_script:
                try:
                    scene['path'] = subclip_videos[scene['timestamp']]
                except KeyError as err:
                    logger.error(f"æ›´æ–°è§†é¢‘è·¯å¾„å¤±è´¥: {err}")
                    
            logger.debug(f"è£å‰ªè§†é¢‘æˆåŠŸï¼Œå…±ç”Ÿæˆ {len(time_list)} ä¸ªè§†é¢‘ç‰‡æ®µ")
            logger.debug(f"è§†é¢‘ç‰‡æ®µè·¯å¾„: {subclip_videos}")
            
            return task_id, subclip_videos
            
        except Exception as e:
            logger.exception("è£å‰ªè§†é¢‘å¤±è´¥")
            raise
</file>

<file path="app/services/video.py">
import traceback

import pysrt
from typing import Optional
from typing import List
from loguru import logger
from moviepy.editor import *
from PIL import ImageFont
from contextlib import contextmanager
from moviepy.editor import (
    VideoFileClip,
    AudioFileClip,
    TextClip,
    CompositeVideoClip,
    CompositeAudioClip
)


from app.models.schema import VideoAspect, SubtitlePosition


def wrap_text(text, max_width, font, fontsize=60):
    """
    æ–‡æœ¬è‡ªåŠ¨æ¢è¡Œå¤„ç†
    Args:
        text: å¾…å¤„ç†çš„æ–‡æœ¬
        max_width: æœ€å¤§å®½åº¦
        font: å­—ä½“æ–‡ä»¶è·¯å¾„
        fontsize: å­—ä½“å¤§å°

    Returns:
        tuple: (æ¢è¡Œåçš„æ–‡æœ¬, æ–‡æœ¬é«˜åº¦)
    """
    # åˆ›å»ºå­—ä½“å¯¹è±¡
    font = ImageFont.truetype(font, fontsize)

    def get_text_size(inner_text):
        inner_text = inner_text.strip()
        left, top, right, bottom = font.getbbox(inner_text)
        return right - left, bottom - top

    width, height = get_text_size(text)
    if width <= max_width:
        return text, height

    logger.debug(f"æ¢è¡Œæ–‡æœ¬, æœ€å¤§å®½åº¦: {max_width}, æ–‡æœ¬å®½åº¦: {width}, æ–‡æœ¬: {text}")

    processed = True

    _wrapped_lines_ = []
    words = text.split(" ")
    _txt_ = ""
    for word in words:
        _before = _txt_
        _txt_ += f"{word} "
        _width, _height = get_text_size(_txt_)
        if _width <= max_width:
            continue
        else:
            if _txt_.strip() == word.strip():
                processed = False
                break
            _wrapped_lines_.append(_before)
            _txt_ = f"{word} "
    _wrapped_lines_.append(_txt_)
    if processed:
        _wrapped_lines_ = [line.strip() for line in _wrapped_lines_]
        result = "\n".join(_wrapped_lines_).strip()
        height = len(_wrapped_lines_) * height
        # logger.warning(f"wrapped text: {result}")
        return result, height

    _wrapped_lines_ = []
    chars = list(text)
    _txt_ = ""
    for word in chars:
        _txt_ += word
        _width, _height = get_text_size(_txt_)
        if _width <= max_width:
            continue
        else:
            _wrapped_lines_.append(_txt_)
            _txt_ = ""
    _wrapped_lines_.append(_txt_)
    result = "\n".join(_wrapped_lines_).strip()
    height = len(_wrapped_lines_) * height
    logger.debug(f"æ¢è¡Œæ–‡æœ¬: {result}")
    return result, height


@contextmanager
def manage_clip(clip):
    """
    è§†é¢‘ç‰‡æ®µèµ„æºç®¡ç†å™¨
    Args:
        clip: è§†é¢‘ç‰‡æ®µå¯¹è±¡

    Yields:
        VideoFileClip: è§†é¢‘ç‰‡æ®µå¯¹è±¡
    """
    try:
        yield clip
    finally:
        clip.close()
        del clip


def combine_clip_videos(combined_video_path: str,
                        video_paths: List[str],
                        video_ost_list: List[int],
                        list_script: list,
                        video_aspect: VideoAspect = VideoAspect.portrait,
                        threads: int = 2,
                        ) -> str:
    """
    åˆå¹¶å­è§†é¢‘
    Args:
        combined_video_path: åˆå¹¶åçš„å­˜å‚¨è·¯å¾„
        video_paths: å­è§†é¢‘è·¯å¾„åˆ—è¡¨
        video_ost_list: åŸå£°æ’­æ”¾åˆ—è¡¨ (0: ä¸ä¿ç•™åŸå£°, 1: åªä¿ç•™åŸå£°, 2: ä¿ç•™åŸå£°å¹¶ä¿ç•™è§£è¯´)
        list_script: å‰ªè¾‘è„šæœ¬
        video_aspect: å±å¹•æ¯”ä¾‹
        threads: çº¿ç¨‹æ•°

    Returns:
        str: åˆå¹¶åçš„è§†é¢‘è·¯å¾„
    """
    from app.utils.utils import calculate_total_duration
    audio_duration = calculate_total_duration(list_script)
    logger.info(f"éŸ³é¢‘çš„æœ€å¤§æŒç»­æ—¶é—´: {audio_duration} s")

    output_dir = os.path.dirname(combined_video_path)
    aspect = VideoAspect(video_aspect)
    video_width, video_height = aspect.to_resolution()

    clips = []
    for video_path, video_ost in zip(video_paths, video_ost_list):
        try:
            clip = VideoFileClip(video_path)

            if video_ost == 0:  # ä¸ä¿ç•™åŸå£°
                clip = clip.without_audio()
            # video_ost ä¸º 1 æˆ– 2 æ—¶éƒ½ä¿ç•™åŸå£°ï¼Œä¸éœ€è¦ç‰¹æ®Šå¤„ç†

            clip = clip.set_fps(30)

            # å¤„ç†è§†é¢‘å°ºå¯¸
            clip_w, clip_h = clip.size
            if clip_w != video_width or clip_h != video_height:
                clip = resize_video_with_padding(
                    clip,
                    target_width=video_width,
                    target_height=video_height
                )
                logger.info(f"è§†é¢‘ {video_path} å·²è°ƒæ•´å°ºå¯¸ä¸º {video_width} x {video_height}")

            clips.append(clip)

        except Exception as e:
            logger.error(f"å¤„ç†è§†é¢‘ {video_path} æ—¶å‡ºé”™: {str(e)}")
            continue

    if not clips:
        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„è§†é¢‘ç‰‡æ®µå¯ä»¥åˆå¹¶")

    try:
        video_clip = concatenate_videoclips(clips)
        video_clip = video_clip.set_fps(30)

        logger.info("å¼€å§‹åˆå¹¶è§†é¢‘... (è¿‡ç¨‹ä¸­å‡ºç° UserWarning: ä¸å¿…ç†ä¼š)")
        video_clip.write_videofile(
            filename=combined_video_path,
            threads=threads,
            audio_codec="aac",
            fps=30,
            temp_audiofile=os.path.join(output_dir, "temp-audio.m4a")
        )
    finally:
        # ç¡®ä¿èµ„æºè¢«æ­£ç¡®æ”¾
        video_clip.close()
        for clip in clips:
            clip.close()

    logger.success("è§†é¢‘åˆå¹¶å®Œæˆ")
    return combined_video_path


def resize_video_with_padding(clip, target_width: int, target_height: int):
    """
    è°ƒæ•´è§†é¢‘å°ºå¯¸å¹¶æ·»åŠ é»‘è¾¹
    Args:
        clip: è§†é¢‘ç‰‡æ®µ
        target_width: ç›®æ ‡å®½åº¦
        target_height: ç›®æ ‡é«˜åº¦

    Returns:
        CompositeVideoClip: è°ƒæ•´å°ºå¯¸åçš„è§†é¢‘
    """
    clip_ratio = clip.w / clip.h
    target_ratio = target_width / target_height

    if clip_ratio == target_ratio:
        return clip.resize((target_width, target_height))

    if clip_ratio > target_ratio:
        scale_factor = target_width / clip.w
    else:
        scale_factor = target_height / clip.h

    new_width = int(clip.w * scale_factor)
    new_height = int(clip.h * scale_factor)
    clip_resized = clip.resize(newsize=(new_width, new_height))

    background = ColorClip(
        size=(target_width, target_height),
        color=(0, 0, 0)
    ).set_duration(clip.duration)

    return CompositeVideoClip([
        background,
        clip_resized.set_position("center")
    ])


def loop_audio_clip(audio_clip: AudioFileClip, target_duration: float) -> AudioFileClip:
    """
    å¾ªç¯éŸ³é¢‘ç‰‡æ®µç›´åˆ°è¾¾åˆ°ç›®æ ‡æ—¶é•¿

    å‚æ•°:
        audio_clip: åŸå§‹éŸ³é¢‘ç‰‡æ®µ
        target_duration: ç›®æ ‡æ—¶é•¿ï¼ˆç§’ï¼‰
    è¿”å›:
        å¾ªç¯åçš„éŸ³é¢‘ç‰‡æ®µ
    """
    # è®¡ç®—éœ€è¦å¾ªç¯çš„æ¬¡æ•°
    loops_needed = int(target_duration / audio_clip.duration) + 1

    # åˆ›å»ºè¶³å¤Ÿé•¿çš„éŸ³é¢‘
    extended_audio = audio_clip
    for _ in range(loops_needed - 1):
        extended_audio = CompositeAudioClip([
            extended_audio,
            audio_clip.set_start(extended_audio.duration)
        ])

    # è£å‰ªåˆ°ç›®æ ‡æ—¶é•¿
    return extended_audio.subclip(0, target_duration)


def calculate_subtitle_position(position, video_height: int, text_height: int = 0) -> tuple:
    """
    è®¡ç®—å­—å¹•åœ¨è§†é¢‘ä¸­çš„å…·ä½“ä½ç½®
    
    Args:
        position: ä½ç½®é…ç½®ï¼Œå¯ä»¥æ˜¯ SubtitlePosition æšä¸¾å€¼æˆ–è¡¨ç¤ºè·é¡¶éƒ¨ç™¾åˆ†æ¯”çš„æµ®ç‚¹æ•°
        video_height: è§†é¢‘é«˜åº¦
        text_height: å­—å¹•æ–‡æœ¬é«˜åº¦
    
    Returns:
        tuple: (x, y) åæ ‡
    """
    margin = 50  # å­—å¹•è·ç¦»è¾¹ç¼˜çš„è¾¹è·
    
    if isinstance(position, (int, float)):
        # ç™¾åˆ†æ¯”ä½ç½®
        return ('center', int(video_height * position))
    
    # é¢„è®¾ä½ç½®
    if position == SubtitlePosition.TOP:
        return ('center', margin)
    elif position == SubtitlePosition.CENTER:
        return ('center', video_height // 2)
    elif position == SubtitlePosition.BOTTOM:
        return ('center', video_height - margin - text_height)
    
    # é»˜è®¤åº•éƒ¨
    return ('center', video_height - margin - text_height)


def generate_video_v3(
        video_path: str,
        subtitle_style: dict,
        volume_config: dict,
        subtitle_path: Optional[str] = None,
        bgm_path: Optional[str] = None,
        narration_path: Optional[str] = None,
        output_path: str = "output.mp4",
        font_path: Optional[str] = None
) -> None:
    """
    åˆå¹¶è§†é¢‘ç´ æï¼ŒåŒ…æ‹¬è§†é¢‘ã€å­—å¹•ã€BGMå’Œè§£è¯´éŸ³é¢‘

    å‚æ•°:
        video_path: åŸè§†é¢‘æ–‡ä»¶è·¯å¾„
        subtitle_path: SRTå­—å¹•æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        bgm_path: èƒŒæ™¯éŸ³ä¹æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        narration_path: è§£è¯´éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        volume_config: éŸ³é‡é…ç½®å­—å…¸ï¼Œå¯åŒ…å«ä»¥ä¸‹é”®ï¼š
            - original: åŸå£°éŸ³é‡ï¼ˆ0-1ï¼‰ï¼Œé»˜è®¤1.0
            - bgm: BGMéŸ³é‡ï¼ˆ0-1ï¼‰ï¼Œé»˜è®¤0.3
            - narration: è§£è¯´éŸ³é‡ï¼ˆ0-1ï¼‰ï¼Œé»˜è®¤1.0
        subtitle_style: å­—å¹•æ ·å¼é…ç½®å­—å…¸ï¼Œå¯åŒ…å«ä»¥ä¸‹é”®ï¼š
            - font: å­—ä½“åç§°
            - fontsize: å­—ä½“å¤§å°
            - color: å­—ä½“é¢œè‰²
            - stroke_color: æè¾¹é¢œè‰²
            - stroke_width: æè¾¹å®½åº¦
            - bg_color: èƒŒæ™¯è‰²
            - position: ä½ç½®æ”¯æŒ SubtitlePosition æšä¸¾å€¼æˆ– 0-1 ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼ˆè¡¨ç¤ºè·é¡¶éƒ¨çš„ç™¾åˆ†æ¯”ï¼‰
            - method: æ–‡å­—æ¸²æŸ“æ–¹æ³•
        font_path: å­—ä½“æ–‡ä»¶è·¯å¾„ï¼ˆ.ttf/.otf ç­‰æ ¼å¼ï¼‰
    """
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

    # åŠ è½½è§†é¢‘
    video = VideoFileClip(video_path)
    subtitle_clips = []

    # å¤„ç†å­—å¹•ï¼ˆå¦‚æœæä¾›ï¼‰
    if subtitle_path:
        if os.path.exists(subtitle_path):
            # æ£€æŸ¥å­—ä½“æ–‡ä»¶
            if font_path and not os.path.exists(font_path):
                logger.warning(f"è­¦å‘Šï¼šå­—ä½“æ–‡ä»¶ä¸å­˜åœ¨: {font_path}")

            try:
                subs = pysrt.open(subtitle_path)
                logger.info(f"è¯»å–åˆ° {len(subs)} æ¡å­—å¹•")

                for index, sub in enumerate(subs):
                    start_time = sub.start.ordinal / 1000
                    end_time = sub.end.ordinal / 1000

                    try:
                        # æ£€æŸ¥å­—å¹•æ–‡æœ¬æ˜¯å¦ä¸ºç©º
                        if not sub.text or sub.text.strip() == '':
                            logger.info(f"è­¦å‘Šï¼šç¬¬ {index + 1} æ¡å­—å¹•å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡")
                            continue

                        # å¤„ç†å­—å¹•æ–‡æœ¬ï¼šç¡®ä¿æ˜¯å­—ç¬¦ä¸²ï¼Œå¹¶å¤„ç†å¯èƒ½çš„åˆ—è¡¨æƒ…å†µ
                        if isinstance(sub.text, (list, tuple)):
                            subtitle_text = ' '.join(str(item) for item in sub.text if item is not None)
                        else:
                            subtitle_text = str(sub.text)

                        subtitle_text = subtitle_text.strip()

                        if not subtitle_text:
                            logger.info(f"è­¦å‘Šï¼šç¬¬ {index + 1} æ¡å­—å¹•å¤„ç†åä¸ºç©ºï¼Œå·²è·³è¿‡")
                            continue

                        # åˆ›å»ºä¸´æ—¶ TextClip æ¥è·å–æ–‡æœ¬é«˜åº¦
                        temp_clip = TextClip(
                            subtitle_text,
                            font=font_path,
                            fontsize=subtitle_style['fontsize'],
                            color=subtitle_style['color']
                        )
                        text_height = temp_clip.h
                        temp_clip.close()

                        # è®¡ç®—å­—å¹•ä½ç½®
                        position = calculate_subtitle_position(
                            subtitle_style['position'],
                            video.h,
                            text_height
                        )

                        # åˆ›å»ºæœ€ç»ˆçš„ TextClip
                        text_clip = (TextClip(
                            subtitle_text,
                            font=font_path,
                            fontsize=subtitle_style['fontsize'],
                            color=subtitle_style['color']
                        )
                            .set_position(position)
                            .set_duration(end_time - start_time)
                            .set_start(start_time))
                        subtitle_clips.append(text_clip)

                    except Exception as e:
                        logger.error(f"è­¦å‘Šï¼šåˆ›å»ºç¬¬ {index + 1} æ¡å­—å¹•æ—¶å‡ºé”™: {traceback.format_exc()}")

                logger.info(f"æˆåŠŸåˆ›å»º {len(subtitle_clips)} æ¡å­—å¹•å‰ªè¾‘")
            except Exception as e:
                logger.info(f"è­¦å‘Šï¼šå¤„ç†å­—å¹•æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        else:
            logger.info(f"æç¤ºï¼šå­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {subtitle_path}")

    # åˆå¹¶éŸ³é¢‘
    audio_clips = []

    # æ·»åŠ åŸå£°ï¼ˆè®¾ç½®éŸ³é‡ï¼‰
    logger.debug(f"éŸ³é‡é…ç½®: {volume_config}")
    if video.audio is not None:
        original_audio = video.audio.volumex(volume_config['original'])
        audio_clips.append(original_audio)

    # æ·»åŠ BGMï¼ˆå¦‚æœæä¾›ï¼‰
    if bgm_path:
        bgm = AudioFileClip(bgm_path)
        if bgm.duration < video.duration:
            bgm = loop_audio_clip(bgm, video.duration)
        else:
            bgm = bgm.subclip(0, video.duration)
        bgm = bgm.volumex(volume_config['bgm'])
        audio_clips.append(bgm)

    # æ·»åŠ è§£è¯´éŸ³é¢‘ï¼ˆå¦‚æœæä¾›ï¼‰
    if narration_path:
        narration = AudioFileClip(narration_path).volumex(volume_config['narration'])
        audio_clips.append(narration)

    # åˆæˆæœ€ç»ˆè§†é¢‘ï¼ˆåŒ…å«å­—å¹•ï¼‰
    if subtitle_clips:
        final_video = CompositeVideoClip([video] + subtitle_clips, size=video.size)
    else:
        logger.info("è­¦å‘Šï¼šæ²¡æœ‰å­—å¹•è¢«æ·»åŠ åˆ°è§†é¢‘ä¸­")
        final_video = video

    if audio_clips:
        final_audio = CompositeAudioClip(audio_clips)
        final_video = final_video.set_audio(final_audio)

    # å¯¼å‡ºè§†é¢‘
    logger.info("å¼€å§‹å¯¼å‡ºè§†é¢‘...")  # è°ƒè¯•ä¿¡æ¯
    final_video.write_videofile(
        output_path,
        codec='libx264',
        audio_codec='aac',
        fps=video.fps
    )
    logger.info(f"è§†é¢‘å·²å¯¼å‡ºåˆ°: {output_path}")  # è°ƒè¯•ä¿¡æ¯

    # æ¸…ç†èµ„æº
    video.close()
    for clip in subtitle_clips:
        clip.close()
    if bgm_path:
        bgm.close()
    if narration_path:
        narration.close()
</file>

<file path="app/services/voice.py">
import os
import re
import json
import traceback
import edge_tts
import asyncio
from loguru import logger
from typing import List
from datetime import datetime
from xml.sax.saxutils import unescape
from edge_tts import submaker, SubMaker
from moviepy.video.tools import subtitles
import time

from app.config import config
from app.utils import utils


def get_all_azure_voices(filter_locals=None) -> list[str]:
    if filter_locals is None:
        filter_locals = ["zh-CN", "en-US", "zh-HK", "zh-TW", "vi-VN"]
    voices_str = """
Name: af-ZA-AdriNeural
Gender: Female

Name: af-ZA-WillemNeural
Gender: Male

Name: am-ET-AmehaNeural
Gender: Male

Name: am-ET-MekdesNeural
Gender: Female

Name: ar-AE-FatimaNeural
Gender: Female

Name: ar-AE-HamdanNeural
Gender: Male

Name: ar-BH-AliNeural
Gender: Male

Name: ar-BH-LailaNeural
Gender: Female

Name: ar-DZ-AminaNeural
Gender: Female

Name: ar-DZ-IsmaelNeural
Gender: Male

Name: ar-EG-SalmaNeural
Gender: Female

Name: ar-EG-ShakirNeural
Gender: Male

Name: ar-IQ-BasselNeural
Gender: Male

Name: ar-IQ-RanaNeural
Gender: Female

Name: ar-JO-SanaNeural
Gender: Female

Name: ar-JO-TaimNeural
Gender: Male

Name: ar-KW-FahedNeural
Gender: Male

Name: ar-KW-NouraNeural
Gender: Female

Name: ar-LB-LaylaNeural
Gender: Female

Name: ar-LB-RamiNeural
Gender: Male

Name: ar-LY-ImanNeural
Gender: Female

Name: ar-LY-OmarNeural
Gender: Male

Name: ar-MA-JamalNeural
Gender: Male

Name: ar-MA-MounaNeural
Gender: Female

Name: ar-OM-AbdullahNeural
Gender: Male

Name: ar-OM-AyshaNeural
Gender: Female

Name: ar-QA-AmalNeural
Gender: Female

Name: ar-QA-MoazNeural
Gender: Male

Name: ar-SA-HamedNeural
Gender: Male

Name: ar-SA-ZariyahNeural
Gender: Female

Name: ar-SY-AmanyNeural
Gender: Female

Name: ar-SY-LaithNeural
Gender: Male

Name: ar-TN-HediNeural
Gender: Male

Name: ar-TN-ReemNeural
Gender: Female

Name: ar-YE-MaryamNeural
Gender: Female

Name: ar-YE-SalehNeural
Gender: Male

Name: az-AZ-BabekNeural
Gender: Male

Name: az-AZ-BanuNeural
Gender: Female

Name: bg-BG-BorislavNeural
Gender: Male

Name: bg-BG-KalinaNeural
Gender: Female

Name: bn-BD-NabanitaNeural
Gender: Female

Name: bn-BD-PradeepNeural
Gender: Male

Name: bn-IN-BashkarNeural
Gender: Male

Name: bn-IN-TanishaaNeural
Gender: Female

Name: bs-BA-GoranNeural
Gender: Male

Name: bs-BA-VesnaNeural
Gender: Female

Name: ca-ES-EnricNeural
Gender: Male

Name: ca-ES-JoanaNeural
Gender: Female

Name: cs-CZ-AntoninNeural
Gender: Male

Name: cs-CZ-VlastaNeural
Gender: Female

Name: cy-GB-AledNeural
Gender: Male

Name: cy-GB-NiaNeural
Gender: Female

Name: da-DK-ChristelNeural
Gender: Female

Name: da-DK-JeppeNeural
Gender: Male

Name: de-AT-IngridNeural
Gender: Female

Name: de-AT-JonasNeural
Gender: Male

Name: de-CH-JanNeural
Gender: Male

Name: de-CH-LeniNeural
Gender: Female

Name: de-DE-AmalaNeural
Gender: Female

Name: de-DE-ConradNeural
Gender: Male

Name: de-DE-FlorianMultilingualNeural
Gender: Male

Name: de-DE-KatjaNeural
Gender: Female

Name: de-DE-KillianNeural
Gender: Male

Name: de-DE-SeraphinaMultilingualNeural
Gender: Female

Name: el-GR-AthinaNeural
Gender: Female

Name: el-GR-NestorasNeural
Gender: Male

Name: en-AU-NatashaNeural
Gender: Female

Name: en-AU-WilliamNeural
Gender: Male

Name: en-CA-ClaraNeural
Gender: Female

Name: en-CA-LiamNeural
Gender: Male

Name: en-GB-LibbyNeural
Gender: Female

Name: en-GB-MaisieNeural
Gender: Female

Name: en-GB-RyanNeural
Gender: Male

Name: en-GB-SoniaNeural
Gender: Female

Name: en-GB-ThomasNeural
Gender: Male

Name: en-HK-SamNeural
Gender: Male

Name: en-HK-YanNeural
Gender: Female

Name: en-IE-ConnorNeural
Gender: Male

Name: en-IE-EmilyNeural
Gender: Female

Name: en-IN-NeerjaExpressiveNeural
Gender: Female

Name: en-IN-NeerjaNeural
Gender: Female

Name: en-IN-PrabhatNeural
Gender: Male

Name: en-KE-AsiliaNeural
Gender: Female

Name: en-KE-ChilembaNeural
Gender: Male

Name: en-NG-AbeoNeural
Gender: Male

Name: en-NG-EzinneNeural
Gender: Female

Name: en-NZ-MitchellNeural
Gender: Male

Name: en-NZ-MollyNeural
Gender: Female

Name: en-PH-JamesNeural
Gender: Male

Name: en-PH-RosaNeural
Gender: Female

Name: en-SG-LunaNeural
Gender: Female

Name: en-SG-WayneNeural
Gender: Male

Name: en-TZ-ElimuNeural
Gender: Male

Name: en-TZ-ImaniNeural
Gender: Female

Name: en-US-AnaNeural
Gender: Female

Name: en-US-AndrewNeural
Gender: Male

Name: en-US-AriaNeural
Gender: Female

Name: en-US-AvaNeural
Gender: Female

Name: en-US-BrianNeural
Gender: Male

Name: en-US-ChristopherNeural
Gender: Male

Name: en-US-EmmaNeural
Gender: Female

Name: en-US-EricNeural
Gender: Male

Name: en-US-GuyNeural
Gender: Male

Name: en-US-JennyNeural
Gender: Female

Name: en-US-MichelleNeural
Gender: Female

Name: en-US-RogerNeural
Gender: Male

Name: en-US-SteffanNeural
Gender: Male

Name: en-ZA-LeahNeural
Gender: Female

Name: en-ZA-LukeNeural
Gender: Male

Name: es-AR-ElenaNeural
Gender: Female

Name: es-AR-TomasNeural
Gender: Male

Name: es-BO-MarceloNeural
Gender: Male

Name: es-BO-SofiaNeural
Gender: Female

Name: es-CL-CatalinaNeural
Gender: Female

Name: es-CL-LorenzoNeural
Gender: Male

Name: es-CO-GonzaloNeural
Gender: Male

Name: es-CO-SalomeNeural
Gender: Female

Name: es-CR-JuanNeural
Gender: Male

Name: es-CR-MariaNeural
Gender: Female

Name: es-CU-BelkysNeural
Gender: Female

Name: es-CU-ManuelNeural
Gender: Male

Name: es-DO-EmilioNeural
Gender: Male

Name: es-DO-RamonaNeural
Gender: Female

Name: es-EC-AndreaNeural
Gender: Female

Name: es-EC-LuisNeural
Gender: Male

Name: es-ES-AlvaroNeural
Gender: Male

Name: es-ES-ElviraNeural
Gender: Female

Name: es-ES-XimenaNeural
Gender: Female

Name: es-GQ-JavierNeural
Gender: Male

Name: es-GQ-TeresaNeural
Gender: Female

Name: es-GT-AndresNeural
Gender: Male

Name: es-GT-MartaNeural
Gender: Female

Name: es-HN-CarlosNeural
Gender: Male

Name: es-HN-KarlaNeural
Gender: Female

Name: es-MX-DaliaNeural
Gender: Female

Name: es-MX-JorgeNeural
Gender: Male

Name: es-NI-FedericoNeural
Gender: Male

Name: es-NI-YolandaNeural
Gender: Female

Name: es-PA-MargaritaNeural
Gender: Female

Name: es-PA-RobertoNeural
Gender: Male

Name: es-PE-AlexNeural
Gender: Male

Name: es-PE-CamilaNeural
Gender: Female

Name: es-PR-KarinaNeural
Gender: Female

Name: es-PR-VictorNeural
Gender: Male

Name: es-PY-MarioNeural
Gender: Male

Name: es-PY-TaniaNeural
Gender: Female

Name: es-SV-LorenaNeural
Gender: Female

Name: es-SV-RodrigoNeural
Gender: Male

Name: es-US-AlonsoNeural
Gender: Male

Name: es-US-PalomaNeural
Gender: Female

Name: es-UY-MateoNeural
Gender: Male

Name: es-UY-ValentinaNeural
Gender: Female

Name: es-VE-PaolaNeural
Gender: Female

Name: es-VE-SebastianNeural
Gender: Male

Name: et-EE-AnuNeural
Gender: Female

Name: et-EE-KertNeural
Gender: Male

Name: fa-IR-DilaraNeural
Gender: Female

Name: fa-IR-FaridNeural
Gender: Male

Name: fi-FI-HarriNeural
Gender: Male

Name: fi-FI-NooraNeural
Gender: Female

Name: fil-PH-AngeloNeural
Gender: Male

Name: fil-PH-BlessicaNeural
Gender: Female

Name: fr-BE-CharlineNeural
Gender: Female

Name: fr-BE-GerardNeural
Gender: Male

Name: fr-CA-AntoineNeural
Gender: Male

Name: fr-CA-JeanNeural
Gender: Male

Name: fr-CA-SylvieNeural
Gender: Female

Name: fr-CA-ThierryNeural
Gender: Male

Name: fr-CH-ArianeNeural
Gender: Female

Name: fr-CH-FabriceNeural
Gender: Male

Name: fr-FR-DeniseNeural
Gender: Female

Name: fr-FR-EloiseNeural
Gender: Female

Name: fr-FR-HenriNeural
Gender: Male

Name: fr-FR-RemyMultilingualNeural
Gender: Male

Name: fr-FR-VivienneMultilingualNeural
Gender: Female

Name: ga-IE-ColmNeural
Gender: Male

Name: ga-IE-OrlaNeural
Gender: Female

Name: gl-ES-RoiNeural
Gender: Male

Name: gl-ES-SabelaNeural
Gender: Female

Name: gu-IN-DhwaniNeural
Gender: Female

Name: gu-IN-NiranjanNeural
Gender: Male

Name: he-IL-AvriNeural
Gender: Male

Name: he-IL-HilaNeural
Gender: Female

Name: hi-IN-MadhurNeural
Gender: Male

Name: hi-IN-SwaraNeural
Gender: Female

Name: hr-HR-GabrijelaNeural
Gender: Female

Name: hr-HR-SreckoNeural
Gender: Male

Name: hu-HU-NoemiNeural
Gender: Female

Name: hu-HU-TamasNeural
Gender: Male

Name: id-ID-ArdiNeural
Gender: Male

Name: id-ID-GadisNeural
Gender: Female

Name: is-IS-GudrunNeural
Gender: Female

Name: is-IS-GunnarNeural
Gender: Male

Name: it-IT-DiegoNeural
Gender: Male

Name: it-IT-ElsaNeural
Gender: Female

Name: it-IT-GiuseppeNeural
Gender: Male

Name: it-IT-IsabellaNeural
Gender: Female

Name: ja-JP-KeitaNeural
Gender: Male

Name: ja-JP-NanamiNeural
Gender: Female

Name: jv-ID-DimasNeural
Gender: Male

Name: jv-ID-SitiNeural
Gender: Female

Name: ka-GE-EkaNeural
Gender: Female

Name: ka-GE-GiorgiNeural
Gender: Male

Name: kk-KZ-AigulNeural
Gender: Female

Name: kk-KZ-DauletNeural
Gender: Male

Name: km-KH-PisethNeural
Gender: Male

Name: km-KH-SreymomNeural
Gender: Female

Name: kn-IN-GaganNeural
Gender: Male

Name: kn-IN-SapnaNeural
Gender: Female

Name: ko-KR-HyunsuNeural
Gender: Male

Name: ko-KR-InJoonNeural
Gender: Male

Name: ko-KR-SunHiNeural
Gender: Female

Name: lo-LA-ChanthavongNeural
Gender: Male

Name: lo-LA-KeomanyNeural
Gender: Female

Name: lt-LT-LeonasNeural
Gender: Male

Name: lt-LT-OnaNeural
Gender: Female

Name: lv-LV-EveritaNeural
Gender: Female

Name: lv-LV-NilsNeural
Gender: Male

Name: mk-MK-AleksandarNeural
Gender: Male

Name: mk-MK-MarijaNeural
Gender: Female

Name: ml-IN-MidhunNeural
Gender: Male

Name: ml-IN-SobhanaNeural
Gender: Female

Name: mn-MN-BataaNeural
Gender: Male

Name: mn-MN-YesuiNeural
Gender: Female

Name: mr-IN-AarohiNeural
Gender: Female

Name: mr-IN-ManoharNeural
Gender: Male

Name: ms-MY-OsmanNeural
Gender: Male

Name: ms-MY-YasminNeural
Gender: Female

Name: mt-MT-GraceNeural
Gender: Female

Name: mt-MT-JosephNeural
Gender: Male

Name: my-MM-NilarNeural
Gender: Female

Name: my-MM-ThihaNeural
Gender: Male

Name: nb-NO-FinnNeural
Gender: Male

Name: nb-NO-PernilleNeural
Gender: Female

Name: ne-NP-HemkalaNeural
Gender: Female

Name: ne-NP-SagarNeural
Gender: Male

Name: nl-BE-ArnaudNeural
Gender: Male

Name: nl-BE-DenaNeural
Gender: Female

Name: nl-NL-ColetteNeural
Gender: Female

Name: nl-NL-FennaNeural
Gender: Female

Name: nl-NL-MaartenNeural
Gender: Male

Name: pl-PL-MarekNeural
Gender: Male

Name: pl-PL-ZofiaNeural
Gender: Female

Name: ps-AF-GulNawazNeural
Gender: Male

Name: ps-AF-LatifaNeural
Gender: Female

Name: pt-BR-AntonioNeural
Gender: Male

Name: pt-BR-FranciscaNeural
Gender: Female

Name: pt-BR-ThalitaNeural
Gender: Female

Name: pt-PT-DuarteNeural
Gender: Male

Name: pt-PT-RaquelNeural
Gender: Female

Name: ro-RO-AlinaNeural
Gender: Female

Name: ro-RO-EmilNeural
Gender: Male

Name: ru-RU-DmitryNeural
Gender: Male

Name: ru-RU-SvetlanaNeural
Gender: Female

Name: si-LK-SameeraNeural
Gender: Male

Name: si-LK-ThiliniNeural
Gender: Female

Name: sk-SK-LukasNeural
Gender: Male

Name: sk-SK-ViktoriaNeural
Gender: Female

Name: sl-SI-PetraNeural
Gender: Female

Name: sl-SI-RokNeural
Gender: Male

Name: so-SO-MuuseNeural
Gender: Male

Name: so-SO-UbaxNeural
Gender: Female

Name: sq-AL-AnilaNeural
Gender: Female

Name: sq-AL-IlirNeural
Gender: Male

Name: sr-RS-NicholasNeural
Gender: Male

Name: sr-RS-SophieNeural
Gender: Female

Name: su-ID-JajangNeural
Gender: Male

Name: su-ID-TutiNeural
Gender: Female

Name: sv-SE-MattiasNeural
Gender: Male

Name: sv-SE-SofieNeural
Gender: Female

Name: sw-KE-RafikiNeural
Gender: Male

Name: sw-KE-ZuriNeural
Gender: Female

Name: sw-TZ-DaudiNeural
Gender: Male

Name: sw-TZ-RehemaNeural
Gender: Female

Name: ta-IN-PallaviNeural
Gender: Female

Name: ta-IN-ValluvarNeural
Gender: Male

Name: ta-LK-KumarNeural
Gender: Male

Name: ta-LK-SaranyaNeural
Gender: Female

Name: ta-MY-KaniNeural
Gender: Female

Name: ta-MY-SuryaNeural
Gender: Male

Name: ta-SG-AnbuNeural
Gender: Male

Name: ta-SG-VenbaNeural
Gender: Female

Name: te-IN-MohanNeural
Gender: Male

Name: te-IN-ShrutiNeural
Gender: Female

Name: th-TH-NiwatNeural
Gender: Male

Name: th-TH-PremwadeeNeural
Gender: Female

Name: tr-TR-AhmetNeural
Gender: Male

Name: tr-TR-EmelNeural
Gender: Female

Name: uk-UA-OstapNeural
Gender: Male

Name: uk-UA-PolinaNeural
Gender: Female

Name: ur-IN-GulNeural
Gender: Female

Name: ur-IN-SalmanNeural
Gender: Male

Name: ur-PK-AsadNeural
Gender: Male

Name: ur-PK-UzmaNeural
Gender: Female

Name: uz-UZ-MadinaNeural
Gender: Female

Name: uz-UZ-SardorNeural
Gender: Male

Name: vi-VN-HoaiMyNeural
Gender: Female

Name: vi-VN-NamMinhNeural
Gender: Male

Name: zh-CN-XiaoxiaoNeural
Gender: Female

Name: zh-CN-XiaoyiNeural
Gender: Female

Name: zh-CN-YunjianNeural
Gender: Male

Name: zh-CN-YunxiNeural
Gender: Male

Name: zh-CN-YunxiaNeural
Gender: Male

Name: zh-CN-YunyangNeural
Gender: Male

Name: zh-CN-liaoning-XiaobeiNeural
Gender: Female

Name: zh-CN-shaanxi-XiaoniNeural
Gender: Female

Name: zh-HK-HiuGaaiNeural
Gender: Female

Name: zh-HK-HiuMaanNeural
Gender: Female

Name: zh-HK-WanLungNeural
Gender: Male

Name: zh-TW-HsiaoChenNeural
Gender: Female

Name: zh-TW-HsiaoYuNeural
Gender: Female

Name: zh-TW-YunJheNeural
Gender: Male

Name: zu-ZA-ThandoNeural
Gender: Female

Name: zu-ZA-ThembaNeural
Gender: Male


Name: en-US-AvaMultilingualNeural-V2
Gender: Female

Name: en-US-AndrewMultilingualNeural-V2
Gender: Male

Name: en-US-EmmaMultilingualNeural-V2
Gender: Female

Name: en-US-BrianMultilingualNeural-V2
Gender: Male

Name: de-DE-FlorianMultilingualNeural-V2
Gender: Male

Name: de-DE-SeraphinaMultilingualNeural-V2
Gender: Female

Name: fr-FR-RemyMultilingualNeural-V2
Gender: Male

Name: fr-FR-VivienneMultilingualNeural-V2
Gender: Female

Name: zh-CN-XiaoxiaoMultilingualNeural-V2
Gender: Female

Name: zh-CN-YunxiNeural-V2
Gender: Male
    """.strip()
    voices = []
    name = ""
    for line in voices_str.split("\n"):
        line = line.strip()
        if not line:
            continue
        if line.startswith("Name: "):
            name = line[6:].strip()
        if line.startswith("Gender: "):
            gender = line[8:].strip()
            if name and gender:
                # voices.append({
                #     "name": name,
                #     "gender": gender,
                # })
                if filter_locals:
                    for filter_local in filter_locals:
                        if name.lower().startswith(filter_local.lower()):
                            voices.append(f"{name}-{gender}")
                else:
                    voices.append(f"{name}-{gender}")
                name = ""
    voices.sort()
    return voices


def parse_voice_name(name: str):
    # zh-CN-XiaoyiNeural-Female
    # zh-CN-YunxiNeural-Male
    # zh-CN-XiaoxiaoMultilingualNeural-V2-Female
    name = name.replace("-Female", "").replace("-Male", "").strip()
    return name


def is_azure_v2_voice(voice_name: str):
    voice_name = parse_voice_name(voice_name)
    if voice_name.endswith("-V2"):
        return voice_name.replace("-V2", "").strip()
    return ""


def tts(
    text: str, voice_name: str, voice_rate: float, voice_pitch: float, voice_file: str
) -> [SubMaker, None]:
    if is_azure_v2_voice(voice_name):
        return azure_tts_v2(text, voice_name, voice_file)
    return azure_tts_v1(text, voice_name, voice_rate, voice_pitch, voice_file)


def convert_rate_to_percent(rate: float) -> str:
    if rate == 1.0:
        return "+0%"
    percent = round((rate - 1.0) * 100)
    if percent > 0:
        return f"+{percent}%"
    else:
        return f"{percent}%"


def convert_pitch_to_percent(rate: float) -> str:
    if rate == 1.0:
        return "+0Hz"
    percent = round((rate - 1.0) * 100)
    if percent > 0:
        return f"+{percent}Hz"
    else:
        return f"{percent}Hz"


def azure_tts_v1(
    text: str, voice_name: str, voice_rate: float, voice_pitch: float, voice_file: str
) -> [SubMaker, None]:
    voice_name = parse_voice_name(voice_name)
    text = text.strip()
    rate_str = convert_rate_to_percent(voice_rate)
    pitch_str = convert_pitch_to_percent(voice_pitch)
    for i in range(3):
        try:
            logger.info(f"ç¬¬ {i+1} æ¬¡ä½¿ç”¨ edge_tts ç”ŸæˆéŸ³é¢‘")

            async def _do() -> tuple[SubMaker, bytes]:
                communicate = edge_tts.Communicate(text, voice_name, rate=rate_str, pitch=pitch_str, proxy=config.proxy.get("http"))
                sub_maker = edge_tts.SubMaker()
                audio_data = bytes()  # ç”¨äºå­˜å‚¨éŸ³é¢‘æ•°æ®
                
                async for chunk in communicate.stream():
                    if chunk["type"] == "audio":
                        audio_data += chunk["data"]
                    elif chunk["type"] == "WordBoundary":
                        sub_maker.create_sub(
                            (chunk["offset"], chunk["duration"]), chunk["text"]
                        )
                return sub_maker, audio_data

            # åˆ¤æ–­éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(voice_file):
                logger.info(f"voice file exists, skip tts: {voice_file}")
                continue

            # è·å–éŸ³é¢‘æ•°æ®å’Œå­—å¹•ä¿¡æ¯
            sub_maker, audio_data = asyncio.run(_do())
            
            # éªŒè¯æ•°æ®æ˜¯å¦æœ‰æ•ˆ
            if not sub_maker or not sub_maker.subs or not audio_data:
                logger.warning(f"failed, invalid data generated")
                if i < 2:
                    time.sleep(1)
                continue

            # æ•°æ®æœ‰æ•ˆï¼Œå†™å…¥æ–‡ä»¶
            with open(voice_file, "wb") as file:
                file.write(audio_data)

            logger.info(f"completed, output file: {voice_file}")
            return sub_maker
        except Exception as e:
            logger.error(f"ç”ŸæˆéŸ³é¢‘æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            if i < 2:
                time.sleep(1)
    return None


def azure_tts_v2(text: str, voice_name: str, voice_file: str) -> [SubMaker, None]:
    voice_name = is_azure_v2_voice(voice_name)
    if not voice_name:
        logger.error(f"invalid voice name: {voice_name}")
        raise ValueError(f"invalid voice name: {voice_name}")
    text = text.strip()

    def _format_duration_to_offset(duration) -> int:
        if isinstance(duration, str):
            time_obj = datetime.strptime(duration, "%H:%M:%S.%f")
            milliseconds = (
                (time_obj.hour * 3600000)
                + (time_obj.minute * 60000)
                + (time_obj.second * 1000)
                + (time_obj.microsecond // 1000)
            )
            return milliseconds * 10000

        if isinstance(duration, int):
            return duration

        return 0

    for i in range(3):
        try:
            logger.info(f"start, voice name: {voice_name}, try: {i + 1}")

            import azure.cognitiveservices.speech as speechsdk

            sub_maker = SubMaker()

            def speech_synthesizer_word_boundary_cb(evt: speechsdk.SessionEventArgs):
                duration = _format_duration_to_offset(str(evt.duration))
                offset = _format_duration_to_offset(evt.audio_offset)
                sub_maker.subs.append(evt.text)
                sub_maker.offset.append((offset, offset + duration))

            # Creates an instance of a speech config with specified subscription key and service region.
            speech_key = config.azure.get("speech_key", "")
            service_region = config.azure.get("speech_region", "")
            audio_config = speechsdk.audio.AudioOutputConfig(
                filename=voice_file, use_default_speaker=True
            )
            speech_config = speechsdk.SpeechConfig(
                subscription=speech_key, region=service_region
            )
            speech_config.speech_synthesis_voice_name = voice_name
            # speech_config.set_property(property_id=speechsdk.PropertyId.SpeechServiceResponse_RequestSentenceBoundary,
            #                            value='true')
            speech_config.set_property(
                property_id=speechsdk.PropertyId.SpeechServiceResponse_RequestWordBoundary,
                value="true",
            )

            speech_config.set_speech_synthesis_output_format(
                speechsdk.SpeechSynthesisOutputFormat.Audio48Khz192KBitRateMonoMp3
            )
            speech_synthesizer = speechsdk.SpeechSynthesizer(
                audio_config=audio_config, speech_config=speech_config
            )
            speech_synthesizer.synthesis_word_boundary.connect(
                speech_synthesizer_word_boundary_cb
            )

            result = speech_synthesizer.speak_text_async(text).get()
            if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
                logger.success(f"azure v2 speech synthesis succeeded: {voice_file}")
                return sub_maker
            elif result.reason == speechsdk.ResultReason.Canceled:
                cancellation_details = result.cancellation_details
                logger.error(
                    f"azure v2 speech synthesis canceled: {cancellation_details.reason}"
                )
                if cancellation_details.reason == speechsdk.CancellationReason.Error:
                    logger.error(
                        f"azure v2 speech synthesis error: {cancellation_details.error_details}"
                    )
            if i < 2:  # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡é‡è¯•ï¼Œåˆ™ç­‰å¾…1ç§’
                time.sleep(1)
            logger.info(f"completed, output file: {voice_file}")
        except Exception as e:
            logger.error(f"failed, error: {str(e)}")
            if i < 2:  # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡é‡è¯•ï¼Œåˆ™ç­‰å¾…1ç§’
                time.sleep(3)
    return None


def _format_text(text: str) -> str:
    # text = text.replace("\n", " ")
    text = text.replace("[", " ")
    text = text.replace("]", " ")
    text = text.replace("(", " ")
    text = text.replace(")", " ")
    text = text.replace("{", " ")
    text = text.replace("}", " ")
    text = text.strip()
    return text


def create_subtitle_from_multiple(text: str, sub_maker_list: List[SubMaker], list_script: List[dict], 
                                  subtitle_file: str):
    """
    æ ¹æ®å¤šä¸ª SubMaker å¯¹è±¡ã€å®Œæ•´æ–‡æœ¬å’ŒåŸå§‹è„šæœ¬åˆ›å»ºä¼˜åŒ–çš„å­—å¹•æ–‡ä»¶
    1. ä½¿ç”¨åŸå§‹è„šæœ¬ä¸­çš„æ—¶é—´æˆ³
    2. è·³è¿‡ OST ä¸º true çš„éƒ¨åˆ†
    3. å°†å­—å¹•æ–‡ä»¶æŒ‰ç…§æ ‡ç‚¹ç¬¦å·åˆ†å‰²æˆå¤šè¡Œ
    4. æ ¹æ®å®Œæ•´æ–‡æœ¬åˆ†æ®µï¼Œä¿æŒåŸæ–‡çš„è¯­å¥ç»“æ„
    5. ç”Ÿæˆæ–°çš„å­—å¹•æ–‡ä»¶ï¼Œæ—¶é—´æˆ³åŒ…å«å°æ—¶å•ä½
    """
    text = _format_text(text)
    sentences = utils.split_string_by_punctuations(text)

    def formatter(idx: int, start_time: str, end_time: str, sub_text: str) -> str:
        return f"{idx}\n{start_time.replace('.', ',')} --> {end_time.replace('.', ',')}\n{sub_text}\n"

    sub_items = []
    sub_index = 0
    sentence_index = 0

    try:
        sub_maker_index = 0
        for script_item in list_script:
            if script_item['OST']:
                continue

            start_time, end_time = script_item['new_timestamp'].split('-')
            if sub_maker_index >= len(sub_maker_list):
                logger.error(f"Sub maker list index out of range: {sub_maker_index}")
                break
            sub_maker = sub_maker_list[sub_maker_index]
            sub_maker_index += 1

            script_duration = utils.time_to_seconds(end_time) - utils.time_to_seconds(start_time)
            audio_duration = get_audio_duration(sub_maker)
            time_ratio = script_duration / audio_duration if audio_duration > 0 else 1

            current_sub = ""
            current_start = None
            current_end = None

            for offset, sub in zip(sub_maker.offset, sub_maker.subs):
                sub = unescape(sub).strip()
                sub_start = utils.seconds_to_time(utils.time_to_seconds(start_time) + offset[0] / 10000000 * time_ratio)
                sub_end = utils.seconds_to_time(utils.time_to_seconds(start_time) + offset[1] / 10000000 * time_ratio)
                
                if current_start is None:
                    current_start = sub_start
                current_end = sub_end
                
                current_sub += sub
                
                # æ£€æŸ¥å½“å‰ç´¯ç§¯çš„å­—å¹•æ˜¯å¦åŒ¹é…ä¸‹ä¸€ä¸ªå¥å­
                while sentence_index < len(sentences) and sentences[sentence_index] in current_sub:
                    sub_index += 1
                    line = formatter(
                        idx=sub_index,
                        start_time=current_start,
                        end_time=current_end,
                        sub_text=sentences[sentence_index].strip(),
                    )
                    sub_items.append(line)
                    current_sub = current_sub.replace(sentences[sentence_index], "", 1).strip()
                    current_start = current_end
                    sentence_index += 1

                # å¦‚æœå½“å‰å­—å¹•é•¿åº¦è¶…è¿‡15ä¸ªå­—ç¬¦ï¼Œä¹Ÿç”Ÿæˆä¸€ä¸ªæ–°çš„å­—å¹•é¡¹
                if len(current_sub) > 15:
                    sub_index += 1
                    line = formatter(
                        idx=sub_index,
                        start_time=current_start,
                        end_time=current_end,
                        sub_text=current_sub.strip(),
                    )
                    sub_items.append(line)
                    current_sub = ""
                    current_start = current_end

            # å¤„ç†å‰©ä½™çš„æ–‡æœ¬
            if current_sub.strip():
                sub_index += 1
                line = formatter(
                    idx=sub_index,
                    start_time=current_start,
                    end_time=current_end,
                    sub_text=current_sub.strip(),
                )
                sub_items.append(line)

        if len(sub_items) == 0:
            logger.error("No subtitle items generated")
            return

        with open(subtitle_file, "w", encoding="utf-8") as file:
            file.write("\n".join(sub_items))

        logger.info(f"completed, subtitle file created: {subtitle_file}")
    except Exception as e:
        logger.error(f"failed, error: {str(e)}")
        traceback.print_exc()


def get_audio_duration(sub_maker: submaker.SubMaker):
    """
    è·å–éŸ³é¢‘æ—¶é•¿
    """
    if not sub_maker.offset:
        return 0.0
    return sub_maker.offset[-1][1] / 10000000


def tts_multiple(task_id: str, list_script: list, voice_name: str, voice_rate: float, voice_pitch: float, force_regenerate: bool = True):
    """
    æ ¹æ®JSONæ–‡ä»¶ä¸­çš„å¤šæ®µæ–‡æœ¬è¿›è¡ŒTTSè½¬æ¢
    
    :param task_id: ä»»åŠ¡ID
    :param list_script: è„šæœ¬åˆ—è¡¨
    :param voice_name: è¯­éŸ³åç§°
    :param voice_rate: è¯­éŸ³é€Ÿç‡
    :param force_regenerate: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆå·²å­˜åœ¨çš„éŸ³é¢‘æ–‡ä»¶
    :return: ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨
    """
    voice_name = parse_voice_name(voice_name)
    output_dir = utils.task_dir(task_id)
    audio_files = []
    sub_maker_list = []

    for item in list_script:
        if item['OST'] != 1:
            # å°†æ—¶é—´æˆ³ä¸­çš„å†’å·æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
            timestamp = item['new_timestamp'].replace(':', '_')
            audio_file = os.path.join(output_dir, f"audio_{timestamp}.mp3")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚å­˜åœ¨ä¸”ä¸å¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼Œåˆ™è·³è¿‡
            if os.path.exists(audio_file) and not force_regenerate:
                logger.info(f"éŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ç”Ÿæˆ: {audio_file}")
                audio_files.append(audio_file)
                continue

            text = item['narration']

            sub_maker = tts(
                text=text,
                voice_name=voice_name,
                voice_rate=voice_rate,
                voice_pitch=voice_pitch,
                voice_file=audio_file,
            )

            if sub_maker is None:
                logger.error(f"æ— æ³•ä¸ºæ—¶é—´æˆ³ {timestamp} ç”ŸæˆéŸ³é¢‘; "
                             f"å¦‚æœæ‚¨åœ¨ä¸­å›½ï¼Œè¯·ä½¿ç”¨VPN; "
                             f"æˆ–è€…ä½¿ç”¨å…¶ä»– tts å¼•æ“")
                continue

            audio_files.append(audio_file)
            sub_maker_list.append(sub_maker)
            logger.info(f"å·²ç”ŸæˆéŸ³é¢‘æ–‡ä»¶: {audio_file}")

    return audio_files, sub_maker_list
</file>

<file path="app/services/youtube_service.py">
import yt_dlp
import os
from typing import List, Dict, Optional, Tuple
from loguru import logger
from uuid import uuid4

from app.utils import utils
from app.services import video as VideoService


class YoutubeService:
    def __init__(self):
        self.supported_formats = ['mp4', 'mkv', 'webm', 'flv', 'avi']

    def _get_video_formats(self, url: str) -> List[Dict]:
        """è·å–è§†é¢‘å¯ç”¨çš„æ ¼å¼åˆ—è¡¨"""
        ydl_opts = {
            'quiet': True,
            'no_warnings': True
        }

        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=False)
                formats = info.get('formats', [])

                format_list = []
                for f in formats:
                    format_info = {
                        'format_id': f.get('format_id', 'N/A'),
                        'ext': f.get('ext', 'N/A'),
                        'resolution': f.get('format_note', 'N/A'),
                        'filesize': f.get('filesize', 'N/A'),
                        'vcodec': f.get('vcodec', 'N/A'),
                        'acodec': f.get('acodec', 'N/A')
                    }
                    format_list.append(format_info)

                return format_list
        except Exception as e:
            logger.error(f"è·å–è§†é¢‘æ ¼å¼å¤±è´¥: {str(e)}")
            raise

    def _validate_format(self, output_format: str) -> None:
        """éªŒè¯è¾“å‡ºæ ¼å¼æ˜¯å¦æ”¯æŒ"""
        if output_format.lower() not in self.supported_formats:
            raise ValueError(
                f"ä¸æ”¯æŒçš„è§†é¢‘æ ¼å¼: {output_format}ã€‚"
                f"æ”¯æŒçš„æ ¼å¼: {', '.join(self.supported_formats)}"
            )

    async def download_video(
            self,
            url: str,
            resolution: str,
            output_format: str = 'mp4',
            rename: Optional[str] = None
    ) -> Tuple[str, str, str]:
        """
        ä¸‹è½½æŒ‡å®šåˆ†è¾¨ç‡çš„è§†é¢‘
        
        Args:
            url: YouTubeè§†é¢‘URL
            resolution: ç›®æ ‡åˆ†è¾¨ç‡ ('2160p', '1440p', '1080p', '720p' etc.)
                       æ³¨æ„ï¼šå¯¹äºç±»ä¼¼'1080p60'çš„è¾“å…¥ä¼šè¢«å¤„ç†ä¸º'1080p'
            output_format: è¾“å‡ºè§†é¢‘æ ¼å¼
            rename: å¯é€‰çš„é‡å‘½å
            
        Returns:
            Tuple[str, str, str]: (task_id, output_path, filename)
        """
        try:
            task_id = str(uuid4())
            self._validate_format(output_format)

            # æ ‡å‡†åŒ–åˆ†è¾¨ç‡æ ¼å¼
            base_resolution = resolution.split('p')[0] + 'p'
            
            # è·å–æ‰€æœ‰å¯ç”¨æ ¼å¼
            formats = self._get_video_formats(url)

            # æŸ¥æ‰¾æŒ‡å®šåˆ†è¾¨ç‡çš„æœ€ä½³è§†é¢‘æ ¼å¼
            target_format = None
            for fmt in formats:
                fmt_resolution = fmt['resolution']
                # å°†æ ¼å¼çš„åˆ†è¾¨ç‡ä¹Ÿæ ‡å‡†åŒ–åè¿›è¡Œæ¯”è¾ƒ
                if fmt_resolution != 'N/A':
                    fmt_base_resolution = fmt_resolution.split('p')[0] + 'p'
                    if fmt_base_resolution == base_resolution and fmt['vcodec'] != 'none':
                        target_format = fmt
                        break

            if target_format is None:
                # æ”¶é›†å¯ç”¨åˆ†è¾¨ç‡æ—¶ä¹Ÿè¿›è¡Œæ ‡å‡†åŒ–
                available_resolutions = set(
                    fmt['resolution'].split('p')[0] + 'p'
                    for fmt in formats
                    if fmt['resolution'] != 'N/A' and fmt['vcodec'] != 'none'
                )
                raise ValueError(
                    f"æœªæ‰¾åˆ° {base_resolution} åˆ†è¾¨ç‡çš„è§†é¢‘ã€‚"
                    f"å¯ç”¨åˆ†è¾¨ç‡: {', '.join(sorted(available_resolutions))}"
                )

            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = utils.video_dir()
            os.makedirs(output_dir, exist_ok=True)

            # è®¾ç½®ä¸‹è½½é€‰é¡¹
            if rename:
                # å¦‚æœæŒ‡å®šäº†é‡å‘½åï¼Œç›´æ¥ä½¿ç”¨æ–°åå­—
                filename = f"{rename}.{output_format}"
                output_template = os.path.join(output_dir, filename)
            else:
                # å¦åˆ™ä½¿ç”¨ä»»åŠ¡IDå’ŒåŸæ ‡é¢˜
                output_template = os.path.join(output_dir, f'{task_id}_%(title)s.%(ext)s')

            ydl_opts = {
                'format': f"{target_format['format_id']}+bestaudio[ext=m4a]/best",
                'outtmpl': output_template,
                'merge_output_format': output_format.lower(),
                'postprocessors': [{
                    'key': 'FFmpegVideoConvertor',
                    'preferedformat': output_format.lower(),
                }]
            }

            # æ‰§è¡Œä¸‹è½½
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=True)
                if rename:
                    # å¦‚æœæŒ‡å®šäº†é‡å‘½åï¼Œä½¿ç”¨æ–°æ–‡ä»¶å
                    output_path = output_template
                    filename = os.path.basename(output_path)
                else:
                    # å¦åˆ™ä½¿ç”¨åŸå§‹æ ‡é¢˜
                    video_title = info.get('title', task_id)
                    filename = f"{task_id}_{video_title}.{output_format}"
                    output_path = os.path.join(output_dir, filename)

            logger.info(f"è§†é¢‘ä¸‹è½½æˆåŠŸ: {output_path}")
            return task_id, output_path, filename

        except Exception as e:
            logger.exception("ä¸‹è½½è§†é¢‘å¤±è´¥")
            raise
</file>

<file path="app/test/test_gemini.py">
import google.generativeai as genai
from app.config import config
import os

os.environ["HTTP_PROXY"] = config.proxy.get("http")
os.environ["HTTPS_PROXY"] = config.proxy.get("https")

genai.configure(api_key="")
model = genai.GenerativeModel("gemini-1.5-pro")


for i in range(50):
    response = model.generate_content("ç›´æ¥å›å¤æˆ‘æ–‡æœ¬'å½“å‰ç½‘ç»œå¯ç”¨'")
    print(i, response.text)
</file>

<file path="app/test/test_moviepy_merge.py">
"""
ä½¿ç”¨ moviepy åˆå¹¶è§†é¢‘ã€éŸ³é¢‘ã€å­—å¹•å’ŒèƒŒæ™¯éŸ³ä¹
"""

from moviepy.editor import (
    VideoFileClip,
    AudioFileClip,
    TextClip,
    CompositeVideoClip,
    concatenate_videoclips
)
# from moviepy.config import change_settings
import os

# è®¾ç½®å­—ä½“æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºä¸­æ–‡å­—å¹•æ˜¾ç¤ºï¼‰
FONT_PATH = "../../resource/fonts/STHeitiMedium.ttc"  # è¯·ç¡®ä¿æ­¤è·¯å¾„ä¸‹æœ‰å¯¹åº”å­—ä½“æ–‡ä»¶
# change_settings(
#     {"IMAGEMAGICK_BINARY": r"C:\Program Files\ImageMagick-7.1.1-Q16\magick.exe"})  # Windowsç³»ç»Ÿéœ€è¦è®¾ç½® ImageMagick è·¯å¾„


class VideoMerger:
    """è§†é¢‘åˆå¹¶å¤„ç†ç±»"""

    def __init__(self, output_path: str = "../../resource/videos/merged_video.mp4"):
        """
        åˆå§‹åŒ–è§†é¢‘åˆå¹¶å™¨
        å‚æ•°:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        self.output_path = output_path
        self.video_clips = []
        self.background_music = None
        self.subtitles = []

    def add_video(self, video_path: str, start_time: str = None, end_time: str = None) -> None:
        """
        æ·»åŠ è§†é¢‘ç‰‡æ®µ
        å‚æ•°:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            start_time: å¼€å§‹æ—¶é—´ (æ ¼å¼: "MM:SS")
            end_time: ç»“æŸæ—¶é—´ (æ ¼å¼: "MM:SS")
        """
        video = VideoFileClip(video_path)
        if start_time and end_time:
            video = video.subclip(self._time_to_seconds(start_time),
                                  self._time_to_seconds(end_time))
        self.video_clips.append(video)

    def add_audio(self, audio_path: str, volume: float = 1.0) -> None:
        """
        æ·»åŠ èƒŒæ™¯éŸ³ä¹
        å‚æ•°:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            volume: éŸ³é‡å¤§å° (0.0-1.0)
        """
        self.background_music = AudioFileClip(audio_path).volumex(volume)

    def add_subtitle(self, text: str, start_time: str, end_time: str,
                     position: tuple = ('center', 'bottom'), fontsize: int = 24) -> None:
        """
        æ·»åŠ å­—å¹•
        å‚æ•°:
            text: å­—å¹•æ–‡æœ¬
            start_time: å¼€å§‹æ—¶é—´ (æ ¼å¼: "MM:SS")
            end_time: ç»“æŸæ—¶é—´ (æ ¼å¼: "MM:SS")
            position: å­—å¹•ä½ç½®
            fontsize: å­—ä½“å¤§å°
        """
        subtitle = TextClip(
            text,
            font=FONT_PATH,
            fontsize=fontsize,
            color='white',
            stroke_color='black',
            stroke_width=2
        )

        subtitle = subtitle.set_position(position).set_duration(
            self._time_to_seconds(end_time) - self._time_to_seconds(start_time)
        ).set_start(self._time_to_seconds(start_time))

        self.subtitles.append(subtitle)

    def merge(self) -> None:
        """åˆå¹¶æ‰€æœ‰åª’ä½“å…ƒç´ å¹¶å¯¼å‡ºè§†é¢‘"""
        if not self.video_clips:
            raise ValueError("è‡³å°‘éœ€è¦æ·»åŠ ä¸€ä¸ªè§†é¢‘ç‰‡æ®µ")

        # åˆå¹¶è§†é¢‘ç‰‡æ®µ
        final_video = concatenate_videoclips(self.video_clips)

        # å¦‚æœæœ‰èƒŒæ™¯éŸ³ä¹ï¼Œè®¾ç½®å…¶æŒç»­æ—¶é—´ä¸è§†é¢‘ç›¸åŒ
        if self.background_music:
            self.background_music = self.background_music.set_duration(final_video.duration)
            final_video = final_video.set_audio(self.background_music)

        # æ·»åŠ å­—å¹•
        if self.subtitles:
            final_video = CompositeVideoClip([final_video] + self.subtitles)

        # å¯¼å‡ºæœ€ç»ˆè§†é¢‘
        final_video.write_videofile(
            self.output_path,
            fps=24,
            codec='libx264',
            audio_codec='aac'
        )

        # é‡Šæ”¾èµ„æº
        final_video.close()
        for clip in self.video_clips:
            clip.close()
        if self.background_music:
            self.background_music.close()

    @staticmethod
    def _time_to_seconds(time_str: str) -> float:
        """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°"""
        minutes, seconds = map(int, time_str.split(':'))
        return minutes * 60 + seconds


def test_merge_video():
    """æµ‹è¯•è§†é¢‘åˆå¹¶åŠŸèƒ½"""
    merger = VideoMerger()

    # æ·»åŠ ä¸¤ä¸ªè§†é¢‘ç‰‡æ®µ
    merger.add_video("../../resource/videos/cut_video.mp4", "00:00", "01:00")
    merger.add_video("../../resource/videos/demo.mp4", "00:00", "00:30")

    # æ·»åŠ èƒŒæ™¯éŸ³ä¹
    merger.add_audio("../../resource/songs/output000.mp3", volume=0.3)

    # æ·»åŠ å­—å¹•
    merger.add_subtitle("ç¬¬ä¸€ä¸ªç²¾å½©ç‰‡æ®µ", "00:00", "00:05")
    merger.add_subtitle("ç¬¬äºŒä¸ªç²¾å½©ç‰‡æ®µ", "01:00", "01:05")

    # åˆå¹¶å¹¶å¯¼å‡º
    merger.merge()


if __name__ == "__main__":
    test_merge_video()
</file>

<file path="app/test/test_moviepy_speed.py">
"""
ä½¿ç”¨ moviepy ä¼˜åŒ–è§†é¢‘å¤„ç†é€Ÿåº¦çš„ç¤ºä¾‹
åŒ…å«ï¼šè§†é¢‘åŠ é€Ÿã€å¤šæ ¸å¤„ç†ã€é¢„è®¾å‚æ•°ä¼˜åŒ–ç­‰
"""

from moviepy.editor import VideoFileClip
from moviepy.video.fx.speedx import speedx
import multiprocessing as mp
import time


class VideoSpeedProcessor:
    """è§†é¢‘é€Ÿåº¦å¤„ç†å™¨"""

    def __init__(self, input_path: str, output_path: str):
        self.input_path = input_path
        self.output_path = output_path
        # è·å–CPUæ ¸å¿ƒæ•°
        self.cpu_cores = mp.cpu_count()

    def process_with_optimization(self, speed_factor: float = 1.0) -> None:
        """
        ä½¿ç”¨ä¼˜åŒ–å‚æ•°å¤„ç†è§†é¢‘
        å‚æ•°:
            speed_factor: é€Ÿåº¦å€æ•° (1.0 ä¸ºåŸé€Ÿ, 2.0 ä¸ºåŒå€é€Ÿ)
        """
        start_time = time.time()
        
        # åŠ è½½è§†é¢‘æ—¶ä½¿ç”¨ä¼˜åŒ–å‚æ•°
        video = VideoFileClip(
            self.input_path,
            audio=True,  # å¦‚æœä¸éœ€è¦éŸ³é¢‘å¯ä»¥è®¾ä¸ºFalse
            target_resolution=(720, None),  # å¯ä»¥é™ä½åˆ†è¾¨ç‡åŠ å¿«å¤„ç†
            resize_algorithm='fast_bilinear'  # ä½¿ç”¨å¿«é€Ÿçš„é‡ç½®ç®—æ³•
        )

        # åº”ç”¨é€Ÿåº¦å˜åŒ–
        if speed_factor != 1.0:
            video = speedx(video, factor=speed_factor)

        # ä½¿ç”¨ä¼˜åŒ–å‚æ•°å¯¼å‡ºè§†é¢‘
        video.write_videofile(
            self.output_path,
            codec='libx264',  # ä½¿ç”¨h264ç¼–ç 
            audio_codec='aac',  # éŸ³é¢‘ç¼–ç 
            temp_audiofile='temp-audio.m4a',  # ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            remove_temp=True,  # å¤„ç†å®Œæˆååˆ é™¤ä¸´æ—¶æ–‡ä»¶
            write_logfile=False,  # å…³é—­æ—¥å¿—æ–‡ä»¶
            threads=self.cpu_cores,  # ä½¿ç”¨å¤šæ ¸å¤„ç†
            preset='ultrafast',  # ä½¿ç”¨æœ€å¿«çš„ç¼–ç é¢„è®¾
            ffmpeg_params=[
                '-brand', 'mp42',
                '-crf', '23',  # å‹ç¼©ç‡ï¼ŒèŒƒå›´0-51ï¼Œæ•°å€¼è¶Šå¤§å‹ç¼©ç‡è¶Šé«˜
            ]
        )

        # é‡Šæ”¾èµ„æº
        video.close()

        end_time = time.time()
        print(f"å¤„ç†å®Œæˆï¼ç”¨æ—¶: {end_time - start_time:.2f} ç§’")

    def batch_process_segments(self, segment_times: list, speed_factor: float = 1.0) -> None:
        """
        æ‰¹é‡å¤„ç†è§†é¢‘ç‰‡æ®µï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
        å‚æ•°:
            segment_times: åˆ—è¡¨ï¼ŒåŒ…å«å¤šä¸ª(start, end)æ—¶é—´å…ƒç»„
            speed_factor: é€Ÿåº¦å€æ•°
        """
        start_time = time.time()
        
        # åˆ›å»ºè¿›ç¨‹æ± 
        with mp.Pool(processes=self.cpu_cores) as pool:
            # å‡†å¤‡å‚æ•°
            args = [(self.input_path, start, end, speed_factor, i) 
                   for i, (start, end) in enumerate(segment_times)]
            
            # å¹¶è¡Œå¤„ç†ç‰‡æ®µ
            pool.starmap(self._process_segment, args)

        end_time = time.time()
        print(f"æ‰¹é‡å¤„ç†å®Œæˆï¼æ€»ç”¨æ—¶: {end_time - start_time:.2f} ç§’")

    @staticmethod
    def _process_segment(video_path: str, start: str, end: str, 
                        speed_factor: float, index: int) -> None:
        """å¤„ç†å•ä¸ªè§†é¢‘ç‰‡æ®µ"""
        # è½¬æ¢æ—¶é—´æ ¼å¼
        start_sec = VideoSpeedProcessor._time_to_seconds(start)
        end_sec = VideoSpeedProcessor._time_to_seconds(end)
        
        # åŠ è½½å¹¶å¤„ç†è§†é¢‘ç‰‡æ®µ
        video = VideoFileClip(
            video_path,
            audio=True,
            target_resolution=(720, None)
        ).subclip(start_sec, end_sec)

        # åº”ç”¨é€Ÿåº¦å˜åŒ–
        if speed_factor != 1.0:
            video = speedx(video, factor=speed_factor)

        # ä¿å­˜å¤„ç†åçš„ç‰‡æ®µ
        output_path = f"../../resource/videos/segment_{index}.mp4"
        video.write_videofile(
            output_path,
            codec='libx264',
            audio_codec='aac',
            preset='ultrafast',
            threads=2  # æ¯ä¸ªè¿›ç¨‹ä½¿ç”¨çš„çº¿ç¨‹æ•°
        )
        
        video.close()

    @staticmethod
    def _time_to_seconds(time_str: str) -> float:
        """å°†æ—¶é—´å­—ç¬¦ä¸²(MM:SS)è½¬æ¢ä¸ºç§’æ•°"""
        minutes, seconds = map(int, time_str.split(':'))
        return minutes * 60 + seconds


def test_video_speed():
    """æµ‹è¯•è§†é¢‘åŠ é€Ÿå¤„ç†"""
    processor = VideoSpeedProcessor(
        "../../resource/videos/best.mp4",
        "../../resource/videos/speed_up.mp4"
    )
    
    # æµ‹è¯•1ï¼šç®€å•åŠ é€Ÿ
    processor.process_with_optimization(speed_factor=1.5)  # 1.5å€é€Ÿ
    
    # æµ‹è¯•2ï¼šå¹¶è¡Œå¤„ç†å¤šä¸ªç‰‡æ®µ
    segments = [
        ("00:00", "01:00"),
        ("01:00", "02:00"),
        ("02:00", "03:00")
    ]
    processor.batch_process_segments(segments, speed_factor=2.0)  # 2å€é€Ÿ


if __name__ == "__main__":
    test_video_speed()
</file>

<file path="app/test/test_moviepy.py">
"""
ä½¿ç”¨ moviepy åº“å‰ªè¾‘æŒ‡å®šæ—¶é—´æˆ³è§†é¢‘ï¼Œæ”¯æŒæ—¶åˆ†ç§’æ¯«ç§’ç²¾åº¦
"""

from moviepy.editor import VideoFileClip
from datetime import datetime
import os


def time_str_to_seconds(time_str: str) -> float:
    """
    å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°
    å‚æ•°:
        time_str: æ ¼å¼ä¸º"HH:MM:SS,mmm"çš„æ—¶é—´å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚"00:01:23,456"
    è¿”å›:
        è½¬æ¢åçš„ç§’æ•°(float)
    """
    try:
        # åˆ†ç¦»æ—¶é—´å’Œæ¯«ç§’
        time_part, ms_part = time_str.split(',')
        # è½¬æ¢æ—¶åˆ†ç§’
        time_obj = datetime.strptime(time_part, "%H:%M:%S")
        # è®¡ç®—æ€»ç§’æ•°
        total_seconds = time_obj.hour * 3600 + time_obj.minute * 60 + time_obj.second
        # æ·»åŠ æ¯«ç§’éƒ¨åˆ†
        total_seconds += int(ms_part) / 1000
        return total_seconds
    except ValueError as e:
        raise ValueError("æ—¶é—´æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ HH:MM:SS,mmm æ ¼å¼ï¼Œä¾‹å¦‚ 00:01:23,456") from e


def format_duration(seconds: float) -> str:
    """
    å°†ç§’æ•°è½¬æ¢ä¸ºå¯è¯»çš„æ—¶é—´æ ¼å¼
    å‚æ•°:
        seconds: ç§’æ•°
    è¿”å›:
        æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸² (HH:MM:SS,mmm)
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    seconds_remain = seconds % 60
    whole_seconds = int(seconds_remain)
    milliseconds = int((seconds_remain - whole_seconds) * 1000)
    
    return f"{hours:02d}:{minutes:02d}:{whole_seconds:02d},{milliseconds:03d}"


def cut_video(video_path: str, start_time: str, end_time: str, output_path: str) -> None:
    """
    å‰ªè¾‘è§†é¢‘
    å‚æ•°:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        start_time: å¼€å§‹æ—¶é—´ (æ ¼å¼: "HH:MM:SS,mmm")
        end_time: ç»“æŸæ—¶é—´ (æ ¼å¼: "HH:MM:SS,mmm")
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_path)
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        # å¦‚æœè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆå°è¯•åˆ é™¤
        if os.path.exists(output_path):
            try:
                os.remove(output_path)
            except PermissionError:
                print(f"æ— æ³•åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶ï¼š{output_path}ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æœªè¢«å…¶ä»–ç¨‹åºå ç”¨")
                return
        
        # è½¬æ¢æ—¶é—´å­—ç¬¦ä¸²ä¸ºç§’æ•°
        start_seconds = time_str_to_seconds(start_time)
        end_seconds = time_str_to_seconds(end_time)
        
        # åŠ è½½è§†é¢‘æ–‡ä»¶
        video = VideoFileClip(video_path)
        
        # éªŒè¯æ—¶é—´èŒƒå›´
        if start_seconds >= video.duration or end_seconds > video.duration:
            raise ValueError(f"å‰ªè¾‘æ—¶é—´è¶…å‡ºè§†é¢‘é•¿åº¦ï¼è§†é¢‘æ€»é•¿åº¦ä¸º: {format_duration(video.duration)}")
        
        if start_seconds >= end_seconds:
            raise ValueError("ç»“æŸæ—¶é—´å¿…é¡»å¤§äºå¼€å§‹æ—¶é—´ï¼")
        
        # è®¡ç®—å‰ªè¾‘æ—¶é•¿
        clip_duration = end_seconds - start_seconds
        print(f"åŸè§†é¢‘æ€»é•¿åº¦: {format_duration(video.duration)}")
        print(f"å‰ªè¾‘æ—¶é•¿: {format_duration(clip_duration)}")
        print(f"å‰ªè¾‘åŒºé—´: {start_time} -> {end_time}")
        
        # å‰ªè¾‘è§†é¢‘
        video = video.subclip(start_seconds, end_seconds)
        
        # æ·»åŠ é”™è¯¯å¤„ç†çš„å†™å…¥è¿‡ç¨‹
        try:
            video.write_videofile(
                output_path,
                codec='libx264',
                audio_codec='aac',
                temp_audiofile='temp-audio.m4a',
                remove_temp=True
            )
        except IOError as e:
            print(f"å†™å…¥è§†é¢‘æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            raise
        finally:
            # ç¡®ä¿èµ„æºè¢«é‡Šæ”¾
            video.close()
            
    except Exception as e:
        print(f"è§†é¢‘å‰ªè¾‘è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
        raise


if __name__ == "__main__":
    cut_video(
        video_path="/Users/apple/Desktop/NarratoAI/resource/videos/duanju_yuansp.mp4",
        start_time="00:00:00,789",
        end_time="00:02:00,123",
        output_path="/Users/apple/Desktop/NarratoAI/resource/videos/duanju_yuansp_cut3.mp4"
    )
</file>

<file path="app/test/test_qwen.py">
import os
import traceback
import json
from openai import OpenAI
from pydantic import BaseModel
from typing import List
from app.utils import utils
from app.services.subtitle import extract_audio_and_create_subtitle


class Step(BaseModel):
    timestamp: str
    picture: str
    narration: str
    OST: int
    new_timestamp: str

class MathReasoning(BaseModel):
    result: List[Step]


def chat_with_qwen(prompt: str, system_message: str, subtitle_path: str) -> str:
    """
    ä¸é€šä¹‰åƒé—®AIæ¨¡å‹è¿›è¡Œå¯¹è¯
    
    Args:
        prompt (str): ç”¨æˆ·è¾“å…¥çš„é—®é¢˜æˆ–æç¤º
        system_message (str): ç³»ç»Ÿæç¤ºä¿¡æ¯ï¼Œç”¨äºè®¾å®šAIåŠ©æ‰‹çš„è¡Œä¸ºã€‚é»˜è®¤ä¸º"You are a helpful assistant."
        subtitle_path (str): å­—å¹•æ–‡ä»¶è·¯å¾„
    Returns:
        str: AIåŠ©æ‰‹çš„å›å¤å†…å®¹

    Raises:
        Exception: å½“APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    try:
        client = OpenAI(
            api_key="sk-a1acd853d88d41d3ae92777d7bfa2612",
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )

        # è¯»å–å­—å¹•æ–‡ä»¶
        with open(subtitle_path, "r", encoding="utf-8") as file:
            subtitle_content = file.read()

        completion = client.chat.completions.create(
            model="qwen-turbo-2024-11-01",
            messages=[
                {'role': 'system', 'content': system_message},
                {'role': 'user', 'content': prompt + subtitle_content}
            ]
        )
        return completion.choices[0].message.content

    except Exception as e:
        error_message = f"è°ƒç”¨åƒé—®APIæ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
        print(error_message)
        print("è¯·å‚è€ƒæ–‡æ¡£ï¼šhttps://help.aliyun.com/zh/model-studio/developer-reference/error-code")
        raise Exception(error_message)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    try:
        video_path = utils.video_dir("duanju_yuansp.mp4")
        # # åˆ¤æ–­è§†é¢‘æ˜¯å¦å­˜åœ¨
        # if not os.path.exists(video_path):
        #     print(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼š{video_path}")
        #     exit(1)
        # æå–å­—å¹•
        subtitle_path = os.path.join(utils.video_dir(""), f"duanju_yuan.srt")
        extract_audio_and_create_subtitle(video_file=video_path, subtitle_file=subtitle_path)
        # åˆ†æå­—å¹•
        system_message = """
        ä½ æ˜¯ä¸€ä¸ªè§†é¢‘srtå­—å¹•åˆ†æå‰ªè¾‘å™¨, è¾“å…¥è§†é¢‘çš„srtå­—å¹•, åˆ†æå…¶ä¸­çš„ç²¾å½©ä¸”å°½å¯èƒ½è¿ç»­çš„ç‰‡æ®µå¹¶è£å‰ªå‡ºæ¥, æ³¨æ„ç¡®ä¿æ–‡å­—ä¸æ—¶é—´æˆ³çš„æ­£ç¡®åŒ¹é…ã€‚
        è¾“å‡ºéœ€ä¸¥æ ¼æŒ‰ç…§å¦‚ä¸‹ json æ ¼å¼:
        [
            {
                "timestamp": "00:00:50,020-00,01:44,000",
                "picture": "ç”»é¢1",
                "narration": "æ’­æ”¾åŸå£°",
                "OST": 0,
                "new_timestamp": "00:00:00,000-00:00:54,020"
            },
            {
                "timestamp": "01:49-02:30",
                "picture": "ç”»é¢2",
                "narration": "æ’­æ”¾åŸå£°",
                "OST": 2,
                "new_timestamp": "00:54-01:35"
            },
        ]
        """
        prompt = "å­—å¹•å¦‚ä¸‹ï¼š\n"
        response = chat_with_qwen(prompt, system_message, subtitle_path)
        print(response)
        # ä¿å­˜jsonï¼Œæ³¨æ„jsonä¸­æ˜¯æ—¶é—´æˆ³éœ€è¦è½¬æ¢ä¸º åˆ†:ç§’(ç°åœ¨çš„æ—¶é—´æ˜¯ "timestamp": "00:00:00,020-00:00:01,660", éœ€è¦è½¬æ¢ä¸º "timestamp": "00:00-01:66")
        # response = json.loads(response)
        # for item in response:
        #     item["timestamp"] = item["timestamp"].replace(":", "-")
        # with open(os.path.join(utils.video_dir(""), "duanju_yuan.json"), "w", encoding="utf-8") as file:
        #     json.dump(response, file, ensure_ascii=False)

    except Exception as e:
        print(traceback.format_exc())
</file>

<file path="app/utils/check_script.py">
import json
from typing import Dict, Any

def check_format(script_content: str) -> Dict[str, Any]:
    """æ£€æŸ¥è„šæœ¬æ ¼å¼
    Args:
        script_content: è„šæœ¬å†…å®¹
    Returns:
        Dict: {'success': bool, 'message': str}
    """
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„JSON
        data = json.loads(script_content)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ—è¡¨
        if not isinstance(data, list):
            return {
                'success': False,
                'message': 'è„šæœ¬å¿…é¡»æ˜¯JSONæ•°ç»„æ ¼å¼'
            }
        
        # æ£€æŸ¥æ¯ä¸ªç‰‡æ®µ
        for i, clip in enumerate(data):
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ['narration', 'picture', 'timestamp']
            for field in required_fields:
                if field not in clip:
                    return {
                        'success': False,
                        'message': f'ç¬¬{i+1}ä¸ªç‰‡æ®µç¼ºå°‘å¿…éœ€å­—æ®µ: {field}'
                    }
            
            # æ£€æŸ¥å­—æ®µç±»å‹
            if not isinstance(clip['narration'], str):
                return {
                    'success': False,
                    'message': f'ç¬¬{i+1}ä¸ªç‰‡æ®µçš„narrationå¿…é¡»æ˜¯å­—ç¬¦ä¸²'
                }
            if not isinstance(clip['picture'], str):
                return {
                    'success': False,
                    'message': f'ç¬¬{i+1}ä¸ªç‰‡æ®µçš„pictureå¿…é¡»æ˜¯å­—ç¬¦ä¸²'
                }
            if not isinstance(clip['timestamp'], str):
                return {
                    'success': False,
                    'message': f'ç¬¬{i+1}ä¸ªç‰‡æ®µçš„timestampå¿…é¡»æ˜¯å­—ç¬¦ä¸²'
                }
            
            # æ£€æŸ¥å­—æ®µå†…å®¹ä¸èƒ½ä¸ºç©º
            if not clip['narration'].strip():
                return {
                    'success': False,
                    'message': f'ç¬¬{i+1}ä¸ªç‰‡æ®µçš„narrationä¸èƒ½ä¸ºç©º'
                }
            if not clip['picture'].strip():
                return {
                    'success': False,
                    'message': f'ç¬¬{i+1}ä¸ªç‰‡æ®µçš„pictureä¸èƒ½ä¸ºç©º'
                }
            if not clip['timestamp'].strip():
                return {
                    'success': False,
                    'message': f'ç¬¬{i+1}ä¸ªç‰‡æ®µçš„timestampä¸èƒ½ä¸ºç©º'
                }

        return {
            'success': True,
            'message': 'è„šæœ¬æ ¼å¼æ£€æŸ¥é€šè¿‡'
        }

    except json.JSONDecodeError as e:
        return {
            'success': False,
            'message': f'JSONæ ¼å¼é”™è¯¯: {str(e)}'
        }
    except Exception as e:
        return {
            'success': False,
            'message': f'æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}'
        }
</file>

<file path="app/utils/gemini_analyzer.py">
import json
from typing import List, Union, Dict
import os
from pathlib import Path
from loguru import logger
from tqdm import tqdm
import asyncio
from tenacity import retry, stop_after_attempt, RetryError, retry_if_exception_type, wait_exponential
from google.api_core import exceptions
import google.generativeai as genai
import PIL.Image
import traceback
from app.utils import utils


class VisionAnalyzer:
    """è§†è§‰åˆ†æå™¨ç±»"""

    def __init__(self, model_name: str = "gemini-1.5-flash", api_key: str = None):
        """åˆå§‹åŒ–è§†è§‰åˆ†æå™¨"""
        if not api_key:
            raise ValueError("å¿…é¡»æä¾›APIå¯†é’¥")

        self.model_name = model_name
        self.api_key = api_key

        # åˆå§‹åŒ–é…ç½®
        self._configure_client()

    def _configure_client(self):
        """é…ç½®APIå®¢æˆ·ç«¯"""
        genai.configure(api_key=self.api_key)
        # å¼€æ”¾ Gemini æ¨¡å‹å®‰å…¨è®¾ç½®
        from google.generativeai.types import HarmCategory, HarmBlockThreshold
        safety_settings = {
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        }
        self.model = genai.GenerativeModel(self.model_name, safety_settings=safety_settings)

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type(exceptions.ResourceExhausted)
    )
    async def _generate_content_with_retry(self, prompt, batch):
        """ä½¿ç”¨é‡è¯•æœºåˆ¶çš„å†…éƒ¨æ–¹æ³•æ¥è°ƒç”¨ generate_content_async"""
        try:
            return await self.model.generate_content_async([prompt, *batch])
        except exceptions.ResourceExhausted as e:
            print(f"APIé…é¢é™åˆ¶: {str(e)}")
            raise RetryError("APIè°ƒç”¨å¤±è´¥")

    async def analyze_images(self,
                           images: Union[List[str], List[PIL.Image.Image]],
                           prompt: str,
                           batch_size: int) -> List[Dict]:
        """æ‰¹é‡åˆ†æå¤šå¼ å›¾ç‰‡"""
        try:
            # åŠ è½½å›¾ç‰‡
            if isinstance(images[0], str):
                logger.info("æ­£åœ¨åŠ è½½å›¾ç‰‡...")
                images = self.load_images(images)

            # éªŒè¯å›¾ç‰‡åˆ—è¡¨
            if not images:
                raise ValueError("å›¾ç‰‡åˆ—è¡¨ä¸ºç©º")

            # éªŒè¯æ¯ä¸ªå›¾ç‰‡å¯¹è±¡
            valid_images = []
            for i, img in enumerate(images):
                if not isinstance(img, PIL.Image.Image):
                    logger.error(f"æ— æ•ˆçš„å›¾ç‰‡å¯¹è±¡ï¼Œç´¢å¼• {i}: {type(img)}")
                    continue
                valid_images.append(img)

            if not valid_images:
                raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡å¯¹è±¡")

            images = valid_images
            results = []
            total_batches = (len(images) + batch_size - 1) // batch_size

            logger.debug(f"å…± {total_batches} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹æ¬¡ {batch_size} å¼ å›¾ç‰‡")

            with tqdm(total=total_batches, desc="åˆ†æè¿›åº¦") as pbar:
                for i in range(0, len(images), batch_size):
                    batch = images[i:i + batch_size]
                    retry_count = 0

                    while retry_count < 3:
                        try:
                            # åœ¨æ¯ä¸ªæ‰¹æ¬¡å¤„ç†å‰æ·»åŠ å°å»¶è¿Ÿ
                            if i > 0:
                                await asyncio.sleep(2)

                            # ç¡®ä¿æ¯ä¸ªæ‰¹æ¬¡çš„å›¾ç‰‡éƒ½æ˜¯æœ‰æ•ˆçš„
                            valid_batch = [img for img in batch if isinstance(img, PIL.Image.Image)]
                            if not valid_batch:
                                raise ValueError(f"æ‰¹æ¬¡ {i // batch_size} ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡")

                            response = await self._generate_content_with_retry(prompt, valid_batch)
                            results.append({
                                'batch_index': i // batch_size,
                                'images_processed': len(valid_batch),
                                'response': response.text,
                                'model_used': self.model_name
                            })
                            break

                        except Exception as e:
                            retry_count += 1
                            error_msg = f"æ‰¹æ¬¡ {i // batch_size} å¤„ç†å‡ºé”™: {str(e)}"
                            logger.error(error_msg)

                            if retry_count >= 3:
                                results.append({
                                    'batch_index': i // batch_size,
                                    'images_processed': len(batch),
                                    'error': error_msg,
                                    'model_used': self.model_name
                                })
                            else:
                                logger.info(f"æ‰¹æ¬¡ {i // batch_size} å¤„ç†å¤±è´¥ï¼Œç­‰å¾…60ç§’åé‡è¯•å½“å‰æ‰¹æ¬¡...")
                                await asyncio.sleep(60)

                    pbar.update(1)

            return results

        except Exception as e:
            error_msg = f"å›¾ç‰‡åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}\n{traceback.format_exc()}"
            logger.error(error_msg)
            raise Exception(error_msg)

    def save_results_to_txt(self, results: List[Dict], output_dir: str):
        """å°†åˆ†æç»“æœä¿å­˜åˆ°txtæ–‡ä»¶"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        for result in results:
            if not result.get('image_paths'):
                continue

            response_text = result['response']
            image_paths = result['image_paths']

            # ä»æ–‡ä»¶åä¸­æå–æ—¶é—´æˆ³å¹¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            def format_timestamp(img_path):
                # ä»æ–‡ä»¶åä¸­æå–æ—¶é—´éƒ¨åˆ†
                timestamp = Path(img_path).stem.split('_')[-1]
                try:
                    # å°†æ—¶é—´è½¬æ¢ä¸ºç§’
                    seconds = utils.time_to_seconds(timestamp.replace('_', ':'))
                    # è½¬æ¢ä¸º HH:MM:SS,mmm æ ¼å¼
                    hours = int(seconds // 3600)
                    minutes = int((seconds % 3600) // 60)
                    seconds_remainder = seconds % 60
                    whole_seconds = int(seconds_remainder)
                    milliseconds = int((seconds_remainder - whole_seconds) * 1000)
                    
                    return f"{hours:02d}:{minutes:02d}:{whole_seconds:02d},{milliseconds:03d}"
                except Exception as e:
                    logger.error(f"æ—¶é—´æˆ³æ ¼å¼è½¬æ¢é”™è¯¯: {timestamp}, {str(e)}")
                    return timestamp

            start_timestamp = format_timestamp(image_paths[0])
            end_timestamp = format_timestamp(image_paths[-1])
            
            txt_path = os.path.join(output_dir, f"frame_{start_timestamp}_{end_timestamp}.txt")

            # ä¿å­˜ç»“æœåˆ°txtæ–‡ä»¶
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(response_text.strip())
            logger.info(f"å·²ä¿å­˜åˆ†æç»“æœåˆ°: {txt_path}")

    def load_images(self, image_paths: List[str]) -> List[PIL.Image.Image]:
        """
        åŠ è½½å¤šå¼ å›¾ç‰‡
        Args:
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        Returns:
            åŠ è½½åçš„PIL Imageå¯¹è±¡åˆ—è¡¨
        """
        images = []
        failed_images = []

        for img_path in image_paths:
            try:
                if not os.path.exists(img_path):
                    logger.error(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
                    failed_images.append(img_path)
                    continue

                img = PIL.Image.open(img_path)
                # ç¡®ä¿å›¾ç‰‡è¢«å®Œå…¨åŠ è½½
                img.load()
                # è½¬æ¢ä¸ºRGBæ¨¡å¼
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                images.append(img)

            except Exception as e:
                logger.error(f"æ— æ³•åŠ è½½å›¾ç‰‡ {img_path}: {str(e)}")
                failed_images.append(img_path)

        if failed_images:
            logger.warning(f"ä»¥ä¸‹å›¾ç‰‡åŠ è½½å¤±è´¥:\n{json.dumps(failed_images, indent=2, ensure_ascii=False)}")

        if not images:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å›¾ç‰‡")

        return images
</file>

<file path="app/utils/qwenvl_analyzer.py">
import json
from typing import List, Union, Dict
import os
from pathlib import Path
from loguru import logger
from tqdm import tqdm
import asyncio
from tenacity import retry, stop_after_attempt, RetryError, wait_exponential
from openai import OpenAI
import PIL.Image
import base64
import io
import traceback


class QwenAnalyzer:
    """åƒé—®è§†è§‰åˆ†æå™¨ç±»"""

    def __init__(self, model_name: str = "qwen-vl-max-latest", api_key: str = None, base_url: str = None):
        """
        åˆå§‹åŒ–åƒé—®è§†è§‰åˆ†æå™¨
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨ qwen-vl-max-latest
            api_key: é˜¿é‡Œäº‘APIå¯†é’¥
            base_url: APIåŸºç¡€URLï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        """
        if not api_key:
            raise ValueError("å¿…é¡»æä¾›APIå¯†é’¥")

        self.model_name = model_name
        self.api_key = api_key
        self.base_url = base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"

        # é…ç½®APIå®¢æˆ·ç«¯
        self._configure_client()

    def _configure_client(self):
        """
        é…ç½®APIå®¢æˆ·ç«¯
        ä½¿ç”¨æœ€ç®€åŒ–çš„å‚æ•°é…ç½®ï¼Œé¿å…ä¸å¿…è¦çš„å‚æ•°
        """
        try:
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            raise

    def _image_to_base64(self, image: PIL.Image.Image) -> str:
        """
        å°†PILå›¾ç‰‡å¯¹è±¡è½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²
        """
        buffered = io.BytesIO()
        image.save(buffered, format="JPEG")
        return base64.b64encode(buffered.getvalue()).decode("utf-8")

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def _generate_content_with_retry(self, prompt: str, batch: List[PIL.Image.Image]):
        """ä½¿ç”¨é‡è¯•æœºåˆ¶çš„å†…éƒ¨æ–¹æ³•æ¥è°ƒç”¨åƒé—®API"""
        try:
            # æ„å»ºæ¶ˆæ¯å†…å®¹
            content = []

            # æ·»åŠ å›¾ç‰‡
            for img in batch:
                base64_image = self._image_to_base64(img)
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}"
                    }
                })

            # æ·»åŠ æ–‡æœ¬æç¤º
            content.append({
                "type": "text",
                "text": prompt
            })

            # è°ƒç”¨API
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=self.model_name,
                messages=[{
                    "role": "user",
                    "content": content
                }]
            )

            return response.choices[0].message.content

        except Exception as e:
            logger.error(f"APIè°ƒç”¨é”™è¯¯: {str(e)}")
            raise RetryError("APIè°ƒç”¨å¤±è´¥")

    async def analyze_images(self,
                             images: Union[List[str], List[PIL.Image.Image]],
                             prompt: str,
                             batch_size: int = 5) -> List[Dict]:
        """
        æ‰¹é‡åˆ†æå¤šå¼ å›¾ç‰‡
        Args:
            images: å›¾ç‰‡è·¯å¾„åˆ—è¡¨æˆ–PILå›¾ç‰‡å¯¹è±¡åˆ—è¡¨
            prompt: åˆ†ææç¤ºè¯
            batch_size: æ‰¹å¤„ç†å¤§å°
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        try:
            # ä¿å­˜åŸå§‹å›¾ç‰‡è·¯å¾„ï¼ˆå¦‚æœæ˜¯è·¯å¾„åˆ—è¡¨çš„è¯ï¼‰
            original_paths = images if isinstance(images[0], str) else None

            # åŠ è½½å›¾ç‰‡
            if isinstance(images[0], str):
                logger.info("æ­£åœ¨åŠ è½½å›¾ç‰‡...")
                images = self.load_images(images)

            # éªŒè¯å›¾ç‰‡åˆ—è¡¨
            if not images:
                raise ValueError("å›¾ç‰‡åˆ—è¡¨ä¸ºç©º")

            # éªŒè¯æ¯ä¸ªå›¾ç‰‡å¯¹è±¡
            valid_images = []
            valid_paths = []
            for i, img in enumerate(images):
                if not isinstance(img, PIL.Image.Image):
                    logger.error(f"æ— æ•ˆçš„å›¾ç‰‡å¯¹è±¡ï¼Œç´¢å¼• {i}: {type(img)}")
                    continue
                valid_images.append(img)
                if original_paths:
                    valid_paths.append(original_paths[i])

            if not valid_images:
                raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡å¯¹è±¡")

            images = valid_images
            results = []
            total_batches = (len(images) + batch_size - 1) // batch_size

            with tqdm(total=total_batches, desc="åˆ†æè¿›åº¦") as pbar:
                for i in range(0, len(images), batch_size):
                    batch = images[i:i + batch_size]
                    batch_paths = valid_paths[i:i + batch_size] if valid_paths else None
                    retry_count = 0

                    while retry_count < 3:
                        try:
                            # åœ¨æ¯ä¸ªæ‰¹æ¬¡å¤„ç†å‰ï¿½ï¿½åŠ å°å»¶è¿Ÿ
                            if i > 0:
                                await asyncio.sleep(2)

                            # ç¡®ä¿æ¯ä¸ªæ‰¹æ¬¡çš„å›¾ç‰‡éƒ½æ˜¯æœ‰æ•ˆçš„
                            valid_batch = [img for img in batch if isinstance(img, PIL.Image.Image)]
                            if not valid_batch:
                                raise ValueError(f"æ‰¹æ¬¡ {i // batch_size} ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡")

                            response = await self._generate_content_with_retry(prompt, valid_batch)
                            result_dict = {
                                'batch_index': i // batch_size,
                                'images_processed': len(valid_batch),
                                'response': response,
                                'model_used': self.model_name
                            }

                            # æ·»åŠ å›¾ç‰‡è·¯å¾„ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                            if batch_paths:
                                result_dict['image_paths'] = batch_paths

                            results.append(result_dict)
                            break

                        except Exception as e:
                            retry_count += 1
                            error_msg = f"æ‰¹æ¬¡ {i // batch_size} å¤„ç†å‡ºé”™: {str(e)}"
                            logger.error(error_msg)

                            if retry_count >= 3:
                                results.append({
                                    'batch_index': i // batch_size,
                                    'images_processed': len(batch),
                                    'error': error_msg,
                                    'model_used': self.model_name,
                                    'image_paths': batch_paths if batch_paths else []
                                })
                            else:
                                logger.info(f"æ‰¹æ¬¡ {i // batch_size} å¤„ç†å¤±è´¥ï¼Œç­‰å¾…60ç§’åé‡è¯•å½“å‰æ‰¹æ¬¡...")
                                await asyncio.sleep(60)

                    pbar.update(1)

            return results

        except Exception as e:
            error_msg = f"å›¾ç‰‡åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}\n{traceback.format_exc()}"
            logger.error(error_msg)
            raise Exception(error_msg)

    def save_results_to_txt(self, results: List[Dict], output_dir: str):
        """å°†åˆ†æç»“æœä¿å­˜åˆ°txtæ–‡ä»¶"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        for i, result in enumerate(results):
            response_text = result['response']

            # å¦‚æœæœ‰å›¾ç‰‡è·¯å¾„ä¿¡æ¯ï¼Œï¿½ï¿½ï¿½ç”¨å®ƒæ¥ç”Ÿæˆæ–‡ä»¶å
            if result.get('image_paths'):
                image_paths = result['image_paths']
                img_name_start = Path(image_paths[0]).stem.split('_')[-1]
                img_name_end = Path(image_paths[-1]).stem.split('_')[-1]
                file_name = f"frame_{img_name_start}_{img_name_end}.txt"
            else:
                # å¦‚æœæ²¡æœ‰è·¯å¾„ä¿¡æ¯ï¼Œä½¿ç”¨æ‰¹æ¬¡ç´¢å¼•
                file_name = f"batch_{result['batch_index']}.txt"

            txt_path = os.path.join(output_dir, file_name)

            # ä¿å­˜ç»“æœåˆ°txtæ–‡ä»¶
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(response_text.strip())
            logger.info(f"å·²ä¿å­˜åˆ†æç»“æœåˆ°: {txt_path}")

    def load_images(self, image_paths: List[str]) -> List[PIL.Image.Image]:
        """
        åŠ è½½å¤šå¼ å›¾ç‰‡
        Args:
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        Returns:
            åŠ è½½åçš„PIL Imageå¯¹è±¡åˆ—è¡¨
        """
        images = []
        failed_images = []

        for img_path in image_paths:
            try:
                if not os.path.exists(img_path):
                    logger.error(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
                    failed_images.append(img_path)
                    continue

                img = PIL.Image.open(img_path)
                # ç¡®ä¿å›¾ç‰‡è¢«å®Œå…¨åŠ è½½
                img.load()
                # è½¬æ¢ä¸ºRGBæ¨¡å¼
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                images.append(img)

            except Exception as e:
                logger.error(f"æ— æ³•åŠ è½½å›¾ç‰‡ {img_path}: {str(e)}")
                failed_images.append(img_path)

        if failed_images:
            logger.warning(f"ä»¥ä¸‹å›¾ç‰‡åŠ è½½å¤±è´¥:\n{json.dumps(failed_images, indent=2, ensure_ascii=False)}")

        if not images:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å›¾ç‰‡")

        return images
</file>

<file path="app/utils/script_generator.py">
import os
import json
import traceback
from loguru import logger
import tiktoken
from typing import List, Dict
from datetime import datetime
from openai import OpenAI
import google.generativeai as genai
import time


class BaseGenerator:
    def __init__(self, model_name: str, api_key: str, prompt: str):
        self.model_name = model_name
        self.api_key = api_key
        self.base_prompt = prompt
        self.conversation_history = []
        self.chunk_overlap = 50
        self.last_chunk_ending = ""
        self.default_params = {
            "temperature": 0.7,
            "max_tokens": 500,
            "top_p": 0.9,
            "frequency_penalty": 0.3,
            "presence_penalty": 0.5
        }

    def _try_generate(self, messages: list, params: dict = None) -> str:
        max_attempts = 3
        tolerance = 5
        
        for attempt in range(max_attempts):
            try:
                response = self._generate(messages, params or self.default_params)
                return self._process_response(response)
            except Exception as e:
                if attempt == max_attempts - 1:
                    raise
                logger.warning(f"Generation attempt {attempt + 1} failed: {str(e)}")
                continue
        return ""

    def _generate(self, messages: list, params: dict) -> any:
        raise NotImplementedError
        
    def _process_response(self, response: any) -> str:
        return response

    def generate_script(self, scene_description: str, word_count: int) -> str:
        """ç”Ÿæˆè„šæœ¬çš„é€šç”¨æ–¹æ³•"""
        prompt = f"""{self.base_prompt}

ä¸Šä¸€æ®µæ–‡æ¡ˆçš„ç»“å°¾ï¼š{self.last_chunk_ending if self.last_chunk_ending else "è¿™æ˜¯ç¬¬ä¸€æ®µï¼Œæ— éœ€è€ƒè™‘ä¸Šæ–‡"}

å½“å‰ç”»é¢æè¿°ï¼š{scene_description}

è¯·ç¡®ä¿æ–°ç”Ÿæˆçš„æ–‡æ¡ˆä¸ä¸Šæ–‡è‡ªç„¶è¡”æ¥ï¼Œä¿æŒå™äº‹çš„è¿è´¯æ€§å’Œè¶£å‘³æ€§ã€‚
ä¸è¦å‡ºç°é™¤äº†æ–‡æ¡ˆä»¥å¤–çš„å…¶ä»–ä»»ä½•å†…å®¹ï¼›
ä¸¥æ ¼å­—æ•°è¦æ±‚ï¼š{word_count}å­—ï¼Œå…è®¸è¯¯å·®Â±5å­—ã€‚"""

        messages = [
            {"role": "system", "content": self.base_prompt},
            {"role": "user", "content": prompt}
        ]

        try:
            generated_script = self._try_generate(messages, self.default_params)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            if generated_script:
                self.last_chunk_ending = generated_script[-self.chunk_overlap:] if len(
                    generated_script) > self.chunk_overlap else generated_script
                
            return generated_script
            
        except Exception as e:
            logger.error(f"Script generation failed: {str(e)}")
            raise


class OpenAIGenerator(BaseGenerator):
    """OpenAI API ç”Ÿæˆå™¨å®ç°"""
    def __init__(self, model_name: str, api_key: str, prompt: str, base_url: str):
        super().__init__(model_name, api_key, prompt)
        base_url = base_url or f"https://api.openai.com/v1"
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.max_tokens = 5000
        
        # OpenAIç‰¹å®šå‚æ•°
        self.default_params = {
            **self.default_params,
            "stream": False,
            "user": "script_generator"
        }
        
        # åˆå§‹åŒ–tokenè®¡æ•°å™¨
        try:
            self.encoding = tiktoken.encoding_for_model(self.model_name)
        except KeyError:
            logger.warning(f"æœªæ‰¾åˆ°æ¨¡å‹ {self.model_name} çš„ä¸“ç”¨ç¼–ç å™¨ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç å™¨")
            self.encoding = tiktoken.get_encoding("cl100k_base")

    def _generate(self, messages: list, params: dict) -> any:
        """å®ç°OpenAIç‰¹å®šçš„ç”Ÿæˆé€»è¾‘"""
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                **params
            )
            return response
        except Exception as e:
            logger.error(f"OpenAI generation error: {str(e)}")
            raise

    def _process_response(self, response: any) -> str:
        """å¤„ç†OpenAIçš„å“åº”"""
        if not response or not response.choices:
            raise ValueError("Invalid response from OpenAI API")
        return response.choices[0].message.content.strip()

    def _count_tokens(self, messages: list) -> int:
        """è®¡ç®—tokenæ•°é‡"""
        num_tokens = 0
        for message in messages:
            num_tokens += 3
            for key, value in message.items():
                num_tokens += len(self.encoding.encode(str(value)))
                if key == "role":
                    num_tokens += 1
        num_tokens += 3
        return num_tokens


class GeminiGenerator(BaseGenerator):
    """Google Gemini API ç”Ÿæˆå™¨å®ç°"""
    def __init__(self, model_name: str, api_key: str, prompt: str):
        super().__init__(model_name, api_key, prompt)
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        
        # Geminiç‰¹å®šå‚æ•°
        self.default_params = {
            "temperature": self.default_params["temperature"],
            "top_p": self.default_params["top_p"],
            "candidate_count": 1,
            "stop_sequences": None
        }

    def _generate(self, messages: list, params: dict) -> any:
        """å®ç°Geminiç‰¹å®šçš„ç”Ÿæˆé€»è¾‘"""
        while True:
            try:
                # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºGeminiæ ¼å¼
                prompt = "\n".join([m["content"] for m in messages])
                response = self.model.generate_content(
                    prompt,
                    generation_config=params
                )
                
                # æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«æœ‰æ•ˆå†…å®¹
                if (hasattr(response, 'result') and 
                    hasattr(response.result, 'candidates') and 
                    response.result.candidates):
                    
                    candidate = response.result.candidates[0]
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹å­—æ®µ
                    if not hasattr(candidate, 'content'):
                        logger.warning("Gemini API è¿”å›é€Ÿç‡é™åˆ¶å“åº”ï¼Œç­‰å¾…30ç§’åé‡è¯•...")
                        time.sleep(30)  # ç­‰å¾…3ç§’åé‡è¯•
                        continue
                return response
                
            except Exception as e:
                error_str = str(e)
                if "429" in error_str:
                    logger.warning("Gemini API è§¦å‘é™æµï¼Œç­‰å¾…65ç§’åé‡è¯•...")
                    time.sleep(65)  # ç­‰å¾…65ç§’åé‡è¯•
                    continue
                else:
                    logger.error(f"Gemini ç”Ÿæˆæ–‡æ¡ˆé”™è¯¯: \n{error_str}")
                    raise

    def _process_response(self, response: any) -> str:
        """å¤„ç†Geminiçš„å“åº”"""
        if not response or not response.text:
            raise ValueError("Invalid response from Gemini API")
        return response.text.strip()


class QwenGenerator(BaseGenerator):
    """é˜¿é‡Œäº‘åƒé—® API ç”Ÿæˆå™¨å®ç°"""
    def __init__(self, model_name: str, api_key: str, prompt: str, base_url: str):
        super().__init__(model_name, api_key, prompt)
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        
        # Qwenç‰¹å®šå‚æ•°
        self.default_params = {
            **self.default_params,
            "stream": False,
            "user": "script_generator"
        }

    def _generate(self, messages: list, params: dict) -> any:
        """å®ç°åƒé—®ç‰¹å®šçš„ç”Ÿæˆé€»è¾‘"""
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                **params
            )
            return response
        except Exception as e:
            logger.error(f"Qwen generation error: {str(e)}")
            raise

    def _process_response(self, response: any) -> str:
        """å¤„ç†åƒé—®çš„å“åº”"""
        if not response or not response.choices:
            raise ValueError("Invalid response from Qwen API")
        return response.choices[0].message.content.strip()


class MoonshotGenerator(BaseGenerator):
    """Moonshot API ç”Ÿæˆå™¨å®ç°"""
    def __init__(self, model_name: str, api_key: str, prompt: str, base_url: str):
        super().__init__(model_name, api_key, prompt)
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url or "https://api.moonshot.cn/v1"
        )
        
        # Moonshotç‰¹å®šå‚æ•°
        self.default_params = {
            **self.default_params,
            "stream": False,
            "stop": None,
            "user": "script_generator",
            "tools": None
        }

    def _generate(self, messages: list, params: dict) -> any:
        """å®ç°Moonshotç‰¹å®šçš„ç”Ÿæˆé€»è¾‘ï¼ŒåŒ…å«429è¯¯é‡è¯•æœºåˆ¶"""
        while True:
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    **params
                )
                return response
            except Exception as e:
                error_str = str(e)
                if "Error code: 429" in error_str:
                    logger.warning("Moonshot API è§¦å‘é™æµï¼Œç­‰å¾…65ç§’åé‡è¯•...")
                    time.sleep(65)  # ç­‰å¾…65ç§’åé‡è¯•
                    continue
                else:
                    logger.error(f"Moonshot generation error: {error_str}")
                    raise

    def _process_response(self, response: any) -> str:
        """å¤„ç†Moonshotçš„å“åº”"""
        if not response or not response.choices:
            raise ValueError("Invalid response from Moonshot API")
        return response.choices[0].message.content.strip()


class DeepSeekGenerator(BaseGenerator):
    """DeepSeek API ç”Ÿæˆå™¨å®ç°"""
    def __init__(self, model_name: str, api_key: str, prompt: str, base_url: str):
        super().__init__(model_name, api_key, prompt)
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url or "https://api.deepseek.com"
        )
        
        # DeepSeekç‰¹å®šå‚æ•°
        self.default_params = {
            **self.default_params,
            "stream": False,
            "user": "script_generator"
        }

    def _generate(self, messages: list, params: dict) -> any:
        """å®ç°DeepSeekç‰¹å®šçš„ç”Ÿæˆé€»è¾‘"""
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,  # deepseek-chat æˆ– deepseek-coder
                messages=messages,
                **params
            )
            return response
        except Exception as e:
            logger.error(f"DeepSeek generation error: {str(e)}")
            raise

    def _process_response(self, response: any) -> str:
        """å¤„ç†DeepSeekçš„å“åº”"""
        if not response or not response.choices:
            raise ValueError("Invalid response from DeepSeek API")
        return response.choices[0].message.content.strip()


class ScriptProcessor:
    def __init__(self, model_name: str, api_key: str = None, base_url: str = None, prompt: str = None, video_theme: str = ""):
        self.model_name = model_name
        self.api_key = api_key
        self.base_url = base_url
        self.video_theme = video_theme
        self.prompt = prompt or self._get_default_prompt()

        # æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©å¯¹åº”çš„ç”Ÿæˆå™¨
        logger.info(f"æ–‡æœ¬ LLM æä¾›å•†: {model_name}")
        if 'gemini' in model_name.lower():
            self.generator = GeminiGenerator(model_name, self.api_key, self.prompt)
        elif 'qwen' in model_name.lower():
            self.generator = QwenGenerator(model_name, self.api_key, self.prompt, self.base_url)
        elif 'moonshot' in model_name.lower():
            self.generator = MoonshotGenerator(model_name, self.api_key, self.prompt, self.base_url)
        elif 'deepseek' in model_name.lower():
            self.generator = DeepSeekGenerator(model_name, self.api_key, self.prompt, self.base_url)
        else:
            self.generator = OpenAIGenerator(model_name, self.api_key, self.prompt, self.base_url)

    def _get_default_prompt(self) -> str:
        return f"""
        ä½ æ˜¯ä¸€ä½æå…·å¹½é»˜æ„Ÿçš„çŸ­è§†é¢‘è„šæœ¬åˆ›ä½œå¤§å¸ˆï¼Œæ“…é•¿ç”¨"æ¸©å’Œçš„è¿å"åˆ¶é€ ç¬‘ç‚¹ï¼Œè®©ä¸»é¢˜ä¸º ã€Š{self.video_theme}ã€‹ çš„è§†é¢‘æ—¢æœ‰è¶£åˆå¯Œæœ‰ä¼ æ’­åŠ›ã€‚
ä½ çš„ä»»åŠ¡æ˜¯å°†è§†é¢‘ç”»é¢æè¿°è½¬åŒ–ä¸ºèƒ½åœ¨ç¤¾äº¤å¹³å°ç–¯ç‹‚ä¼ æ’­çš„çˆ†æ¬¾å£æ’­æ–‡æ¡ˆã€‚

ç›®æ ‡å—ä¼—ï¼šçƒ­çˆ±ç”Ÿæ´»ã€è¿½æ±‚ç‹¬ç‰¹ä½“éªŒçš„18-35å²å¹´è½»äºº
æ–‡æ¡ˆé£æ ¼ï¼šåŸºäºHKRRç†è®º + æ®µå­æ‰‹ç²¾ç¥
ä¸»é¢˜ï¼š{self.video_theme}

ã€åˆ›ä½œæ ¸å¿ƒç†å¿µã€‘
1. æ•¢äºç”¨"æ¸©å’Œçš„è¿å"åˆ¶é€ ç¬‘ç‚¹ï¼Œä½†ä¸èƒ½è¿‡äºå†’çŠ¯
2. å·§å¦™è¿ç”¨ä¸­å›½å¼å¹½é»˜ï¼Œè®©è§‚ä¼—ä¼šå¿ƒä¸€ç¬‘
3. ä¿æŒè½»æ¾æ„‰å¿«çš„å™äº‹åŸºè°ƒ

ã€çˆ†æ¬¾å†…å®¹å››è¦ç´ ã€‘

ã€å¿«ä¹å…ƒç´  Happyã€‘
1. ç”¨è°ƒä¾ƒçš„è¯­æ°”æè¿°ç”»é¢
2. å·§å¦™æ¤å…¥ç½‘ç»œæµè¡Œæ¢—ï¼Œå¢åŠ å†…å®¹çš„ä¼ æ’­æ€§
3. é€‚æ—¶è‡ªå˜²ï¼Œå±•ç°çœŸå®ä¸”æœ‰è¶£çš„ä¸€é¢

ã€çŸ¥è¯†ä»·å€¼ Knowledgeã€‘
1. ç”¨æ®µå­æ‰‹çš„æ–¹å¼è§£é‡Šä¸“ä¸šçŸ¥è¯†
2. åœ¨å¹½é»˜ä¸­ä¼ é€’å®ç”¨çš„ç”Ÿæ´»å¸¸è¯†

ã€æƒ…æ„Ÿå…±é¸£ Resonanceã€‘
1. æè¿°"çœŸå®ä½†å¤¸å¼ "çš„ç¯å¢ƒæè¿°
2. æŠŠå¯¹è‡ªç„¶çš„æ„Ÿæ‚Ÿèå…¥ä¿çš®è¯ä¸­
3. ç”¨æ¥åœ°æ°”çš„è¡¨è¾¾æ–¹å¼æ‹‰è¿‘ä¸è§‚ä¼—è·ç¦»

ã€èŠ‚å¥æ§åˆ¶ Rhythmã€‘
1. åƒè®²æ®µå­ä¸€æ ·ï¼Œæ³¨æ„é“ºå«å’ŒåŒ…è¢±çš„èŠ‚å¥
2. ç¡®ä¿æ¯æ®µéƒ½æœ‰ç¬‘ç‚¹ï¼Œä½†ä¸å¼ºæ±‚
3. æ®µè½ç»“å°¾å¹²å‡€åˆ©è½ï¼Œä¸æ‹–æ³¥å¸¦æ°´

ã€è¿è´¯æ€§è¦æ±‚ã€‘
1. æ–°ç”Ÿæˆçš„å†…å®¹å¿…é¡»è‡ªç„¶è¡”æ¥ä¸Šä¸€æ®µæ–‡æ¡ˆçš„ç»“å°¾
2. ä½¿ç”¨æ°å½“çš„è¿æ¥è¯å’Œè¿‡æ¸¡è¯­ï¼Œç¡®ä¿å™äº‹æµç•…
3. ä¿æŒäººç‰©è§†è§’å’Œè¯­æ°”çš„ä¸€è‡´æ€§
4. é¿å…é‡å¤ä¸Šä¸€æ®µå·²ç»æåˆ°çš„ä¿¡æ¯
5. ç¡®ä¿æƒ…èŠ‚çš„é€»è¾‘è¿ç»­æ€§

æˆ‘ä¼šæŒ‰é¡ºåºæä¾›å¤šæ®µè§†é¢‘ç”»é¢æè¿°ã€‚è¯·åˆ›ä½œæ—¢æç¬‘åˆèƒ½ç«çˆ†å…¨ç½‘çš„å£æ’­æ–‡æ¡ˆã€‚
è®°ä½ï¼šè¦æ•¢äºç”¨"æ¸©å’Œçš„è¿å"åˆ¶é€ ç¬‘ç‚¹ï¼Œä½†è¦æŠŠæ¡å¥½å°ºåº¦ï¼Œè®©è§‚ä¼—åœ¨è½»æ¾æ„‰å¿«ä¸­æ„Ÿå—åˆ°ä¹è¶£ã€‚"""

    def calculate_duration_and_word_count(self, time_range: str) -> int:
        """
        è®¡ç®—æ—¶é—´èŒƒå›´çš„æŒç»­æ—¶é•¿å¹¶ä¼°ç®—åˆé€‚çš„å­—æ•°
        
        Args:
            time_range: æ—¶é—´èŒƒå›´å­—ç¬¦ä¸²,æ ¼å¼ä¸º "HH:MM:SS,mmm-HH:MM:SS,mmm"
                       ä¾‹å¦‚: "00:00:50,100-00:01:21,500"
        
        Returns:
            int: ä¼°ç®—çš„åˆé€‚å­—æ•°
                  åŸºäºç»éªŒå…¬å¼: æ¯0.35ç§’å¯ä»¥è¯´ä¸€ä¸ªå­—
                  ä¾‹å¦‚: 10ç§’å¯ä»¥è¯´çº¦28ä¸ªå­— (10/0.35â‰ˆ28.57)
        """
        try:
            start_str, end_str = time_range.split('-')
            
            def time_to_seconds(time_str: str) -> float:
                """
                å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°(å¸¦æ¯«ç§’ç²¾åº¦)
                
                Args:
                    time_str: æ—¶é—´å­—ç¬¦ä¸²,æ ¼å¼ä¸º "HH:MM:SS,mmm"
                             ä¾‹å¦‚: "00:00:50,100" è¡¨ç¤º50.1ç§’
                
                Returns:
                    float: è½¬æ¢åçš„ç§’æ•°(å¸¦æ¯«ç§’)
                """
                try:
                    # å¤„ç†æ¯«ç§’éƒ¨åˆ†
                    time_part, ms_part = time_str.split(',')
                    hours, minutes, seconds = map(int, time_part.split(':'))
                    milliseconds = int(ms_part)
                    
                    # è½¬æ¢ä¸ºç§’
                    total_seconds = (hours * 3600) + (minutes * 60) + seconds + (milliseconds / 1000)
                    return total_seconds
                    
                except ValueError as e:
                    logger.warning(f"æ—¶é—´æ ¼å¼è§£æé”™è¯¯: {time_str}, error: {e}")
                    return 0.0
            
            # è®¡ç®—å¼€å§‹å’Œç»“æŸæ—¶é—´çš„ç§’æ•°
            start_seconds = time_to_seconds(start_str)
            end_seconds = time_to_seconds(end_str)
            
            # è®¡ç®—æŒç»­æ—¶é—´(ç§’)
            duration = end_seconds - start_seconds
            
            # æ ¹æ®ç»éªŒå…¬å¼è®¡ç®—å­—æ•°: æ¯0.5ç§’ä¸€ä¸ªå­—
            word_count = int(duration / 0.4)
            
            # ç¡®ä¿å­—æ•°åœ¨åˆç†èŒƒå›´å†…
            word_count = max(10, min(word_count, 500))  # é™åˆ¶åœ¨10-500å­—ä¹‹é—´
            
            logger.debug(f"æ—¶é—´èŒƒå›´ {time_range} çš„æŒç»­æ—¶é—´ä¸º {duration:.3f}ç§’, ä¼°ç®—å­—æ•°: {word_count}")
            return word_count
            
        except Exception as e:
            logger.warning(f"å­—æ•°è®¡ç®—é”™è¯¯: {traceback.format_exc()}")
            return 100  # å‘ç”Ÿé”™è¯¯æ—¶è¿”å›é»˜è®¤å­—æ•°

    def process_frames(self, frame_content_list: List[Dict]) -> List[Dict]:
        for frame_content in frame_content_list:
            word_count = self.calculate_duration_and_word_count(frame_content["timestamp"])
            script = self.generator.generate_script(frame_content["picture"], word_count)
            frame_content["narration"] = script
            frame_content["OST"] = 2
            logger.info(f"æ—¶é—´èŒƒå›´: {frame_content['timestamp']}, å»ºè®®å­—æ•°: {word_count}")
            logger.info(script)

        self._save_results(frame_content_list)
        return frame_content_list

    def _save_results(self, frame_content_list: List[Dict]):
        """ä¿å­˜å¤„ç†ç»“æœï¼Œå¹¶æ·»åŠ æ–°çš„æ—¶é—´æˆ³"""
        try:
            def format_timestamp(seconds: float) -> str:
                """å°†ç§’æ•°è½¬æ¢ä¸º HH:MM:SS,mmm æ ¼å¼"""
                hours = int(seconds // 3600)
                minutes = int((seconds % 3600) // 60)
                seconds_remainder = seconds % 60
                whole_seconds = int(seconds_remainder)
                milliseconds = int((seconds_remainder - whole_seconds) * 1000)
                
                return f"{hours:02d}:{minutes:02d}:{whole_seconds:02d},{milliseconds:03d}"

            # è®¡ç®—æ–°çš„æ—¶é—´æˆ³
            current_time = 0.0  # å½“å‰æ—¶é—´ç‚¹ï¼ˆç§’ï¼ŒåŒ…å«æ¯«ç§’ï¼‰

            for frame in frame_content_list:
                # è·å–åŸå§‹æ—¶é—´æˆ³çš„æŒç»­æ—¶é—´
                start_str, end_str = frame['timestamp'].split('-')

                def time_to_seconds(time_str: str) -> float:
                    """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°ï¼ˆåŒ…å«æ¯«ç§’ï¼‰"""
                    try:
                        if ',' in time_str:
                            time_part, ms_part = time_str.split(',')
                            ms = float(ms_part) / 1000
                        else:
                            time_part = time_str
                            ms = 0

                        parts = time_part.split(':')
                        if len(parts) == 3:  # HH:MM:SS
                            h, m, s = map(float, parts)
                            seconds = h * 3600 + m * 60 + s
                        elif len(parts) == 2:  # MM:SS
                            m, s = map(float, parts)
                            seconds = m * 60 + s
                        else:  # SS
                            seconds = float(parts[0])

                        return seconds + ms
                    except Exception as e:
                        logger.error(f"æ—¶é—´æ ¼å¼è½¬æ¢é”™è¯¯ {time_str}: {str(e)}")
                        return 0.0

                # è®¡ç®—å½“å‰ç‰‡æ®µçš„æŒç»­æ—¶é—´
                start_seconds = time_to_seconds(start_str)
                end_seconds = time_to_seconds(end_str)
                duration = end_seconds - start_seconds

                # è®¾ç½®æ–°çš„æ—¶é—´æˆ³
                new_start = format_timestamp(current_time)
                new_end = format_timestamp(current_time + duration)
                frame['new_timestamp'] = f"{new_start}-{new_end}"

                # æ›´æ–°å½“å‰æ—¶é—´ç‚¹
                current_time += duration

            # ä¿å­˜ç»“æœ
            file_name = f"storage/json/step2_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            os.makedirs(os.path.dirname(file_name), exist_ok=True)

            with open(file_name, 'w', encoding='utf-8') as file:
                json.dump(frame_content_list, file, ensure_ascii=False, indent=4)

            logger.info(f"ä¿å­˜è„šæœ¬æˆåŠŸï¼Œæ€»æ—¶é•¿: {format_timestamp(current_time)}")

        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\n{traceback.format_exc()}")
            raise
</file>

<file path="app/utils/utils.py">
import locale
import os
import traceback

import requests
import threading
from typing import Any
from loguru import logger
import streamlit as st
import json
from uuid import uuid4
import urllib3
from datetime import datetime, timedelta

from app.models import const
from app.utils import check_script
from app.services import material

urllib3.disable_warnings()


def get_response(status: int, data: Any = None, message: str = ""):
    obj = {
        "status": status,
    }
    if data:
        obj["data"] = data
    if message:
        obj["message"] = message
    return obj


def to_json(obj):
    try:
        # å®šä¹‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°æ¥å¤„ç†ä¸åŒç±»å‹çš„å¯¹è±¡
        def serialize(o):
            # å¦‚æœå¯¹è±¡æ˜¯å¯åºåˆ—åŒ–ç±»å‹ï¼Œç›´æ¥è¿”å›
            if isinstance(o, (int, float, bool, str)) or o is None:
                return o
            # å¦‚æœå¯¹è±¡æ˜¯äºŒè¿›åˆ¶æ•°æ®ï¼Œè½¬æ¢ä¸ºbase64ç¼–ç çš„å­—ç¬¦ä¸²
            elif isinstance(o, bytes):
                return "*** binary data ***"
            # å¦‚æœè±¡æ˜¯å­—å…¸ï¼Œé€’å½’å¤„ç†æ¯ä¸ªé”®å€¼å¯¹
            elif isinstance(o, dict):
                return {k: serialize(v) for k, v in o.items()}
            # å¦‚æœå¯¹è±¡æ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼Œé€’å½’å¤„ç†æ¯ä¸ªå…ƒç´ 
            elif isinstance(o, (list, tuple)):
                return [serialize(item) for item in o]
            # å¦‚æœå¯¹è±¡æ˜¯è‡ªå®šä¹‰ç±»å‹ï¼Œå°è¯•è¿”å›å…¶__dict__å±æ€§
            elif hasattr(o, "__dict__"):
                return serialize(o.__dict__)
            # å…¶ä»–æƒ…å†µè¿”å›Noneï¼ˆæˆ–è€…å¯ä»¥é€‰æ‹©æŠ›å‡ºå¼‚å¸¸ï¼‰
            else:
                return None

        # ä½¿ç”¨serializeå‡½æ•°å¤„ç†è¾“å…¥å¯¹è±¡
        serialized_obj = serialize(obj)

        # åºåˆ—åŒ–å¤„ç†åçš„å¯¹è±¡ä¸ºJSONç¬¦ä¸²
        return json.dumps(serialized_obj, ensure_ascii=False, indent=4)
    except Exception as e:
        return None


def get_uuid(remove_hyphen: bool = False):
    u = str(uuid4())
    if remove_hyphen:
        u = u.replace("-", "")
    return u


def root_dir():
    return os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))


def storage_dir(sub_dir: str = "", create: bool = False):
    d = os.path.join(root_dir(), "storage")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if create and not os.path.exists(d):
        os.makedirs(d)

    return d


def resource_dir(sub_dir: str = ""):
    d = os.path.join(root_dir(), "resource")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    return d


def task_dir(sub_dir: str = ""):
    d = os.path.join(storage_dir(), "tasks")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if not os.path.exists(d):
        os.makedirs(d)
    return d


def font_dir(sub_dir: str = ""):
    d = resource_dir("fonts")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if not os.path.exists(d):
        os.makedirs(d)
    return d


def song_dir(sub_dir: str = ""):
    d = resource_dir("songs")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if not os.path.exists(d):
        os.makedirs(d)
    return d


def get_bgm_file(bgm_type: str = "random", bgm_file: str = ""):
    """
    è·å–èƒŒæ™¯éŸ³ä¹æ–‡ä»¶è·¯å¾„
    Args:
        bgm_type: èƒŒæ™¯éŸ³ä¹ç±»å‹ï¼Œå¯é€‰å€¼: random(éšæœº), ""(æ— èƒŒæ™¯éŸ³ä¹)
        bgm_file: æŒ‡å®šçš„èƒŒæ™¯éŸ³ä¹æ–‡ä»¶è·¯å¾„

    Returns:
        str: èƒŒæ™¯éŸ³ä¹æ–‡ä»¶è·¯å¾„
    """
    import glob
    import random
    if not bgm_type:
        return ""

    if bgm_file and os.path.exists(bgm_file):
        return bgm_file

    if bgm_type == "random":
        song_dir_path = song_dir()

        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(song_dir_path):
            logger.warning(f"èƒŒæ™¯éŸ³ä¹ç›®å½•ä¸å­˜åœ¨: {song_dir_path}")
            return ""

        # æ”¯æŒ mp3 å’Œ flac æ ¼å¼
        mp3_files = glob.glob(os.path.join(song_dir_path, "*.mp3"))
        flac_files = glob.glob(os.path.join(song_dir_path, "*.flac"))
        files = mp3_files + flac_files

        # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°éŸ³ä¹æ–‡ä»¶
        if not files:
            logger.warning(f"åœ¨ç›®å½• {song_dir_path} ä¸­æ²¡æœ‰æ‰¾åˆ° MP3 æˆ– FLAC æ–‡ä»¶")
            return ""

        return random.choice(files)

    return ""


def public_dir(sub_dir: str = ""):
    d = resource_dir(f"public")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if not os.path.exists(d):
        os.makedirs(d)
    return d


def srt_dir(sub_dir: str = ""):
    d = resource_dir(f"srt")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if not os.path.exists(d):
        os.makedirs(d)
    return d


def run_in_background(func, *args, **kwargs):
    def run():
        try:
            func(*args, **kwargs)
        except Exception as e:
            logger.error(f"run_in_background error: {e}")

    thread = threading.Thread(target=run)
    thread.start()
    return thread


def time_convert_seconds_to_hmsm(seconds) -> str:
    hours = int(seconds // 3600)
    seconds = seconds % 3600
    minutes = int(seconds // 60)
    milliseconds = int(seconds * 1000) % 1000
    seconds = int(seconds % 60)
    return "{:02d}:{:02d}:{:02d},{:03d}".format(hours, minutes, seconds, milliseconds)


def text_to_srt(idx: int, msg: str, start_time: float, end_time: float) -> str:
    start_time = time_convert_seconds_to_hmsm(start_time)
    end_time = time_convert_seconds_to_hmsm(end_time)
    srt = """%d
%s --> %s
%s
        """ % (
        idx,
        start_time,
        end_time,
        msg,
    )
    return srt


def str_contains_punctuation(word):
    for p in const.PUNCTUATIONS:
        if p in word:
            return True
    return False


def split_string_by_punctuations(s):
    result = []
    txt = ""

    previous_char = ""
    next_char = ""
    for i in range(len(s)):
        char = s[i]
        if char == "\n":
            result.append(txt.strip())
            txt = ""
            continue

        if i > 0:
            previous_char = s[i - 1]
        if i < len(s) - 1:
            next_char = s[i + 1]

        if char == "." and previous_char.isdigit() and next_char.isdigit():
            # å–ç°1ä¸‡ï¼ŒæŒ‰2.5%æ”¶å–æ‰‹ç»­è´¹, 2.5 ä¸­çš„ . ä¸èƒ½ä½œä¸ºæ¢è¡Œæ ‡è®°
            txt += char
            continue

        if char not in const.PUNCTUATIONS:
            txt += char
        else:
            result.append(txt.strip())
            txt = ""
    result.append(txt.strip())
    # filter empty string
    result = list(filter(None, result))
    return result


def md5(text):
    import hashlib

    return hashlib.md5(text.encode("utf-8")).hexdigest()


def get_system_locale():
    try:
        loc = locale.getdefaultlocale()
        # zh_CN, zh_TW return zh
        # en_US, en_GB return en
        language_code = loc[0].split("_")[0]
        return language_code
    except Exception as e:
        return "en"


def load_locales(i18n_dir):
    _locales = {}
    for root, dirs, files in os.walk(i18n_dir):
        for file in files:
            if file.endswith(".json"):
                lang = file.split(".")[0]
                with open(os.path.join(root, file), "r", encoding="utf-8") as f:
                    _locales[lang] = json.loads(f.read())
    return _locales


def parse_extension(filename):
    return os.path.splitext(filename)[1].strip().lower().replace(".", "")


def script_dir(sub_dir: str = ""):
    d = resource_dir(f"scripts")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if not os.path.exists(d):
        os.makedirs(d)
    return d


def video_dir(sub_dir: str = ""):
    d = resource_dir(f"videos")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if not os.path.exists(d):
        os.makedirs(d)
    return d


def split_timestamp(timestamp):
    """
    æ‹†åˆ†æ—¶é—´æˆ³
    """
    start, end = timestamp.split('-')
    start_hour, start_minute = map(int, start.split(':'))
    end_hour, end_minute = map(int, end.split(':'))

    start_time = '00:{:02d}:{:02d}'.format(start_hour, start_minute)
    end_time = '00:{:02d}:{:02d}'.format(end_hour, end_minute)

    return start_time, end_time


def reduce_video_time(txt: str, duration: float = 0.21531):
    """
    æŒ‰ç…§å­—æ•°ç¼©å‡è§†é¢‘æ—¶é•¿ï¼Œä¸€ä¸ªå­—è€—æ—¶çº¦ 0.21531 s,
    Returns:
    """
    # è¿”å›ç»“æœå››èˆäº”å…¥ä¸ºæ•´æ•°
    duration = len(txt) * duration
    return int(duration)


def get_current_country():
    """
    åˆ¤æ–­å½“å‰ç½‘ç»œIPåœ°å€æ‰€åœ¨çš„å›½å®¶
    """
    try:
        # ä½¿ç”¨ipapi.coçš„å…è´¹APIè·å–IPåœ°å€ä¿¡æ¯
        response = requests.get('https://ipapi.co/json/')
        data = response.json()

        # è·å–å›½å®¶åç§°
        country = data.get('country_name')

        if country:
            logger.debug(f"å½“å‰ç½‘ç»œIPåœ°å€ä½äºï¼š{country}")
            return country
        else:
            logger.debug("æ— æ³•ç¡®å®šå½“å‰ç½‘ç»œIPåœ°å€æ‰€åœ¨çš„å›½å®¶")
            return None

    except requests.RequestException:
        logger.error("è·å–IPåœ°å€ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
        return None


def time_to_seconds(time_str: str) -> float:
    """
    å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
    - "HH:MM:SS,mmm" -> å°æ—¶:åˆ†é’Ÿ:ç§’,æ¯«ç§’
    - "MM:SS,mmm" -> åˆ†é’Ÿ:ç§’,æ¯«ç§’
    - "SS,mmm" -> ç§’,æ¯«ç§’
    - "SS-mmm" -> ç§’-æ¯«ç§’
    
    Args:
        time_str: æ—¶é—´å­—ç¬¦ä¸²
        
    Returns:
        float: è½¬æ¢åçš„ç§’æ•°(åŒ…å«æ¯«ç§’)
    """
    try:
        # å¤„ç†å¸¦æœ‰'-'çš„æ¯«ç§’æ ¼å¼
        if '-' in time_str:
            time_part, ms_part = time_str.split('-')
            ms = float(ms_part) / 1000
        # å¤„ç†å¸¦æœ‰','çš„æ¯«ç§’æ ¼å¼
        elif ',' in time_str:
            time_part, ms_part = time_str.split(',')
            ms = float(ms_part) / 1000
        else:
            time_part = time_str
            ms = 0

        # åˆ†å‰²æ—¶é—´éƒ¨åˆ†
        parts = time_part.split(':')

        if len(parts) == 3:  # HH:MM:SS
            h, m, s = map(float, parts)
            seconds = h * 3600 + m * 60 + s
        elif len(parts) == 2:  # MM:SS
            m, s = map(float, parts)
            seconds = m * 60 + s
        else:  # SS
            seconds = float(parts[0])

        return seconds + ms

    except (ValueError, IndexError) as e:
        logger.error(f"æ—¶é—´æ ¼å¼è½¬æ¢é”™è¯¯ {time_str}: {str(e)}")
        return 0.0


def seconds_to_time(seconds: float) -> str:
    h, remainder = divmod(seconds, 3600)
    m, s = divmod(remainder, 60)
    return f"{int(h):02d}:{int(m):02d}:{s:06.3f}"


def calculate_total_duration(scenes):
    """
    è®¡ç®—åœºæ™¯åˆ—è¡¨çš„æ€»æ—¶é•¿
    
    Args:
        scenes: åœºæ™¯åˆ—è¡¨ï¼Œæ¯ä¸ªåœºæ™¯åŒ…å« timestamp å­—æ®µï¼Œæ ¼å¼å¦‚ "00:00:28,350-00:00:41,000"
        
    Returns:
        float: æ€»æ—¶é•¿ï¼ˆç§’ï¼‰
    """
    total_seconds = 0

    for scene in scenes:
        start, end = scene['timestamp'].split('-')
        # ä½¿ç”¨ time_to_seconds å‡½æ•°å¤„ç†æ›´ç²¾ç¡®çš„æ—¶é—´æ ¼å¼
        start_seconds = time_to_seconds(start)
        end_seconds = time_to_seconds(end)

        duration = end_seconds - start_seconds
        total_seconds += duration

    return total_seconds


def add_new_timestamps(scenes):
    """
    æ–°å¢æ–°è§†é¢‘çš„æ—¶é—´æˆ³ï¼Œå¹¶ä¸º"åŸç”Ÿæ’­æ”¾"çš„narrationæ·»åŠ å”¯ä¸€æ ‡è¯†ç¬¦
    Args:
        scenes: åœºæ™¯åˆ—è¡¨

    Returns:
        æ›´æ–°åçš„åœºæ™¯åˆ—è¡¨
    """
    current_time = timedelta()
    updated_scenes = []

    # ä¿å­˜è„šæœ¬å‰å…ˆæ£€æŸ¥è„šæœ¬æ˜¯å¦æ­£ç¡®
    check_script.check_script(scenes, calculate_total_duration(scenes))

    for scene in scenes:
        new_scene = scene.copy()  # åˆ›å»ºåœºæ™¯çš„å‰¯æœ¬ï¼Œä»¥ä¿ç•™åŸå§‹æ•°æ®
        start, end = new_scene['timestamp'].split('-')
        start_time = datetime.strptime(start, '%M:%S')
        end_time = datetime.strptime(end, '%M:%S')
        duration = end_time - start_time

        new_start = current_time
        current_time += duration
        new_end = current_time

        # å°† timedelta è½¬æ¢ä¸ºåˆ†é’Ÿå’Œç§’
        new_start_str = f"{int(new_start.total_seconds() // 60):02d}:{int(new_start.total_seconds() % 60):02d}"
        new_end_str = f"{int(new_end.total_seconds() // 60):02d}:{int(new_end.total_seconds() % 60):02d}"

        new_scene['new_timestamp'] = f"{new_start_str}-{new_end_str}"

        # ä¸º"åŸç”Ÿæ’­æ”¾"çš„narrationæ·»åŠ å”¯ä¸€æ ‡è¯†ç¬¦
        if new_scene.get('narration') == "" or new_scene.get('narration') == None:
            unique_id = str(uuid4())[:8]  # ä½¿ç”¨UUIDçš„å‰8ä¸ªå­—ç¬¦ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦
            new_scene['narration'] = f"åŸå£°æ’­æ”¾_{unique_id}"

        updated_scenes.append(new_scene)

    return updated_scenes


def clean_model_output(output):
    # ç§»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
    output = output.strip('```json').strip('```')
    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½å­—ç¬¦
    output = output.strip()
    return output


def cut_video(params, progress_callback=None):
    try:
        task_id = str(uuid4())
        st.session_state['task_id'] = task_id

        if not st.session_state.get('video_clip_json'):
            raise ValueError("è§†é¢‘è„šæœ¬ä¸èƒ½ä¸ºç©º")

        video_script_list = st.session_state['video_clip_json']
        time_list = [i['timestamp'] for i in video_script_list]

        def clip_progress(current, total):
            progress = int((current / total) * 100)
            if progress_callback:
                progress_callback(progress)

        subclip_videos = material.clip_videos(
            task_id=task_id,
            timestamp_terms=time_list,
            origin_video=params.video_origin_path,
            progress_callback=clip_progress
        )

        if subclip_videos is None:
            raise ValueError("è£å‰ªè§†é¢‘å¤±è´¥")

        st.session_state['subclip_videos'] = subclip_videos
        for i, video_script in enumerate(video_script_list):
            try:
                video_script['path'] = subclip_videos[video_script['timestamp']]
            except KeyError as err:
                logger.error(f"è£å‰ªè§†é¢‘å¤±è´¥: {err}")

        return task_id, subclip_videos

    except Exception as e:
        logger.error(f"è§†é¢‘è£å‰ªè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: \n{traceback.format_exc()}")
        raise


def temp_dir(sub_dir: str = ""):
    """
    è·å–ä¸´æ—¶æ–‡ä»¶ç›®å½•
    Args:
        sub_dir: å­ç›®å½•å
    Returns:
        str: ä¸´æ—¶æ–‡ä»¶ç›®å½•è·¯å¾„
    """
    d = os.path.join(storage_dir(), "temp")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if not os.path.exists(d):
        os.makedirs(d)
    return d


def clear_keyframes_cache(video_path: str = None):
    """
    æ¸…ç†å…³é”®å¸§ç¼“å­˜
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæŒ‡å®šåˆ™åªæ¸…ç†è¯¥è§†é¢‘çš„ç¼“å­˜
    """
    try:
        keyframes_dir = os.path.join(temp_dir(), "keyframes")
        if not os.path.exists(keyframes_dir):
            return

        if video_path:
            # ç†æŒ‡å®šè§†é¢‘çš„ç¼“å­˜
            video_hash = md5(video_path + str(os.path.getmtime(video_path)))
            video_keyframes_dir = os.path.join(keyframes_dir, video_hash)
            if os.path.exists(video_keyframes_dir):
                import shutil
                shutil.rmtree(video_keyframes_dir)
                logger.info(f"å·²æ¸…ç†è§†é¢‘å…³é”®å¸§ç¼“å­˜: {video_path}")
        else:
            # æ¸…ç†æ‰€æœ‰ç¼“å­˜
            import shutil
            shutil.rmtree(keyframes_dir)
            logger.info("å·²æ¸…ç†æ‰€æœ‰å…³é”®å¸§ç¼“å­˜")

    except Exception as e:
        logger.error(f"æ¸…ç†å…³é”®å¸§ç¼“å­˜å¤±è´¥: {e}")


def init_resources():
    """åˆå§‹åŒ–èµ„æºæ–‡ä»¶"""
    try:
        # åˆ›å»ºå­—ä½“ç›®å½•
        font_dir = os.path.join(root_dir(), "resource", "fonts")
        os.makedirs(font_dir, exist_ok=True)

        # æ£€æŸ¥å­—ä½“æ–‡ä»¶
        font_files = [
            ("SourceHanSansCN-Regular.otf",
             "https://github.com/adobe-fonts/source-han-sans/raw/release/OTF/SimplifiedChinese/SourceHanSansSC-Regular.otf"),
            ("simhei.ttf", "C:/Windows/Fonts/simhei.ttf"),  # Windows é»‘ä½“
            ("simkai.ttf", "C:/Windows/Fonts/simkai.ttf"),  # Windows æ¥·ä½“
            ("simsun.ttc", "C:/Windows/Fonts/simsun.ttc"),  # Windows å®‹ä½“
        ]

        # ä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿå­—ä½“
        system_font_found = False
        for font_name, source in font_files:
            if not source.startswith("http") and os.path.exists(source):
                target_path = os.path.join(font_dir, font_name)
                if not os.path.exists(target_path):
                    import shutil
                    shutil.copy2(source, target_path)
                    logger.info(f"å·²å¤åˆ¶ç³»ç»Ÿå­—ä½“: {font_name}")
                system_font_found = True
                break

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç³»ç»Ÿå­—ä½“ï¼Œåˆ™ä¸‹è½½æ€æºé»‘ä½“
        if not system_font_found:
            source_han_path = os.path.join(font_dir, "SourceHanSansCN-Regular.otf")
            if not os.path.exists(source_han_path):
                download_font(font_files[0][1], source_han_path)

    except Exception as e:
        logger.error(f"åˆå§‹åŒ–èµ„æºæ–‡ä»¶å¤±è´¥: {e}")


def download_font(url: str, font_path: str):
    """ä¸‹è½½å­—ä½“æ–‡ä»¶"""
    try:
        logger.info(f"æ­£åœ¨ä¸‹è½½å­—ä½“æ–‡ä»¶: {url}")
        import requests
        response = requests.get(url)
        response.raise_for_status()

        with open(font_path, 'wb') as f:
            f.write(response.content)

        logger.info(f"å­—ä½“æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {font_path}")

    except Exception as e:
        logger.error(f"ä¸‹è½½å­—ä½“æ–‡ä»¶å¤±è´¥: {e}")
        raise


def init_imagemagick():
    """åˆå§‹åŒ– ImageMagick é…ç½®"""
    try:
        # æ£€æŸ¥ ImageMagick æ˜¯å¦å·²å®‰è£…
        import subprocess
        result = subprocess.run(['magick', '-version'], capture_output=True, text=True)
        if result.returncode != 0:
            logger.error("ImageMagick æœªå®‰è£…æˆ–é…ç½®ä¸æ­£ç¡®")
            return False

        # è®¾ç½® IMAGEMAGICK_BINARY ç¯å¢ƒå˜é‡
        os.environ['IMAGEMAGICK_BINARY'] = 'magick'

        return True
    except Exception as e:
        logger.error(f"åˆå§‹åŒ– ImageMagick å¤±è´¥: {str(e)}")
        return False
</file>

<file path="app/utils/video_processor_v2.py">
import cv2
import numpy as np
from sklearn.cluster import KMeans
import os
import re
from typing import List, Tuple, Generator
from loguru import logger
import subprocess
from tqdm import tqdm


class VideoProcessor:
    def __init__(self, video_path: str):
        """
        åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        """
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

        self.video_path = video_path
        self.cap = cv2.VideoCapture(video_path)

        if not self.cap.isOpened():
            raise RuntimeError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿è§†é¢‘èµ„æºè¢«é‡Šæ”¾"""
        if hasattr(self, 'cap'):
            self.cap.release()

    def preprocess_video(self) -> Generator[np.ndarray, None, None]:
        """
        ä½¿ç”¨ç”Ÿæˆå™¨æ–¹å¼è¯»å–è§†é¢‘å¸§

        Yields:
            np.ndarray: è§†é¢‘å¸§
        """
        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # é‡ç½®åˆ°è§†é¢‘å¼€å§‹
        while self.cap.isOpened():
            ret, frame = self.cap.read()
            if not ret:
                break
            yield frame

    def detect_shot_boundaries(self, frames: List[np.ndarray], threshold: int = 30) -> List[int]:
        """
        ä½¿ç”¨å¸§å·®æ³•æ£€æµ‹é•œå¤´è¾¹ç•Œ
        
        Args:
            frames: è§†é¢‘å¸§åˆ—è¡¨
            threshold: å·®å¼‚é˜ˆå€¼ï¼Œé»˜è®¤å€¼è°ƒä½ä¸º30
        
        Returns:
            List[int]: é•œå¤´è¾¹ç•Œå¸§çš„ç´¢å¼•åˆ—è¡¨
        """
        shot_boundaries = []
        if len(frames) < 2:  # æ·»åŠ å¸§æ•°æ£€æŸ¥
            logger.warning("è§†é¢‘å¸§æ•°è¿‡å°‘ï¼Œæ— æ³•æ£€æµ‹åœºæ™¯è¾¹ç•Œ")
            return [len(frames) - 1]  # è¿”å›æœ€åä¸€å¸§ä½œä¸ºè¾¹ç•Œ
        
        for i in range(1, len(frames)):
            prev_frame = cv2.cvtColor(frames[i - 1], cv2.COLOR_BGR2GRAY)
            curr_frame = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
            
            # è®¡ç®—å¸§å·®
            diff = np.mean(np.abs(curr_frame.astype(float) - prev_frame.astype(float)))
            
            if diff > threshold:
                shot_boundaries.append(i)

        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•è¾¹ç•Œï¼Œè‡³å°‘è¿”å›æœ€åä¸€å¸§
        if not shot_boundaries:
            logger.warning("æœªæ£€æµ‹åˆ°åœºæ™¯è¾¹ç•Œï¼Œå°†è§†é¢‘ä½œä¸ºå•ä¸ªåœºæ™¯å¤„ç†")
            shot_boundaries.append(len(frames) - 1)
        
        return shot_boundaries

    def extract_keyframes(self, frames: List[np.ndarray], shot_boundaries: List[int]) -> Tuple[
        List[np.ndarray], List[int]]:
        """
        ä»æ¯ä¸ªé•œå¤´ä¸­æå–å…³é”®å¸§

        Args:
            frames: è§†é¢‘å¸§åˆ—è¡¨
            shot_boundaries: é•œå¤´è¾¹ç•Œåˆ—è¡¨

        Returns:
            Tuple[List[np.ndarray], List[int]]: å…³é”®å¸§åˆ—è¡¨å’Œå¯¹åº”çš„å¸§ç´¢å¼•
        """
        keyframes = []
        keyframe_indices = []

        for i in tqdm(range(len(shot_boundaries)), desc="æå–å…³é”®å¸§"):
            start = shot_boundaries[i - 1] if i > 0 else 0
            end = shot_boundaries[i]
            shot_frames = frames[start:end]

            if not shot_frames:
                continue

            # å°†æ¯ä¸€å¸§è½¬æ¢ä¸ºç°åº¦å›¾å¹¶å±•å¹³ä¸ºä¸€ç»´æ•°ç»„
            frame_features = np.array([cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).flatten()
                                       for frame in shot_frames])
            
            try:
                # å°è¯•ä½¿ç”¨ KMeans
                kmeans = KMeans(n_clusters=1, random_state=0).fit(frame_features)
                center_idx = np.argmin(np.sum((frame_features - kmeans.cluster_centers_[0]) ** 2, axis=1))
            except Exception as e:
                logger.warning(f"KMeans èšç±»å¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ: {str(e)}")
                # å¤‡é€‰æ–¹æ¡ˆï¼šé€‰æ‹©é•œå¤´ä¸­é—´çš„å¸§ä½œä¸ºå…³é”®å¸§
                center_idx = len(shot_frames) // 2

            keyframes.append(shot_frames[center_idx])
            keyframe_indices.append(start + center_idx)

        return keyframes, keyframe_indices

    def save_keyframes(self, keyframes: List[np.ndarray], keyframe_indices: List[int],
                       output_dir: str, desc: str = "ä¿å­˜å…³é”®å¸§") -> None:
        """
        ä¿å­˜å…³é”®å¸§åˆ°æŒ‡å®šç›®å½•ï¼Œæ–‡ä»¶åæ ¼å¼ä¸ºï¼škeyframe_å¸§åºå·_æ—¶é—´æˆ³.jpg
        æ—¶é—´æˆ³ç²¾ç¡®åˆ°æ¯«ç§’ï¼Œæ ¼å¼ä¸ºï¼šHHMMSSmmm
        """
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        for keyframe, frame_idx in tqdm(zip(keyframes, keyframe_indices),
                                        total=len(keyframes),
                                        desc=desc):
            # è®¡ç®—ç²¾ç¡®åˆ°æ¯«ç§’çš„æ—¶é—´æˆ³
            timestamp = frame_idx / self.fps
            hours = int(timestamp // 3600)
            minutes = int((timestamp % 3600) // 60)
            seconds = int(timestamp % 60)
            milliseconds = int((timestamp % 1) * 1000)  # è®¡ç®—æ¯«ç§’éƒ¨åˆ†
            time_str = f"{hours:02d}{minutes:02d}{seconds:02d}{milliseconds:03d}"

            output_path = os.path.join(output_dir,
                                       f'keyframe_{frame_idx:06d}_{time_str}.jpg')
            cv2.imwrite(output_path, keyframe)

    def extract_frames_by_numbers(self, frame_numbers: List[int], output_folder: str) -> None:
        """
        æ ¹æ®æŒ‡å®šçš„å¸§å·æå–å¸§ï¼Œå¦‚æœå¤šä¸ªå¸§åœ¨åŒä¸€æ¯«ç§’å†…ï¼Œåªä¿ç•™ä¸€ä¸ª
        """
        if not frame_numbers:
            raise ValueError("æœªæä¾›å¸§å·åˆ—è¡¨")

        if any(fn >= self.total_frames or fn < 0 for fn in frame_numbers):
            raise ValueError("å­˜åœ¨æ— æ•ˆçš„å¸§å·")

        if not os.path.exists(output_folder):
            os.makedirs(output_folder)

        # ç”¨äºè®°å½•å·²å¤„ç†çš„æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        processed_timestamps = set()

        for frame_number in tqdm(frame_numbers, desc="æå–é«˜æ¸…å¸§"):
            # è®¡ç®—ç²¾ç¡®åˆ°æ¯«ç§’çš„æ—¶é—´æˆ³
            timestamp = frame_number / self.fps
            timestamp_ms = int(timestamp * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

            # å¦‚æœè¿™ä¸€æ¯«ç§’å·²ç»å¤„ç†è¿‡ï¼Œè·³è¿‡
            if timestamp_ms in processed_timestamps:
                continue

            self.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
            ret, frame = self.cap.read()

            if ret:
                # è®°å½•è¿™ä¸€æ¯«ç§’å·²ç»å¤„ç†
                processed_timestamps.add(timestamp_ms)

                # è®¡ç®—æ—¶é—´æˆ³å­—ç¬¦ä¸²
                hours = int(timestamp // 3600)
                minutes = int((timestamp % 3600) // 60)
                seconds = int(timestamp % 60)
                milliseconds = int((timestamp % 1) * 1000)  # è®¡ç®—æ¯«ç§’éƒ¨åˆ†
                time_str = f"{hours:02d}{minutes:02d}{seconds:02d}{milliseconds:03d}"

                output_path = os.path.join(output_folder,
                                           f"keyframe_{frame_number:06d}_{time_str}.jpg")
                cv2.imwrite(output_path, frame)
            else:
                logger.info(f"æ— æ³•è¯»å–å¸§ {frame_number}")

        logger.info(f"å…±æå–äº† {len(processed_timestamps)} ä¸ªä¸åŒæ—¶é—´æˆ³çš„å¸§")

    @staticmethod
    def extract_numbers_from_folder(folder_path: str) -> List[int]:
        """
        ä»æ–‡ä»¶å¤¹ä¸­æå–å¸§å·
        
        Args:
            folder_path: å…³é”®å¸§æ–‡ä»¶å¤¹è·¯å¾„
        
        Returns:
            List[int]: æ’åºåçš„å¸§å·åˆ—è¡¨
        """
        files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]
        # æ›´æ–°æ­£åˆ™è¡¨è¾¾å¼ä»¥åŒ¹é…æ–°çš„æ–‡ä»¶åæ ¼å¼ï¼škeyframe_000123_010534123.jpg
        pattern = re.compile(r'keyframe_(\d+)_\d{9}\.jpg$')
        numbers = []
        
        for f in files:
            match = pattern.search(f)
            if match:
                numbers.append(int(match.group(1)))
            else:
                logger.warning(f"æ–‡ä»¶åæ ¼å¼ä¸åŒ¹é…: {f}")
        
        if not numbers:
            logger.error(f"åœ¨ç›®å½• {folder_path} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„å…³é”®å¸§æ–‡ä»¶")
        
        return sorted(numbers)

    def process_video(self, output_dir: str, skip_seconds: float = 0, threshold: int = 30) -> None:
        """
        å¤„ç†è§†é¢‘å¹¶æå–å…³é”®å¸§

        Args:
            output_dir: è¾“å‡ºç›®å½•
            skip_seconds: è·³è¿‡è§†é¢‘å¼€å¤´çš„ç§’æ•°
        """
        skip_frames = int(skip_seconds * self.fps)

        logger.info("è¯»å–è§†é¢‘å¸§...")
        frames = []
        for frame in tqdm(self.preprocess_video(),
                          total=self.total_frames,
                          desc="è¯»å–è§†é¢‘"):
            frames.append(frame)

        frames = frames[skip_frames:]

        if not frames:
            raise ValueError(f"è·³è¿‡ {skip_seconds} ç§’åæ²¡æœ‰å‰©ä½™å¸§å¯ä»¥å¤„ç†")

        logger.info("æ£€æµ‹åœºæ™¯è¾¹ç•Œ...")
        shot_boundaries = self.detect_shot_boundaries(frames, threshold)
        logger.info(f"æ£€æµ‹åˆ° {len(shot_boundaries)} ä¸ªåœºæ™¯è¾¹ç•Œ")

        keyframes, keyframe_indices = self.extract_keyframes(frames, shot_boundaries)

        adjusted_indices = [idx + skip_frames for idx in keyframe_indices]
        self.save_keyframes(keyframes, adjusted_indices, output_dir, desc="ä¿å­˜å‹ç¼©å…³é”®å¸§")

    def process_video_pipeline(self,
                               output_dir: str,
                               skip_seconds: float = 0,
                               threshold: int = 20,  # é™ä½é»˜è®¤é˜ˆå€¼
                               compressed_width: int = 320,
                               keep_temp: bool = False) -> None:
        """
        æ‰§è¡Œå®Œæ•´çš„è§†é¢‘å¤„ç†æµç¨‹
        
        Args:
            threshold: é™ä½é»˜è®¤é˜ˆå€¼ä¸º20ï¼Œä½¿åœºæ™¯æ£€æµ‹æ›´æ•æ„Ÿ
        """
        os.makedirs(output_dir, exist_ok=True)
        temp_dir = os.path.join(output_dir, 'temp')
        compressed_dir = os.path.join(temp_dir, 'compressed')
        mini_frames_dir = os.path.join(temp_dir, 'mini_frames')
        hd_frames_dir = output_dir

        os.makedirs(temp_dir, exist_ok=True)
        os.makedirs(compressed_dir, exist_ok=True)
        os.makedirs(mini_frames_dir, exist_ok=True)
        os.makedirs(hd_frames_dir, exist_ok=True)

        mini_processor = None
        compressed_video = None

        try:
            # 1. å‹ç¼©è§†é¢‘
            video_name = os.path.splitext(os.path.basename(self.video_path))[0]
            compressed_video = os.path.join(compressed_dir, f"{video_name}_compressed.mp4")

            # è·å–åŸå§‹è§†é¢‘çš„å®½åº¦å’Œé«˜åº¦
            original_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            original_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            logger.info("æ­¥éª¤1: å‹ç¼©è§†é¢‘...")
            if original_width > original_height:
                # æ¨ªç‰ˆè§†é¢‘
                scale_filter = f'scale={compressed_width}:-1'
            else:
                # ç«–ç‰ˆè§†é¢‘
                scale_filter = f'scale=-1:{compressed_width}'
            
            ffmpeg_cmd = [
                'ffmpeg', '-i', self.video_path,
                '-vf', scale_filter,
                '-y',
                compressed_video
            ]
            
            try:
                subprocess.run(ffmpeg_cmd, check=True, capture_output=True, text=True)
            except subprocess.CalledProcessError as e:
                logger.error(f"FFmpeg é”™è¯¯è¾“å‡º: {e.stderr}")
                raise

            # 2. ä»å‹ç¼©è§†é¢‘ä¸­æå–å…³é”®å¸§
            logger.info("\næ­¥éª¤2: ä»å‹ç¼©è§†é¢‘æå–å…³é”®å¸§...")
            mini_processor = VideoProcessor(compressed_video)
            mini_processor.process_video(mini_frames_dir, skip_seconds, threshold)

            # 3. ä»åŸå§‹è§†é¢‘æå–é«˜æ¸…å…³é”®å¸§
            logger.info("\næ­¥éª¤3: æå–é«˜æ¸…å…³é”®å¸§...")
            frame_numbers = self.extract_numbers_from_folder(mini_frames_dir)

            if not frame_numbers:
                raise ValueError("æœªèƒ½ä»å‹ç¼©è§†é¢‘ä¸­æå–åˆ°æœ‰æ•ˆçš„å…³é”®å¸§")

            self.extract_frames_by_numbers(frame_numbers, hd_frames_dir)

            logger.info(f"å¤„ç†å®Œæˆï¼é«˜æ¸…å…³é”®å¸§ä¿å­˜åœ¨: {hd_frames_dir}")

        except Exception as e:
            import traceback
            logger.error(f"è§†é¢‘å¤„ç†å¤±è´¥: \n{traceback.format_exc()}")
            raise

        finally:
            # é‡Šæ”¾èµ„æº
            if mini_processor:
                mini_processor.cap.release()
                del mini_processor

            # ç¡®ä¿è§†é¢‘æ–‡ä»¶å¥æŸ„è¢«é‡Šæ”¾
            if hasattr(self, 'cap'):
                self.cap.release()

            # ç­‰å¾…èµ„æºé‡Šæ”¾
            import time
            time.sleep(0.5)

            if not keep_temp:
                try:
                    # å…ˆåˆ é™¤å‹ç¼©è§†é¢‘æ–‡ä»¶
                    if compressed_video and os.path.exists(compressed_video):
                        try:
                            os.remove(compressed_video)
                        except Exception as e:
                            logger.warning(f"åˆ é™¤å‹ç¼©è§†é¢‘å¤±è´¥: {e}")

                    # å†åˆ é™¤ä¸´æ—¶ç›®å½•
                    import shutil
                    if os.path.exists(temp_dir):
                        max_retries = 3
                        for i in range(max_retries):
                            try:
                                shutil.rmtree(temp_dir)
                                break
                            except Exception as e:
                                if i == max_retries - 1:
                                    logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
                                else:
                                    time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                                    continue

                    logger.info("ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
                except Exception as e:
                    logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")


if __name__ == "__main__":
    import time

    start_time = time.time()
    processor = VideoProcessor("E:\\projects\\NarratoAI\\resource\\videos\\test.mp4")
    processor.process_video_pipeline(output_dir="output")
    end_time = time.time()
    print(f"å¤„ç†å®Œæˆï¼æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
</file>

<file path="app/utils/video_processor.py">
import cv2
import numpy as np
from sklearn.cluster import MiniBatchKMeans
import os
import re
from typing import List, Tuple, Generator
from loguru import logger
import gc
from tqdm import tqdm


class VideoProcessor:
    def __init__(self, video_path: str, batch_size: int = 100):
        """
        åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œæ§åˆ¶å†…å­˜ä½¿ç”¨
        """
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        self.video_path = video_path
        self.batch_size = batch_size
        self.cap = cv2.VideoCapture(video_path)
        
        if not self.cap.isOpened():
            raise RuntimeError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿è§†é¢‘èµ„æºè¢«é‡Šæ”¾"""
        if hasattr(self, 'cap'):
            self.cap.release()
        gc.collect()

    def preprocess_video(self) -> Generator[Tuple[int, np.ndarray], None, None]:
        """
        ä½¿ç”¨ç”Ÿæˆå™¨æ–¹å¼åˆ†æ‰¹è¯»å–è§†é¢‘å¸§
        
        Yields:
            Tuple[int, np.ndarray]: (å¸§ç´¢å¼•, è§†é¢‘å¸§)
        """
        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
        frame_idx = 0
        
        while self.cap.isOpened():
            ret, frame = self.cap.read()
            if not ret:
                break
                
            # é™ä½åˆ†è¾¨ç‡ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
            frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)
            yield frame_idx, frame
            
            frame_idx += 1
            
            # å®šæœŸè¿›è¡Œåƒåœ¾å›æ”¶
            if frame_idx % 1000 == 0:
                gc.collect()

    def detect_shot_boundaries(self, threshold: int = 70) -> List[int]:
        """
        ä½¿ç”¨æ‰¹å¤„ç†æ–¹å¼æ£€æµ‹é•œå¤´è¾¹ç•Œ
        
        Args:
            threshold: å·®å¼‚é˜ˆå€¼
            
        Returns:
            List[int]: é•œå¤´è¾¹ç•Œå¸§çš„ç´¢å¼•åˆ—è¡¨
        """
        shot_boundaries = []
        prev_frame = None
        prev_idx = -1
        
        pbar = tqdm(self.preprocess_video(), 
                   total=self.total_frames,
                   desc="æ£€æµ‹é•œå¤´è¾¹ç•Œ",
                   unit="å¸§")
        
        for frame_idx, curr_frame in pbar:
            if prev_frame is not None:
                prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
                curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)
                
                diff = np.mean(np.abs(curr_gray.astype(float) - prev_gray.astype(float)))
                if diff > threshold:
                    shot_boundaries.append(frame_idx)
                    pbar.set_postfix({"æ£€æµ‹åˆ°è¾¹ç•Œ": len(shot_boundaries)})
            
            prev_frame = curr_frame.copy()
            prev_idx = frame_idx
            
            del curr_frame
            if frame_idx % 100 == 0:
                gc.collect()
        
        return shot_boundaries

    def process_shot(self, shot_frames: List[Tuple[int, np.ndarray]]) -> Tuple[np.ndarray, int]:
        """
        å¤„ç†å•ä¸ªé•œå¤´çš„å¸§
        
        Args:
            shot_frames: é•œå¤´ä¸­çš„å¸§åˆ—è¡¨
            
        Returns:
            Tuple[np.ndarray, int]: (å…³é”®å¸§, å¸§ç´¢å¼•)
        """
        if not shot_frames:
            return None, -1
            
        frame_features = []
        frame_indices = []
        
        for idx, frame in tqdm(shot_frames, 
                             desc="å¤„ç†é•œå¤´å¸§",
                             unit="å¸§",
                             leave=False):
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            resized_gray = cv2.resize(gray, (32, 32))
            frame_features.append(resized_gray.flatten())
            frame_indices.append(idx)
            
        frame_features = np.array(frame_features)
        
        kmeans = MiniBatchKMeans(n_clusters=1, batch_size=min(len(frame_features), 100),
                                random_state=0).fit(frame_features)
        
        center_idx = np.argmin(np.sum((frame_features - kmeans.cluster_centers_[0]) ** 2, axis=1))
        
        return shot_frames[center_idx][1], frame_indices[center_idx]

    def extract_keyframes(self, shot_boundaries: List[int]) -> Generator[Tuple[np.ndarray, int], None, None]:
        """
        ä½¿ç”¨ç”Ÿæˆå™¨æ–¹å¼æå–å…³é”®å¸§
        
        Args:
            shot_boundaries: é•œå¤´è¾¹ç•Œåˆ—è¡¨
            
        Yields:
            Tuple[np.ndarray, int]: (å…³é”®å¸§, å¸§ç´¢å¼•)
        """
        shot_frames = []
        current_shot_start = 0
        
        for frame_idx, frame in self.preprocess_video():
            if frame_idx in shot_boundaries:
                if shot_frames:
                    keyframe, keyframe_idx = self.process_shot(shot_frames)
                    if keyframe is not None:
                        yield keyframe, keyframe_idx
                    
                    # æ¸…ç†å†…å­˜
                    shot_frames.clear()
                    gc.collect()
                
                current_shot_start = frame_idx
            
            shot_frames.append((frame_idx, frame))
            
            # æ§åˆ¶å•ä¸ªé•œå¤´çš„æœ€å¤§å¸§æ•°
            if len(shot_frames) > self.batch_size:
                keyframe, keyframe_idx = self.process_shot(shot_frames)
                if keyframe is not None:
                    yield keyframe, keyframe_idx
                shot_frames.clear()
                gc.collect()
        
        # å¤„ç†æœ€åä¸€ä¸ªé•œå¤´
        if shot_frames:
            keyframe, keyframe_idx = self.process_shot(shot_frames)
            if keyframe is not None:
                yield keyframe, keyframe_idx

    def process_video(self, output_dir: str, skip_seconds: float = 0) -> None:
        """
        å¤„ç†è§†é¢‘å¹¶æå–å…³é”®å¸§ï¼Œä½¿ç”¨åˆ†æ‰¹å¤„ç†æ–¹å¼
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            skip_seconds: è·³è¿‡è§†é¢‘å¼€å¤´çš„ç§’æ•°
        """
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)
            
            # è®¡ç®—è¦è·³è¿‡çš„å¸§æ•°
            skip_frames = int(skip_seconds * self.fps)
            self.cap.set(cv2.CAP_PROP_POS_FRAMES, skip_frames)
            
            # æ£€æµ‹é•œå¤´è¾¹ç•Œ
            logger.info("å¼€å§‹æ£€æµ‹é•œå¤´è¾¹ç•Œ...")
            shot_boundaries = self.detect_shot_boundaries()
            
            # æå–å…³é”®å¸§
            logger.info("å¼€å§‹æå–å…³é”®å¸§...")
            frame_count = 0
            
            pbar = tqdm(self.extract_keyframes(shot_boundaries),
                       desc="æå–å…³é”®å¸§",
                       unit="å¸§")
            
            for keyframe, frame_idx in pbar:
                if frame_idx < skip_frames:
                    continue
                    
                # è®¡ç®—æ—¶é—´æˆ³
                timestamp = frame_idx / self.fps
                hours = int(timestamp // 3600)
                minutes = int((timestamp % 3600) // 60)
                seconds = int(timestamp % 60)
                time_str = f"{hours:02d}{minutes:02d}{seconds:02d}"
                
                # ä¿å­˜å…³é”®å¸§
                output_path = os.path.join(output_dir, 
                                         f'keyframe_{frame_idx:06d}_{time_str}.jpg')
                cv2.imwrite(output_path, keyframe)
                frame_count += 1
                
                pbar.set_postfix({"å·²ä¿å­˜": frame_count})
                
                if frame_count % 10 == 0:
                    gc.collect()
            
            logger.info(f"å…³é”®å¸§æå–å®Œæˆï¼Œå…±ä¿å­˜ {frame_count} å¸§åˆ° {output_dir}")
            
        except Exception as e:
            logger.error(f"è§†é¢‘å¤„ç†å¤±è´¥: {str(e)}")
            raise
        finally:
            # ç¡®ä¿èµ„æºè¢«é‡Šæ”¾
            self.cap.release()
            gc.collect()
</file>

<file path="app/asgi.py">
"""Application implementation - ASGI."""

import os

from fastapi import FastAPI, Request
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from loguru import logger
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware

from app.config import config
from app.models.exception import HttpException
from app.router import root_api_router
from app.utils import utils


def exception_handler(request: Request, e: HttpException):
    return JSONResponse(
        status_code=e.status_code,
        content=utils.get_response(e.status_code, e.data, e.message),
    )


def validation_exception_handler(request: Request, e: RequestValidationError):
    return JSONResponse(
        status_code=400,
        content=utils.get_response(
            status=400, data=e.errors(), message="field required"
        ),
    )


def get_application() -> FastAPI:
    """Initialize FastAPI application.

    Returns:
       FastAPI: Application object instance.

    """
    instance = FastAPI(
        title=config.project_name,
        description=config.project_description,
        version=config.project_version,
        debug=False,
    )
    instance.include_router(root_api_router)
    instance.add_exception_handler(HttpException, exception_handler)
    instance.add_exception_handler(RequestValidationError, validation_exception_handler)
    return instance


app = get_application()

# Configures the CORS middleware for the FastAPI app
cors_allowed_origins_str = os.getenv("CORS_ALLOWED_ORIGINS", "")
origins = cors_allowed_origins_str.split(",") if cors_allowed_origins_str else ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

task_dir = utils.task_dir()
app.mount(
    "/tasks", StaticFiles(directory=task_dir, html=True, follow_symlink=True), name=""
)

public_dir = utils.public_dir()
app.mount("/", StaticFiles(directory=public_dir, html=True), name="")


@app.on_event("shutdown")
def shutdown_event():
    logger.info("shutdown event")


@app.on_event("startup")
def startup_event():
    logger.info("startup event")
</file>

<file path="app/router.py">
"""Application configuration - root APIRouter.

Defines all FastAPI application endpoints.

Resources:
    1. https://fastapi.tiangolo.com/tutorial/bigger-applications

"""

from fastapi import APIRouter

from app.controllers.v1 import llm, video
from app.controllers.v2 import script

root_api_router = APIRouter()
# v1
root_api_router.include_router(video.router)
root_api_router.include_router(llm.router)

# v2
root_api_router.include_router(script.router)
</file>

<file path="docker/Dockerfile_MiniCPM">
ARG BASE=nvidia/cuda:12.1.0-devel-ubuntu22.04
FROM ${BASE}

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV http_proxy=http://host.docker.internal:7890
ENV https_proxy=http://host.docker.internal:7890
ENV DEBIAN_FRONTEND=noninteractive

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc g++ make git python3 python3-dev python3-pip python3-venv python3-wheel \
    espeak-ng libsndfile1-dev nano vim unzip wget xz-utils && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /root/MiniCPM-V/

# å®‰è£… Python ä¾èµ–
RUN git clone https://github.com/OpenBMB/MiniCPM-V.git && \
    cd MiniCPM-V && \
    pip3 install decord && \
    pip3 install --no-cache-dir -r requirements.txt && \
    pip3 install flash_attn

# æ¸…ç†ä»£ç†ç¯å¢ƒå˜é‡
ENV http_proxy=""
ENV https_proxy=""

# è®¾ç½® PYTHONPATH
ENV PYTHONPATH="/root/MiniCPM-V/"
</file>

<file path="docs/voice-list.txt">
Name: af-ZA-AdriNeural
Gender: Female

Name: af-ZA-WillemNeural
Gender: Male

Name: am-ET-AmehaNeural
Gender: Male

Name: am-ET-MekdesNeural
Gender: Female

Name: ar-AE-FatimaNeural
Gender: Female

Name: ar-AE-HamdanNeural
Gender: Male

Name: ar-BH-AliNeural
Gender: Male

Name: ar-BH-LailaNeural
Gender: Female

Name: ar-DZ-AminaNeural
Gender: Female

Name: ar-DZ-IsmaelNeural
Gender: Male

Name: ar-EG-SalmaNeural
Gender: Female

Name: ar-EG-ShakirNeural
Gender: Male

Name: ar-IQ-BasselNeural
Gender: Male

Name: ar-IQ-RanaNeural
Gender: Female

Name: ar-JO-SanaNeural
Gender: Female

Name: ar-JO-TaimNeural
Gender: Male

Name: ar-KW-FahedNeural
Gender: Male

Name: ar-KW-NouraNeural
Gender: Female

Name: ar-LB-LaylaNeural
Gender: Female

Name: ar-LB-RamiNeural
Gender: Male

Name: ar-LY-ImanNeural
Gender: Female

Name: ar-LY-OmarNeural
Gender: Male

Name: ar-MA-JamalNeural
Gender: Male

Name: ar-MA-MounaNeural
Gender: Female

Name: ar-OM-AbdullahNeural
Gender: Male

Name: ar-OM-AyshaNeural
Gender: Female

Name: ar-QA-AmalNeural
Gender: Female

Name: ar-QA-MoazNeural
Gender: Male

Name: ar-SA-HamedNeural
Gender: Male

Name: ar-SA-ZariyahNeural
Gender: Female

Name: ar-SY-AmanyNeural
Gender: Female

Name: ar-SY-LaithNeural
Gender: Male

Name: ar-TN-HediNeural
Gender: Male

Name: ar-TN-ReemNeural
Gender: Female

Name: ar-YE-MaryamNeural
Gender: Female

Name: ar-YE-SalehNeural
Gender: Male

Name: az-AZ-BabekNeural
Gender: Male

Name: az-AZ-BanuNeural
Gender: Female

Name: bg-BG-BorislavNeural
Gender: Male

Name: bg-BG-KalinaNeural
Gender: Female

Name: bn-BD-NabanitaNeural
Gender: Female

Name: bn-BD-PradeepNeural
Gender: Male

Name: bn-IN-BashkarNeural
Gender: Male

Name: bn-IN-TanishaaNeural
Gender: Female

Name: bs-BA-GoranNeural
Gender: Male

Name: bs-BA-VesnaNeural
Gender: Female

Name: ca-ES-EnricNeural
Gender: Male

Name: ca-ES-JoanaNeural
Gender: Female

Name: cs-CZ-AntoninNeural
Gender: Male

Name: cs-CZ-VlastaNeural
Gender: Female

Name: cy-GB-AledNeural
Gender: Male

Name: cy-GB-NiaNeural
Gender: Female

Name: da-DK-ChristelNeural
Gender: Female

Name: da-DK-JeppeNeural
Gender: Male

Name: de-AT-IngridNeural
Gender: Female

Name: de-AT-JonasNeural
Gender: Male

Name: de-CH-JanNeural
Gender: Male

Name: de-CH-LeniNeural
Gender: Female

Name: de-DE-AmalaNeural
Gender: Female

Name: de-DE-ConradNeural
Gender: Male

Name: de-DE-FlorianMultilingualNeural
Gender: Male

Name: de-DE-KatjaNeural
Gender: Female

Name: de-DE-KillianNeural
Gender: Male

Name: de-DE-SeraphinaMultilingualNeural
Gender: Female

Name: el-GR-AthinaNeural
Gender: Female

Name: el-GR-NestorasNeural
Gender: Male

Name: en-AU-NatashaNeural
Gender: Female

Name: en-AU-WilliamNeural
Gender: Male

Name: en-CA-ClaraNeural
Gender: Female

Name: en-CA-LiamNeural
Gender: Male

Name: en-GB-LibbyNeural
Gender: Female

Name: en-GB-MaisieNeural
Gender: Female

Name: en-GB-RyanNeural
Gender: Male

Name: en-GB-SoniaNeural
Gender: Female

Name: en-GB-ThomasNeural
Gender: Male

Name: en-HK-SamNeural
Gender: Male

Name: en-HK-YanNeural
Gender: Female

Name: en-IE-ConnorNeural
Gender: Male

Name: en-IE-EmilyNeural
Gender: Female

Name: en-IN-NeerjaExpressiveNeural
Gender: Female

Name: en-IN-NeerjaNeural
Gender: Female

Name: en-IN-PrabhatNeural
Gender: Male

Name: en-KE-AsiliaNeural
Gender: Female

Name: en-KE-ChilembaNeural
Gender: Male

Name: en-NG-AbeoNeural
Gender: Male

Name: en-NG-EzinneNeural
Gender: Female

Name: en-NZ-MitchellNeural
Gender: Male

Name: en-NZ-MollyNeural
Gender: Female

Name: en-PH-JamesNeural
Gender: Male

Name: en-PH-RosaNeural
Gender: Female

Name: en-SG-LunaNeural
Gender: Female

Name: en-SG-WayneNeural
Gender: Male

Name: en-TZ-ElimuNeural
Gender: Male

Name: en-TZ-ImaniNeural
Gender: Female

Name: en-US-AnaNeural
Gender: Female

Name: en-US-AndrewNeural
Gender: Male

Name: en-US-AriaNeural
Gender: Female

Name: en-US-AvaNeural
Gender: Female

Name: en-US-BrianNeural
Gender: Male

Name: en-US-ChristopherNeural
Gender: Male

Name: en-US-EmmaNeural
Gender: Female

Name: en-US-EricNeural
Gender: Male

Name: en-US-GuyNeural
Gender: Male

Name: en-US-JennyNeural
Gender: Female

Name: en-US-MichelleNeural
Gender: Female

Name: en-US-RogerNeural
Gender: Male

Name: en-US-SteffanNeural
Gender: Male

Name: en-ZA-LeahNeural
Gender: Female

Name: en-ZA-LukeNeural
Gender: Male

Name: es-AR-ElenaNeural
Gender: Female

Name: es-AR-TomasNeural
Gender: Male

Name: es-BO-MarceloNeural
Gender: Male

Name: es-BO-SofiaNeural
Gender: Female

Name: es-CL-CatalinaNeural
Gender: Female

Name: es-CL-LorenzoNeural
Gender: Male

Name: es-CO-GonzaloNeural
Gender: Male

Name: es-CO-SalomeNeural
Gender: Female

Name: es-CR-JuanNeural
Gender: Male

Name: es-CR-MariaNeural
Gender: Female

Name: es-CU-BelkysNeural
Gender: Female

Name: es-CU-ManuelNeural
Gender: Male

Name: es-DO-EmilioNeural
Gender: Male

Name: es-DO-RamonaNeural
Gender: Female

Name: es-EC-AndreaNeural
Gender: Female

Name: es-EC-LuisNeural
Gender: Male

Name: es-ES-AlvaroNeural
Gender: Male

Name: es-ES-ElviraNeural
Gender: Female

Name: es-ES-XimenaNeural
Gender: Female

Name: es-GQ-JavierNeural
Gender: Male

Name: es-GQ-TeresaNeural
Gender: Female

Name: es-GT-AndresNeural
Gender: Male

Name: es-GT-MartaNeural
Gender: Female

Name: es-HN-CarlosNeural
Gender: Male

Name: es-HN-KarlaNeural
Gender: Female

Name: es-MX-DaliaNeural
Gender: Female

Name: es-MX-JorgeNeural
Gender: Male

Name: es-NI-FedericoNeural
Gender: Male

Name: es-NI-YolandaNeural
Gender: Female

Name: es-PA-MargaritaNeural
Gender: Female

Name: es-PA-RobertoNeural
Gender: Male

Name: es-PE-AlexNeural
Gender: Male

Name: es-PE-CamilaNeural
Gender: Female

Name: es-PR-KarinaNeural
Gender: Female

Name: es-PR-VictorNeural
Gender: Male

Name: es-PY-MarioNeural
Gender: Male

Name: es-PY-TaniaNeural
Gender: Female

Name: es-SV-LorenaNeural
Gender: Female

Name: es-SV-RodrigoNeural
Gender: Male

Name: es-US-AlonsoNeural
Gender: Male

Name: es-US-PalomaNeural
Gender: Female

Name: es-UY-MateoNeural
Gender: Male

Name: es-UY-ValentinaNeural
Gender: Female

Name: es-VE-PaolaNeural
Gender: Female

Name: es-VE-SebastianNeural
Gender: Male

Name: et-EE-AnuNeural
Gender: Female

Name: et-EE-KertNeural
Gender: Male

Name: fa-IR-DilaraNeural
Gender: Female

Name: fa-IR-FaridNeural
Gender: Male

Name: fi-FI-HarriNeural
Gender: Male

Name: fi-FI-NooraNeural
Gender: Female

Name: fil-PH-AngeloNeural
Gender: Male

Name: fil-PH-BlessicaNeural
Gender: Female

Name: fr-BE-CharlineNeural
Gender: Female

Name: fr-BE-GerardNeural
Gender: Male

Name: fr-CA-AntoineNeural
Gender: Male

Name: fr-CA-JeanNeural
Gender: Male

Name: fr-CA-SylvieNeural
Gender: Female

Name: fr-CA-ThierryNeural
Gender: Male

Name: fr-CH-ArianeNeural
Gender: Female

Name: fr-CH-FabriceNeural
Gender: Male

Name: fr-FR-DeniseNeural
Gender: Female

Name: fr-FR-EloiseNeural
Gender: Female

Name: fr-FR-HenriNeural
Gender: Male

Name: fr-FR-RemyMultilingualNeural
Gender: Male

Name: fr-FR-VivienneMultilingualNeural
Gender: Female

Name: ga-IE-ColmNeural
Gender: Male

Name: ga-IE-OrlaNeural
Gender: Female

Name: gl-ES-RoiNeural
Gender: Male

Name: gl-ES-SabelaNeural
Gender: Female

Name: gu-IN-DhwaniNeural
Gender: Female

Name: gu-IN-NiranjanNeural
Gender: Male

Name: he-IL-AvriNeural
Gender: Male

Name: he-IL-HilaNeural
Gender: Female

Name: hi-IN-MadhurNeural
Gender: Male

Name: hi-IN-SwaraNeural
Gender: Female

Name: hr-HR-GabrijelaNeural
Gender: Female

Name: hr-HR-SreckoNeural
Gender: Male

Name: hu-HU-NoemiNeural
Gender: Female

Name: hu-HU-TamasNeural
Gender: Male

Name: id-ID-ArdiNeural
Gender: Male

Name: id-ID-GadisNeural
Gender: Female

Name: is-IS-GudrunNeural
Gender: Female

Name: is-IS-GunnarNeural
Gender: Male

Name: it-IT-DiegoNeural
Gender: Male

Name: it-IT-ElsaNeural
Gender: Female

Name: it-IT-GiuseppeNeural
Gender: Male

Name: it-IT-IsabellaNeural
Gender: Female

Name: ja-JP-KeitaNeural
Gender: Male

Name: ja-JP-NanamiNeural
Gender: Female

Name: jv-ID-DimasNeural
Gender: Male

Name: jv-ID-SitiNeural
Gender: Female

Name: ka-GE-EkaNeural
Gender: Female

Name: ka-GE-GiorgiNeural
Gender: Male

Name: kk-KZ-AigulNeural
Gender: Female

Name: kk-KZ-DauletNeural
Gender: Male

Name: km-KH-PisethNeural
Gender: Male

Name: km-KH-SreymomNeural
Gender: Female

Name: kn-IN-GaganNeural
Gender: Male

Name: kn-IN-SapnaNeural
Gender: Female

Name: ko-KR-HyunsuNeural
Gender: Male

Name: ko-KR-InJoonNeural
Gender: Male

Name: ko-KR-SunHiNeural
Gender: Female

Name: lo-LA-ChanthavongNeural
Gender: Male

Name: lo-LA-KeomanyNeural
Gender: Female

Name: lt-LT-LeonasNeural
Gender: Male

Name: lt-LT-OnaNeural
Gender: Female

Name: lv-LV-EveritaNeural
Gender: Female

Name: lv-LV-NilsNeural
Gender: Male

Name: mk-MK-AleksandarNeural
Gender: Male

Name: mk-MK-MarijaNeural
Gender: Female

Name: ml-IN-MidhunNeural
Gender: Male

Name: ml-IN-SobhanaNeural
Gender: Female

Name: mn-MN-BataaNeural
Gender: Male

Name: mn-MN-YesuiNeural
Gender: Female

Name: mr-IN-AarohiNeural
Gender: Female

Name: mr-IN-ManoharNeural
Gender: Male

Name: ms-MY-OsmanNeural
Gender: Male

Name: ms-MY-YasminNeural
Gender: Female

Name: mt-MT-GraceNeural
Gender: Female

Name: mt-MT-JosephNeural
Gender: Male

Name: my-MM-NilarNeural
Gender: Female

Name: my-MM-ThihaNeural
Gender: Male

Name: nb-NO-FinnNeural
Gender: Male

Name: nb-NO-PernilleNeural
Gender: Female

Name: ne-NP-HemkalaNeural
Gender: Female

Name: ne-NP-SagarNeural
Gender: Male

Name: nl-BE-ArnaudNeural
Gender: Male

Name: nl-BE-DenaNeural
Gender: Female

Name: nl-NL-ColetteNeural
Gender: Female

Name: nl-NL-FennaNeural
Gender: Female

Name: nl-NL-MaartenNeural
Gender: Male

Name: pl-PL-MarekNeural
Gender: Male

Name: pl-PL-ZofiaNeural
Gender: Female

Name: ps-AF-GulNawazNeural
Gender: Male

Name: ps-AF-LatifaNeural
Gender: Female

Name: pt-BR-AntonioNeural
Gender: Male

Name: pt-BR-FranciscaNeural
Gender: Female

Name: pt-BR-ThalitaNeural
Gender: Female

Name: pt-PT-DuarteNeural
Gender: Male

Name: pt-PT-RaquelNeural
Gender: Female

Name: ro-RO-AlinaNeural
Gender: Female

Name: ro-RO-EmilNeural
Gender: Male

Name: ru-RU-DmitryNeural
Gender: Male

Name: ru-RU-SvetlanaNeural
Gender: Female

Name: si-LK-SameeraNeural
Gender: Male

Name: si-LK-ThiliniNeural
Gender: Female

Name: sk-SK-LukasNeural
Gender: Male

Name: sk-SK-ViktoriaNeural
Gender: Female

Name: sl-SI-PetraNeural
Gender: Female

Name: sl-SI-RokNeural
Gender: Male

Name: so-SO-MuuseNeural
Gender: Male

Name: so-SO-UbaxNeural
Gender: Female

Name: sq-AL-AnilaNeural
Gender: Female

Name: sq-AL-IlirNeural
Gender: Male

Name: sr-RS-NicholasNeural
Gender: Male

Name: sr-RS-SophieNeural
Gender: Female

Name: su-ID-JajangNeural
Gender: Male

Name: su-ID-TutiNeural
Gender: Female

Name: sv-SE-MattiasNeural
Gender: Male

Name: sv-SE-SofieNeural
Gender: Female

Name: sw-KE-RafikiNeural
Gender: Male

Name: sw-KE-ZuriNeural
Gender: Female

Name: sw-TZ-DaudiNeural
Gender: Male

Name: sw-TZ-RehemaNeural
Gender: Female

Name: ta-IN-PallaviNeural
Gender: Female

Name: ta-IN-ValluvarNeural
Gender: Male

Name: ta-LK-KumarNeural
Gender: Male

Name: ta-LK-SaranyaNeural
Gender: Female

Name: ta-MY-KaniNeural
Gender: Female

Name: ta-MY-SuryaNeural
Gender: Male

Name: ta-SG-AnbuNeural
Gender: Male

Name: ta-SG-VenbaNeural
Gender: Female

Name: te-IN-MohanNeural
Gender: Male

Name: te-IN-ShrutiNeural
Gender: Female

Name: th-TH-NiwatNeural
Gender: Male

Name: th-TH-PremwadeeNeural
Gender: Female

Name: tr-TR-AhmetNeural
Gender: Male

Name: tr-TR-EmelNeural
Gender: Female

Name: uk-UA-OstapNeural
Gender: Male

Name: uk-UA-PolinaNeural
Gender: Female

Name: ur-IN-GulNeural
Gender: Female

Name: ur-IN-SalmanNeural
Gender: Male

Name: ur-PK-AsadNeural
Gender: Male

Name: ur-PK-UzmaNeural
Gender: Female

Name: uz-UZ-MadinaNeural
Gender: Female

Name: uz-UZ-SardorNeural
Gender: Male

Name: vi-VN-HoaiMyNeural
Gender: Female

Name: vi-VN-NamMinhNeural
Gender: Male

Name: zh-CN-XiaoxiaoNeural
Gender: Female

Name: zh-CN-XiaoyiNeural
Gender: Female

Name: zh-CN-YunjianNeural
Gender: Male

Name: zh-CN-YunxiNeural
Gender: Male

Name: zh-CN-YunxiaNeural
Gender: Male

Name: zh-CN-YunyangNeural
Gender: Male

Name: zh-CN-liaoning-XiaobeiNeural
Gender: Female

Name: zh-CN-shaanxi-XiaoniNeural
Gender: Female

Name: zh-HK-HiuGaaiNeural
Gender: Female

Name: zh-HK-HiuMaanNeural
Gender: Female

Name: zh-HK-WanLungNeural
Gender: Male

Name: zh-TW-HsiaoChenNeural
Gender: Female

Name: zh-TW-HsiaoYuNeural
Gender: Female

Name: zh-TW-YunJheNeural
Gender: Male

Name: zu-ZA-ThandoNeural
Gender: Female

Name: zu-ZA-ThembaNeural
Gender: Male
</file>

<file path="resource/fonts/fonts_in_here.txt">
æ­¤å¤„æ”¾å­—ä½“æ–‡ä»¶
</file>

<file path="resource/public/index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>NarratoAI</title>
</head>
<body>
<h1>NarratoAI</h1>
<a href="https://github.com/linyqh/NarratoAI">é¡¹ç›®åœ°å€ï¼šhttps://github.com/linyqh/NarratoAI</a>
<hr>
</hr>
<a href="http://127.0.0.1:8501">webui åœ°å€ï¼šhttp://127.0.0.1:8501</a>
<br>
<a href="http://127.0.0.1:8080/docs">api swagger åœ°å€ï¼šhttp://127.0.0.1:8080/docs</a>
<hr>
</hr>
<p>
    NarratoAI æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–å½±è§†è§£è¯´å·¥å…·ï¼ŒåŸºäºLLMå®ç°æ–‡æ¡ˆæ’°å†™ã€è‡ªåŠ¨åŒ–è§†é¢‘å‰ªè¾‘ã€é…éŸ³å’Œå­—å¹•ç”Ÿæˆçš„ä¸€ç«™å¼æµç¨‹ï¼ŒåŠ©åŠ›é«˜æ•ˆå†…å®¹åˆ›ä½œã€‚
</p>

<p>
    NarratoAI is an automated film and television commentary tool that implements a one-stop process of copywriting, automated video editing, dubbing and subtitle generation based on LLM, facilitating efficient content creation.
</p>
</body>
</html>
</file>

<file path="webui/components/__init__.py">
from .basic_settings import render_basic_settings
from .script_settings import render_script_panel
from .video_settings import render_video_panel
from .audio_settings import render_audio_panel
from .subtitle_settings import render_subtitle_panel
from .review_settings import render_review_panel

__all__ = [
    'render_basic_settings',
    'render_script_panel',
    'render_video_panel',
    'render_audio_panel',
    'render_subtitle_panel',
    'render_review_panel'
]
</file>

<file path="webui/components/audio_settings.py">
import streamlit as st
import os
from uuid import uuid4
from app.config import config
from app.services import voice
from app.utils import utils
from webui.utils.cache import get_songs_cache


def render_audio_panel(tr):
    """æ¸²æŸ“éŸ³é¢‘è®¾ç½®é¢æ¿"""
    with st.container(border=True):
        st.write(tr("Audio Settings"))

        # æ¸²æŸ“TTSè®¾ç½®
        render_tts_settings(tr)

        # æ¸²æŸ“èƒŒæ™¯éŸ³ä¹è®¾ç½®
        render_bgm_settings(tr)


def render_tts_settings(tr):
    """æ¸²æŸ“TTS(æ–‡æœ¬è½¬è¯­éŸ³)è®¾ç½®"""
    # è·å–æ”¯æŒçš„è¯­éŸ³åˆ—è¡¨
    support_locales = ["zh-CN", "en-US"]
    voices = voice.get_all_azure_voices(filter_locals=support_locales)

    # åˆ›å»ºå‹å¥½çš„æ˜¾ç¤ºåç§°
    friendly_names = {
        v: v.replace("Female", tr("Female"))
        .replace("Male", tr("Male"))
        .replace("Neural", "")
        for v in voices
    }

    # è·å–ä¿å­˜çš„è¯­éŸ³è®¾ç½®
    saved_voice_name = config.ui.get("voice_name", "")
    saved_voice_name_index = 0

    if saved_voice_name in friendly_names:
        saved_voice_name_index = list(friendly_names.keys()).index(saved_voice_name)
    else:
        # å¦‚æœæ²¡æœ‰ä¿å­˜çš„è®¾ç½®ï¼Œé€‰æ‹©ä¸UIè¯­è¨€åŒ¹é…çš„ç¬¬ä¸€ä¸ªè¯­éŸ³
        for i, v in enumerate(voices):
            if (v.lower().startswith(st.session_state["ui_language"].lower())
                    and "V2" not in v):
                saved_voice_name_index = i
                break

    # è¯­éŸ³é€‰æ‹©ä¸‹æ‹‰æ¡†
    selected_friendly_name = st.selectbox(
        tr("Speech Synthesis"),
        options=list(friendly_names.values()),
        index=saved_voice_name_index,
    )

    # è·å–å®é™…çš„è¯­éŸ³åç§°
    voice_name = list(friendly_names.keys())[
        list(friendly_names.values()).index(selected_friendly_name)
    ]

    # ä¿å­˜è®¾ç½®
    config.ui["voice_name"] = voice_name

    # Azure V2è¯­éŸ³ç‰¹æ®Šå¤„ç†
    if voice.is_azure_v2_voice(voice_name):
        render_azure_v2_settings(tr)

    # è¯­éŸ³å‚æ•°è®¾ç½®
    render_voice_parameters(tr)

    # è¯•å¬æŒ‰é’®
    render_voice_preview(tr, voice_name)


def render_azure_v2_settings(tr):
    """æ¸²æŸ“Azure V2è¯­éŸ³è®¾ç½®"""
    saved_azure_speech_region = config.azure.get("speech_region", "")
    saved_azure_speech_key = config.azure.get("speech_key", "")

    azure_speech_region = st.text_input(
        tr("Speech Region"),
        value=saved_azure_speech_region
    )
    azure_speech_key = st.text_input(
        tr("Speech Key"),
        value=saved_azure_speech_key,
        type="password"
    )

    config.azure["speech_region"] = azure_speech_region
    config.azure["speech_key"] = azure_speech_key


def render_voice_parameters(tr):
    """æ¸²æŸ“è¯­éŸ³å‚æ•°è®¾ç½®"""
    # éŸ³é‡
    voice_volume = st.slider(
        tr("Speech Volume"),
        min_value=0.0,
        max_value=1.0,
        value=1.0,
        step=0.01,
        help=tr("Adjust the volume of the original audio")
    )
    st.session_state['voice_volume'] = voice_volume


    # è¯­é€Ÿ
    voice_rate = st.selectbox(
        tr("Speech Rate"),
        options=[0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.5, 1.8, 2.0],
        index=2,
    )
    st.session_state['voice_rate'] = voice_rate

    # éŸ³è°ƒ
    voice_pitch = st.selectbox(
        tr("Speech Pitch"),
        options=[0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.5, 1.8, 2.0],
        index=2,
    )
    st.session_state['voice_pitch'] = voice_pitch


def render_voice_preview(tr, voice_name):
    """æ¸²æŸ“è¯­éŸ³è¯•å¬åŠŸèƒ½"""
    if st.button(tr("Play Voice")):
        play_content = "æ„Ÿè°¢å…³æ³¨ NarratoAIï¼Œæœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œå¯ä»¥å…³æ³¨å¾®ä¿¡å…¬ä¼—å·ï¼Œæ±‚åŠ©æˆ–è®¨è®º"
        if not play_content:
            play_content = st.session_state.get('video_script', '')
        if not play_content:
            play_content = tr("Voice Example")

        with st.spinner(tr("Synthesizing Voice")):
            temp_dir = utils.storage_dir("temp", create=True)
            audio_file = os.path.join(temp_dir, f"tmp-voice-{str(uuid4())}.mp3")

            sub_maker = voice.tts(
                text=play_content,
                voice_name=voice_name,
                voice_rate=st.session_state.get('voice_rate', 1.0),
                voice_pitch=st.session_state.get('voice_pitch', 1.0),
                voice_file=audio_file,
            )

            # å¦‚æœè¯­éŸ³æ–‡ä»¶ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å†…å®¹é‡è¯•
            if not sub_maker:
                play_content = "This is a example voice. if you hear this, the voice synthesis failed with the original content."
                sub_maker = voice.tts(
                    text=play_content,
                    voice_name=voice_name,
                    voice_rate=st.session_state.get('voice_rate', 1.0),
                    voice_pitch=st.session_state.get('voice_pitch', 1.0),
                    voice_file=audio_file,
                )

            if sub_maker and os.path.exists(audio_file):
                st.audio(audio_file, format="audio/mp3")
                if os.path.exists(audio_file):
                    os.remove(audio_file)


def render_bgm_settings(tr):
    """æ¸²æŸ“èƒŒæ™¯éŸ³ä¹è®¾ç½®"""
    # èƒŒæ™¯éŸ³ä¹é€‰é¡¹
    bgm_options = [
        (tr("No Background Music"), ""),
        (tr("Random Background Music"), "random"),
        (tr("Custom Background Music"), "custom"),
    ]

    selected_index = st.selectbox(
        tr("Background Music"),
        index=1,
        options=range(len(bgm_options)),
        format_func=lambda x: bgm_options[x][0],
    )

    # è·å–é€‰æ‹©çš„èƒŒæ™¯éŸ³ä¹ç±»å‹
    bgm_type = bgm_options[selected_index][1]
    st.session_state['bgm_type'] = bgm_type

    # è‡ªå®šä¹‰èƒŒæ™¯éŸ³ä¹å¤„ç†
    if bgm_type == "custom":
        custom_bgm_file = st.text_input(tr("Custom Background Music File"))
        if custom_bgm_file and os.path.exists(custom_bgm_file):
            st.session_state['bgm_file'] = custom_bgm_file

    # èƒŒæ™¯éŸ³ä¹éŸ³é‡
    bgm_volume = st.slider(
        tr("Background Music Volume"),
        min_value=0.0,
        max_value=1.0,
        value=0.3,
        step=0.01,
        help=tr("Adjust the volume of the original audio")
    )
    st.session_state['bgm_volume'] = bgm_volume


def get_audio_params():
    """è·å–éŸ³é¢‘å‚æ•°"""
    return {
        'voice_name': config.ui.get("voice_name", ""),
        'voice_volume': st.session_state.get('voice_volume', 1.0),
        'voice_rate': st.session_state.get('voice_rate', 1.0),
        'voice_pitch': st.session_state.get('voice_pitch', 1.0),
        'bgm_type': st.session_state.get('bgm_type', 'random'),
        'bgm_file': st.session_state.get('bgm_file', ''),
        'bgm_volume': st.session_state.get('bgm_volume', 0.3),
    }
</file>

<file path="webui/components/basic_settings.py">
import streamlit as st
import os
from app.config import config
from app.utils import utils


def render_basic_settings(tr):
    """æ¸²æŸ“åŸºç¡€è®¾ç½®é¢æ¿"""
    with st.expander(tr("Basic Settings"), expanded=False):
        config_panels = st.columns(3)
        left_config_panel = config_panels[0]
        middle_config_panel = config_panels[1]
        right_config_panel = config_panels[2]

        with left_config_panel:
            render_language_settings(tr)
            render_proxy_settings(tr)

        with middle_config_panel:
            render_vision_llm_settings(tr)  # è§†é¢‘åˆ†ææ¨¡å‹è®¾ç½®

        with right_config_panel:
            render_text_llm_settings(tr)  # æ–‡æ¡ˆç”Ÿæˆæ¨¡å‹è®¾ç½®


def render_language_settings(tr):
    st.subheader(tr("Proxy Settings"))

    """æ¸²æŸ“è¯­è¨€è®¾ç½®"""
    system_locale = utils.get_system_locale()
    i18n_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "i18n")
    locales = utils.load_locales(i18n_dir)

    display_languages = []
    selected_index = 0
    for i, code in enumerate(locales.keys()):
        display_languages.append(f"{code} - {locales[code].get('Language')}")
        if code == st.session_state.get('ui_language', system_locale):
            selected_index = i

    selected_language = st.selectbox(
        tr("Language"),
        options=display_languages,
        index=selected_index
    )

    if selected_language:
        code = selected_language.split(" - ")[0].strip()
        st.session_state['ui_language'] = code
        config.ui['language'] = code


def render_proxy_settings(tr):
    """æ¸²æŸ“ä»£ç†è®¾ç½®"""
    # è·å–å½“å‰ä»£ç†çŠ¶æ€
    proxy_enabled = config.proxy.get("enabled", True)
    proxy_url_http = config.proxy.get("http")
    proxy_url_https = config.proxy.get("https")

    # æ·»åŠ ä»£ç†å¼€å…³
    proxy_enabled = st.checkbox(tr("Enable Proxy"), value=proxy_enabled)
    
    # ä¿å­˜ä»£ç†å¼€å…³çŠ¶æ€
    config.proxy["enabled"] = proxy_enabled

    # åªæœ‰åœ¨ä»£ç†å¯ç”¨æ—¶æ‰æ˜¾ç¤ºä»£ç†è®¾ç½®è¾“å…¥æ¡†
    if proxy_enabled:
        HTTP_PROXY = st.text_input(tr("HTTP_PROXY"), value=proxy_url_http)
        HTTPS_PROXY = st.text_input(tr("HTTPs_PROXY"), value=proxy_url_https)

        if HTTP_PROXY:
            config.proxy["http"] = HTTP_PROXY
            os.environ["HTTP_PROXY"] = HTTP_PROXY
        if HTTPS_PROXY:
            config.proxy["https"] = HTTPS_PROXY
            os.environ["HTTPS_PROXY"] = HTTPS_PROXY
    else:
        # å½“ä»£ç†è¢«ç¦ç”¨æ—¶ï¼Œæ¸…é™¤ç¯å¢ƒå˜é‡å’Œé…ç½®
        os.environ.pop("HTTP_PROXY", None)
        os.environ.pop("HTTPS_PROXY", None)
        config.proxy["http"] = ""
        config.proxy["https"] = ""


def test_vision_model_connection(api_key, base_url, model_name, provider, tr):
    """æµ‹è¯•è§†è§‰æ¨¡å‹è¿æ¥
    
    Args:
        api_key: APIå¯†é’¥
        base_url: åŸºç¡€URL
        model_name: æ¨¡å‹åç§°
        provider: æä¾›å•†åç§°
    
    Returns:
        bool: è¿æ¥æ˜¯å¦æˆåŠŸ
        str: æµ‹è¯•ç»“æœæ¶ˆæ¯
    """
    if provider.lower() == 'gemini':
        import google.generativeai as genai
        
        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(model_name)
            model.generate_content("ç›´æ¥å›å¤æˆ‘æ–‡æœ¬'å½“å‰ç½‘ç»œå¯ç”¨'")
            return True, tr("gemini model is available")
        except Exception as e:
            return False, f"{tr('gemini model is not available')}: {str(e)}"

    elif provider.lower() == 'qwenvl':
        from openai import OpenAI
        try:
            client = OpenAI(
                api_key=api_key,
                base_url=base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
            
            # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
            response = client.chat.completions.create(
                model=model_name or "qwen-vl-max-latest",
                messages=[{"role": "user", "content": "ç›´æ¥å›å¤æˆ‘æ–‡æœ¬'å½“å‰ç½‘ç»œå¯ç”¨'"}]
            )
            
            if response and response.choices:
                return True, tr("QwenVL model is available")
            else:
                return False, tr("QwenVL model returned invalid response")
                
        except Exception as e:
            return False, f"{tr('QwenVL model is not available')}: {str(e)}"
            
    elif provider.lower() == 'narratoapi':
        import requests
        try:
            # æ„å»ºæµ‹è¯•è¯·æ±‚
            headers = {
                "Authorization": f"Bearer {api_key}"
            }
        
            test_url = f"{base_url.rstrip('/')}/health"
            response = requests.get(test_url, headers=headers, timeout=10)
        
            if response.status_code == 200:
                return True, tr("NarratoAPI is available")
            else:
                return False, f"{tr('NarratoAPI is not available')}: HTTP {response.status_code}"
        except Exception as e:
            return False, f"{tr('NarratoAPI is not available')}: {str(e)}"
            
    else:
        return False, f"{tr('Unsupported provider')}: {provider}"


def render_vision_llm_settings(tr):
    """æ¸²æŸ“è§†é¢‘åˆ†ææ¨¡å‹è®¾ç½®"""
    st.subheader(tr("Vision Model Settings"))

    # è§†é¢‘åˆ†ææ¨¡å‹æä¾›å•†é€‰æ‹©
    vision_providers = ['Gemini', 'QwenVL', 'NarratoAPI(å¾…å‘å¸ƒ)']
    saved_vision_provider = config.app.get("vision_llm_provider", "Gemini").lower()
    saved_provider_index = 0

    for i, provider in enumerate(vision_providers):
        if provider.lower() == saved_vision_provider:
            saved_provider_index = i
            break

    vision_provider = st.selectbox(
        tr("Vision Model Provider"),
        options=vision_providers,
        index=saved_provider_index
    )
    vision_provider = vision_provider.lower()
    config.app["vision_llm_provider"] = vision_provider
    st.session_state['vision_llm_providers'] = vision_provider

    # è·å–å·²ä¿å­˜çš„è§†è§‰æ¨¡å‹é…ç½®
    vision_api_key = config.app.get(f"vision_{vision_provider}_api_key", "")
    vision_base_url = config.app.get(f"vision_{vision_provider}_base_url", "")
    vision_model_name = config.app.get(f"vision_{vision_provider}_model_name", "")

    # æ¸²æŸ“è§†è§‰æ¨¡å‹é…ç½®è¾“å…¥æ¡†
    st_vision_api_key = st.text_input(tr("Vision API Key"), value=vision_api_key, type="password")
    
    # æ ¹æ®ä¸åŒæä¾›å•†è®¾ç½®é»˜è®¤å€¼å’Œå¸®åŠ©ä¿¡æ¯
    if vision_provider == 'gemini':
        st_vision_base_url = st.text_input(
            tr("Vision Base URL"), 
            value=vision_base_url,
            disabled=True,
            help=tr("Gemini API does not require a base URL")
        )
        st_vision_model_name = st.text_input(
            tr("Vision Model Name"), 
            value=vision_model_name or "gemini-1.5-flash",
            help=tr("Default: gemini-1.5-flash")
        )
    elif vision_provider == 'qwenvl':
        st_vision_base_url = st.text_input(
            tr("Vision Base URL"), 
            value=vision_base_url,
            help=tr("Default: https://dashscope.aliyuncs.com/compatible-mode/v1")
        )
        st_vision_model_name = st.text_input(
            tr("Vision Model Name"), 
            value=vision_model_name or "qwen-vl-max-latest",
            help=tr("Default: qwen-vl-max-latest")
        )
    else:
        st_vision_base_url = st.text_input(tr("Vision Base URL"), value=vision_base_url)
        st_vision_model_name = st.text_input(tr("Vision Model Name"), value=vision_model_name)

    # åœ¨é…ç½®è¾“å…¥æ¡†åæ·»åŠ æµ‹è¯•æŒ‰é’®
    if st.button(tr("Test Connection"), key="test_vision_connection"):
        with st.spinner(tr("Testing connection...")):
            success, message = test_vision_model_connection(
                api_key=st_vision_api_key,
                base_url=st_vision_base_url,
                model_name=st_vision_model_name,
                provider=vision_provider,
                tr=tr
            )
            
            if success:
                st.success(tr(message))
            else:
                st.error(tr(message))

    # ä¿å­˜è§†è§‰æ¨¡å‹é…ç½®
    if st_vision_api_key:
        config.app[f"vision_{vision_provider}_api_key"] = st_vision_api_key
        st.session_state[f"vision_{vision_provider}_api_key"] = st_vision_api_key
    if st_vision_base_url:
        config.app[f"vision_{vision_provider}_base_url"] = st_vision_base_url
        st.session_state[f"vision_{vision_provider}_base_url"] = st_vision_base_url
    if st_vision_model_name:
        config.app[f"vision_{vision_provider}_model_name"] = st_vision_model_name
        st.session_state[f"vision_{vision_provider}_model_name"] = st_vision_model_name


def test_text_model_connection(api_key, base_url, model_name, provider, tr):
    """æµ‹è¯•æ–‡æœ¬æ¨¡å‹è¿æ¥
    
    Args:
        api_key: APIå¯†é’¥
        base_url: åŸºç¡€URL
        model_name: æ¨¡å‹åç§°
        provider: æä¾›å•†åç§°
    
    Returns:
        bool: è¿æ¥æ˜¯å¦æˆåŠŸ
        str: æµ‹è¯•ç»“æœæ¶ˆæ¯
    """
    import requests
    
    try:
        # æ„å»ºç»Ÿä¸€çš„æµ‹è¯•è¯·æ±‚ï¼ˆéµå¾ªOpenAIæ ¼å¼ï¼‰
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šbase_urlï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not base_url:
            if provider.lower() == 'openai':
                base_url = "https://api.openai.com/v1"
            elif provider.lower() == 'moonshot':
                base_url = "https://api.moonshot.cn/v1"
            elif provider.lower() == 'deepseek':
                base_url = "https://api.deepseek.com/v1"
                
        # æ„å»ºæµ‹è¯•URL
        test_url = f"{base_url.rstrip('/')}/chat/completions"
        
        # ç‰¹æ®Šå¤„ç†Gemini
        if provider.lower() == 'gemini':
            import google.generativeai as genai
            try:
                genai.configure(api_key=api_key)
                model = genai.GenerativeModel(model_name or 'gemini-pro')
                model.generate_content("ç›´æ¥å›å¤æˆ‘æ–‡æœ¬'å½“å‰ç½‘ç»œå¯ç”¨'")
                return True, tr("Gemini model is available")
            except Exception as e:
                return False, f"{tr('Gemini model is not available')}: {str(e)}"
        
        # æ„å»ºæµ‹è¯•æ¶ˆæ¯
        test_data = {
            "model": model_name,
            "messages": [
                {"role": "user", "content": "ç›´æ¥å›å¤æˆ‘æ–‡æœ¬'å½“å‰ç½‘ç»œå¯ç”¨'"}
            ],
            "max_tokens": 10
        }
        
        # å‘é€æµ‹è¯•è¯·æ±‚
        response = requests.post(
            test_url,
            headers=headers,
            json=test_data,
            timeout=10
        )
        
        if response.status_code == 200:
            return True, tr("Text model is available")
        else:
            return False, f"{tr('Text model is not available')}: HTTP {response.status_code}"
            
    except Exception as e:
        return False, f"{tr('Connection failed')}: {str(e)}"


def render_text_llm_settings(tr):
    """æ¸²æŸ“æ–‡æ¡ˆç”Ÿæˆæ¨¡å‹è®¾ç½®"""
    st.subheader(tr("Text Generation Model Settings"))

    # æ–‡æ¡ˆç”Ÿæˆæ¨¡å‹æä¾›å•†é€‰æ‹©
    text_providers = ['DeepSeek', 'OpenAI', 'Qwen', 'Moonshot', 'Gemini']
    saved_text_provider = config.app.get("text_llm_provider", "DeepSeek").lower()
    saved_provider_index = 0

    for i, provider in enumerate(text_providers):
        if provider.lower() == saved_text_provider:
            saved_provider_index = i
            break

    text_provider = st.selectbox(
        tr("Text Model Provider"),
        options=text_providers,
        index=saved_provider_index
    )
    text_provider = text_provider.lower()
    config.app["text_llm_provider"] = text_provider

    # è·å–å·²ä¿å­˜çš„æ–‡æœ¬æ¨¡å‹é…ç½®
    text_api_key = config.app.get(f"text_{text_provider}_api_key", "")
    text_base_url = config.app.get(f"text_{text_provider}_base_url", "")
    text_model_name = config.app.get(f"text_{text_provider}_model_name", "")

    # æ¸²æŸ“æ–‡æœ¬æ¨¡å‹é…ç½®è¾“å…¥æ¡†
    st_text_api_key = st.text_input(tr("Text API Key"), value=text_api_key, type="password")
    st_text_base_url = st.text_input(tr("Text Base URL"), value=text_base_url)
    st_text_model_name = st.text_input(tr("Text Model Name"), value=text_model_name)

    # æ·»åŠ æµ‹è¯•æŒ‰é’®
    if st.button(tr("Test Connection"), key="test_text_connection"):
        with st.spinner(tr("Testing connection...")):
            success, message = test_text_model_connection(
                api_key=st_text_api_key,
                base_url=st_text_base_url,
                model_name=st_text_model_name,
                provider=text_provider,
                tr=tr
            )
            
            if success:
                st.success(message)
            else:
                st.error(message)

    # ä¿å­˜æ–‡æœ¬æ¨¡å‹é…ç½®
    if st_text_api_key:
        config.app[f"text_{text_provider}_api_key"] = st_text_api_key
    if st_text_base_url:
        config.app[f"text_{text_provider}_base_url"] = st_text_base_url
    if st_text_model_name:
        config.app[f"text_{text_provider}_model_name"] = st_text_model_name

    # Cloudflare ç‰¹æ®Šé…ç½®
    if text_provider == 'cloudflare':
        st_account_id = st.text_input(
            tr("Account ID"),
            value=config.app.get(f"text_{text_provider}_account_id", "")
        )
        if st_account_id:
            config.app[f"text_{text_provider}_account_id"] = st_account_id
</file>

<file path="webui/components/merge_settings.py">
import os
import time
import math
import sys
import tempfile
import traceback
import shutil

import streamlit as st
from loguru import logger
from typing import List, Dict, Tuple
from dataclasses import dataclass
from streamlit.runtime.uploaded_file_manager import UploadedFile

from webui.utils.merge_video import merge_videos_and_subtitles
from app.utils.utils import video_dir, srt_dir
from app.services.subtitle import extract_audio_and_create_subtitle

# å®šä¹‰ä¸´æ—¶ç›®å½•è·¯å¾„
TEMP_MERGE_DIR = os.path.join("storage", "temp", "merge")

# ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
os.makedirs(TEMP_MERGE_DIR, exist_ok=True)


@dataclass
class VideoSubtitlePair:
    video_file: UploadedFile | None
    subtitle_file: str | None
    base_name: str
    order: int = 0


def save_uploaded_file(uploaded_file: UploadedFile, target_dir: str) -> str:
    """Save uploaded file to target directory and return the file path"""
    file_path = os.path.join(target_dir, uploaded_file.name)
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤å®ƒ
    if os.path.exists(file_path):
        os.remove(file_path)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getvalue())
    return file_path


def clean_temp_dir():
    """æ¸…ç©ºä¸´æ—¶ç›®å½•"""
    if os.path.exists(TEMP_MERGE_DIR):
        for file in os.listdir(TEMP_MERGE_DIR):
            file_path = os.path.join(TEMP_MERGE_DIR, file)
            try:
                if os.path.isfile(file_path):
                    os.unlink(file_path)
            except Exception as e:
                logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")


def group_files(files: List[UploadedFile]) -> Dict[str, VideoSubtitlePair]:
    """Group uploaded files by their base names"""
    pairs = {}
    order_counter = 0
    
    # é¦–å…ˆå¤„ç†æ‰€æœ‰è§†é¢‘æ–‡ä»¶
    for file in files:
        base_name = os.path.splitext(file.name)[0]
        ext = os.path.splitext(file.name)[1].lower()
        
        if ext == ".mp4":
            if base_name not in pairs:
                pairs[base_name] = VideoSubtitlePair(None, None, base_name, order_counter)
                order_counter += 1
            pairs[base_name].video_file = file
            # ä¿å­˜è§†é¢‘æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            video_path = save_uploaded_file(file, TEMP_MERGE_DIR)
    
    # ç„¶åå¤„ç†æ‰€æœ‰å­—å¹•æ–‡ä»¶
    for file in files:
        base_name = os.path.splitext(file.name)[0]
        ext = os.path.splitext(file.name)[1].lower()
        
        if ext == ".srt":
            # å³ä½¿æ²¡æœ‰å¯¹åº”è§†é¢‘ä¹Ÿä¿å­˜å­—å¹•æ–‡ä»¶
            subtitle_path = os.path.join(TEMP_MERGE_DIR, f"{base_name}.srt")
            save_uploaded_file(file, TEMP_MERGE_DIR)
            
            if base_name in pairs:  # å¦‚æœæœ‰å¯¹åº”çš„è§†é¢‘
                pairs[base_name].subtitle_file = subtitle_path
            
    return pairs


def render_merge_settings(tr):
    """Render the merge settings section"""
    with st.expander(tr("Video Subtitle Merge"), expanded=False):
        # ä¸Šä¼ æ–‡ä»¶åŒºåŸŸ
        uploaded_files = st.file_uploader(
            tr("Upload Video and Subtitle Files"),
            type=["mp4", "srt"],
            accept_multiple_files=True,
            key="merge_files"
        )
        
        if uploaded_files:
            all_pairs = group_files(uploaded_files)
            
            if all_pairs:
                st.write(tr("All Uploaded Files"))
                
                # åˆå§‹åŒ–æˆ–æ›´æ–°session stateä¸­çš„æ’åºä¿¡æ¯
                if 'file_orders' not in st.session_state:
                    st.session_state.file_orders = {
                        name: pair.order for name, pair in all_pairs.items()
                    }
                    st.session_state.needs_reorder = False
                
                # ç¡®ä¿æ‰€æœ‰æ–°æ–‡ä»¶éƒ½æœ‰æ’åºå€¼
                for name, pair in all_pairs.items():
                    if name not in st.session_state.file_orders:
                        st.session_state.file_orders[name] = pair.order
                
                # ç§»é™¤ä¸å­˜åœ¨çš„æ–‡ä»¶çš„æ’åºå€¼
                st.session_state.file_orders = {
                    k: v for k, v in st.session_state.file_orders.items() 
                    if k in all_pairs
                }
                
                # æŒ‰ç…§æ’åºå€¼å¯¹æ–‡ä»¶å¯¹è¿›è¡Œæ’åº
                sorted_pairs = sorted(
                    all_pairs.items(),
                    key=lambda x: st.session_state.file_orders[x[0]]
                )
                
                # è®¡ç®—éœ€è¦å¤šå°‘è¡Œæ¥æ˜¾ç¤ºæ‰€æœ‰è§†é¢‘ï¼ˆæ¯è¡Œ5ä¸ªï¼‰
                num_pairs = len(sorted_pairs)
                num_rows = (num_pairs + 4) // 5  # å‘ä¸Šå–æ•´,æ¯è¡Œ5ä¸ª
                
                # éå†æ¯ä¸€è¡Œ
                for row in range(num_rows):
                    # åˆ›å»º5åˆ—
                    cols = st.columns(5)
                    
                    # åœ¨è¿™ä¸€è¡Œä¸­å¡«å……è§†é¢‘ï¼ˆæœ€å¤š5ä¸ªï¼‰
                    for col_idx in range(5):
                        pair_idx = row * 5 + col_idx
                        if pair_idx < num_pairs:
                            base_name, pair = sorted_pairs[pair_idx]
                            with cols[col_idx]:
                                st.caption(base_name)
                                
                                # æ˜¾ç¤ºè§†é¢‘é¢„è§ˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                                video_path = os.path.join(TEMP_MERGE_DIR, f"{base_name}.mp4")
                                if os.path.exists(video_path):
                                    st.video(video_path)
                                else:
                                    st.warning(tr("Missing Video"))
                                
                                # æ˜¾ç¤ºå­—å¹•é¢„è§ˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                                subtitle_path = os.path.join(TEMP_MERGE_DIR, f"{base_name}.srt")
                                if os.path.exists(subtitle_path):
                                    with open(subtitle_path, 'r', encoding='utf-8') as f:
                                        subtitle_content = f.read()
                                        st.markdown(tr("Subtitle Preview"))
                                        st.text_area(
                                            "Subtitle Content",
                                            value=subtitle_content,
                                            height=100,  # å‡é«˜åº¦ä»¥é€‚åº”5åˆ—å¸ƒå±€
                                            label_visibility="collapsed",
                                            key=f"subtitle_preview_{base_name}"
                                        )
                                else:
                                    st.warning(tr("Missing Subtitle"))
                                    # å¦‚æœæœ‰è§†é¢‘ä½†æ²¡æœ‰å­—å¹•ï¼Œæ˜¾ç¤ºä¸€é”®è½¬å½•æŒ‰é’®
                                    if os.path.exists(video_path):
                                        if st.button(tr("One-Click Transcribe"), key=f"transcribe_{base_name}"):
                                            with st.spinner(tr("Transcribing...")):
                                                try:
                                                    # ç”Ÿæˆå­—å¹•æ–‡ä»¶
                                                    result = extract_audio_and_create_subtitle(video_path, subtitle_path)
                                                    if result:
                                                        # è¯»å–ç”Ÿæˆçš„å­—å¹•æ–‡ä»¶å†…å®¹å¹¶æ˜¾ç¤ºé¢„è§ˆ
                                                        with open(subtitle_path, 'r', encoding='utf-8') as f:
                                                            subtitle_content = f.read()
                                                            st.markdown(tr("Subtitle Preview"))
                                                            st.text_area(
                                                                "Subtitle Content",
                                                                value=subtitle_content,
                                                                height=150,
                                                                label_visibility="collapsed",
                                                                key=f"subtitle_preview_transcribed_{base_name}"
                                                            )
                                                            st.success(tr("Transcription Complete!"))
                                                            # æ›´æ–°pairçš„å­—å¹•æ–‡ä»¶è·¯å¾„
                                                            pair.subtitle_file = subtitle_path
                                                    else:
                                                        st.error(tr("Transcription Failed. Please try again."))
                                                except Exception as e:
                                                    error_message = str(e)
                                                    logger.error(traceback.format_exc())
                                                    if "rate limit exceeded" in error_message.lower():
                                                        st.error(tr("API rate limit exceeded. Please wait about an hour and try again."))
                                                    elif "resource_exhausted" in error_message.lower():
                                                        st.error(tr("Resources exhausted. Please try again later."))
                                                    else:
                                                        st.error(f"{tr('Transcription Failed')}: {str(e)}")
                                
                                # æ’åºè¾“å…¥æ¡†
                                order = st.number_input(
                                    tr("Order"),
                                    min_value=0,
                                    value=st.session_state.file_orders[base_name],
                                    key=f"order_{base_name}",
                                    on_change=lambda: setattr(st.session_state, 'needs_reorder', True)
                                )
                                if order != st.session_state.file_orders[base_name]:
                                    st.session_state.file_orders[base_name] = order
                                    st.session_state.needs_reorder = True
                
                # å¦‚æœéœ€è¦é‡æ–°æ’åºï¼Œé‡æ–°åŠ è½½é¡µé¢
                if st.session_state.needs_reorder:
                    st.session_state.needs_reorder = False
                    st.rerun()
                
                # æ‰¾å‡ºæœ‰å®Œæ•´è§†é¢‘å’Œå­—å¹•çš„æ–‡ä»¶å¯¹
                complete_pairs = {
                    k: v for k, v in all_pairs.items()
                    if os.path.exists(os.path.join(TEMP_MERGE_DIR, f"{k}.mp4")) and 
                    os.path.exists(os.path.join(TEMP_MERGE_DIR, f"{k}.srt"))
                }
                
                # åˆå¹¶æŒ‰é’®å’Œç»“æœæ˜¾ç¤º
                cols = st.columns([1, 2, 1])
                with cols[0]:
                    st.write(f"{tr('Mergeable Files')}: {len(complete_pairs)}")
                
                merge_videos_result = None
                
                with cols[1]:
                    if st.button(tr("Merge All Files"), type="primary", use_container_width=True):
                        try:
                            # è·å–æ’åºåçš„å®Œæ•´æ–‡ä»¶å¯¹
                            sorted_complete_pairs = sorted(
                                [(k, v) for k, v in complete_pairs.items()],
                                key=lambda x: st.session_state.file_orders[x[0]]
                            )
                            
                            video_paths = []
                            subtitle_paths = []
                            for base_name, _ in sorted_complete_pairs:
                                video_paths.append(os.path.join(TEMP_MERGE_DIR, f"{base_name}.mp4"))
                                subtitle_paths.append(os.path.join(TEMP_MERGE_DIR, f"{base_name}.srt"))
                            
                            # è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„
                            output_video = os.path.join(video_dir(), f"merged_video_{time.strftime('%M%S')}.mp4")
                            output_subtitle = os.path.join(srt_dir(), f"merged_subtitle_{time.strftime('%M%S')}.srt")
                            
                            with st.spinner(tr("Merging files...")):
                                # åˆå¹¶æ–‡ä»¶
                                merge_videos_and_subtitles(
                                    video_paths,
                                    subtitle_paths,
                                    output_video,
                                    output_subtitle
                                )
                                
                                success = True
                                error_msg = ""
                                
                                # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦æˆåŠŸç”Ÿæˆ
                                if not os.path.exists(output_video):
                                    success = False
                                    error_msg += tr("Failed to generate merged video. ")
                                if not os.path.exists(output_subtitle):
                                    success = False
                                    error_msg += tr("Failed to generate merged subtitle. ")
                                
                                if success:
                                    # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
                                    st.success(tr("Merge completed!"))
                                    merge_videos_result = (output_video, output_subtitle)
                                    # æ¸…ç†ä¸´æ—¶ç›®å½•
                                    clean_temp_dir()
                                else:
                                    st.error(error_msg)
                                    
                        except Exception as e:
                            error_message = str(e)
                            if "moviepy" in error_message.lower():
                                st.error(tr("Error processing video files. Please check if the videos are valid MP4 files."))
                            elif "pysrt" in error_message.lower():
                                st.error(tr("Error processing subtitle files. Please check if the subtitles are valid SRT files."))
                            else:
                                st.error(f"{tr('Error during merge')}: {error_message}")
                
                # åˆå¹¶ç»“æœé¢„è§ˆæ”¾åœ¨åˆå¹¶æŒ‰é’®ä¸‹æ–¹
                if merge_videos_result:
                    st.markdown(f"<h3 style='text-align: center'>{tr('Merge Result Preview')}</h3>", unsafe_allow_html=True)
                    # ä½¿ç”¨åˆ—å¸ƒå±€ä½¿è§†é¢‘å±…ä¸­
                    col1, col2, col3 = st.columns([1,2,1])
                    with col2:
                        st.video(merge_videos_result[0])
                        st.code(f"{tr('Video Path')}: {merge_videos_result[0]}")
                        st.code(f"{tr('Subtitle Path')}: {merge_videos_result[1]}")
            else:
                st.warning(tr("No Files Found"))
</file>

<file path="webui/components/review_settings.py">
import streamlit as st
import os
from loguru import logger


def render_review_panel(tr):
    """æ¸²æŸ“è§†é¢‘å®¡æŸ¥é¢æ¿"""
    with st.expander(tr("Video Check"), expanded=False):
        try:
            video_list = st.session_state.get('video_clip_json', [])
            subclip_videos = st.session_state.get('subclip_videos', {})
        except KeyError:
            video_list = []
            subclip_videos = {}

        # è®¡ç®—åˆ—æ•°å’Œè¡Œæ•°
        num_videos = len(video_list)
        cols_per_row = 3
        rows = (num_videos + cols_per_row - 1) // cols_per_row  # å‘ä¸Šå–æ•´è®¡ç®—è¡Œæ•°

        # ä½¿ç”¨å®¹å™¨å±•ç¤ºè§†é¢‘
        for row in range(rows):
            cols = st.columns(cols_per_row)
            for col in range(cols_per_row):
                index = row * cols_per_row + col
                if index < num_videos:
                    with cols[col]:
                        render_video_item(tr, video_list, subclip_videos, index)


def render_video_item(tr, video_list, subclip_videos, index):
    """æ¸²æŸ“å•ä¸ªè§†é¢‘é¡¹"""
    video_script = video_list[index]

    # æ˜¾ç¤ºæ—¶é—´æˆ³
    timestamp = video_script.get('timestamp', '')
    st.text_area(
        tr("Timestamp"),
        value=timestamp,
        height=70,
        disabled=True,
        key=f"timestamp_{index}"
    )

    # æ˜¾ç¤ºè§†é¢‘æ’­æ”¾å™¨
    video_path = subclip_videos.get(timestamp)
    if video_path and os.path.exists(video_path):
        try:
            st.video(video_path)
        except Exception as e:
            logger.error(f"åŠ è½½è§†é¢‘å¤±è´¥ {video_path}: {e}")
            st.error(f"æ— æ³•åŠ è½½è§†é¢‘: {os.path.basename(video_path)}")
    else:
        st.warning(tr("è§†é¢‘æ–‡ä»¶æœªæ‰¾åˆ°"))

    # æ˜¾ç¤ºç”»é¢æè¿°
    st.text_area(
        tr("Picture Description"),
        value=video_script.get('picture', ''),
        height=150,
        disabled=True,
        key=f"picture_{index}"
    )

    # æ˜¾ç¤ºæ—ç™½æ–‡æœ¬
    narration = st.text_area(
        tr("Narration"),
        value=video_script.get('narration', ''),
        height=150,
        key=f"narration_{index}"
    )
    # ä¿å­˜ä¿®æ”¹åçš„æ—ç™½æ–‡æœ¬
    if narration != video_script.get('narration', ''):
        video_script['narration'] = narration
        st.session_state['video_clip_json'] = video_list

    # æ˜¾ç¤ºå‰ªè¾‘æ¨¡å¼
    ost = st.selectbox(
        tr("Clip Mode"),
        options=range(0, 3),
        index=video_script.get('OST', 0),
        key=f"ost_{index}",
        help=tr("0: Keep the audio only, 1: Keep the original sound only, 2: Keep the original sound and audio")
    )
    # ä¿å­˜ä¿®æ”¹åçš„å‰ªè¾‘æ¨¡å¼
    if ost != video_script.get('OST', 0):
        video_script['OST'] = ost
        st.session_state['video_clip_json'] = video_list
</file>

<file path="webui/components/script_settings.py">
import os
import glob
import json
import time
import traceback
import streamlit as st
from loguru import logger

from app.config import config
from app.models.schema import VideoClipParams
from app.utils import utils, check_script
from webui.tools.generate_script_docu import generate_script_docu
from webui.tools.generate_script_short import generate_script_short


def render_script_panel(tr):
    """æ¸²æŸ“è„šæœ¬é…ç½®é¢æ¿"""
    with st.container(border=True):
        st.write(tr("Video Script Configuration"))
        params = VideoClipParams()

        # æ¸²æŸ“è„šæœ¬æ–‡ä»¶é€‰æ‹©
        render_script_file(tr, params)

        # æ¸²æŸ“è§†é¢‘æ–‡ä»¶é€‰æ‹©
        render_video_file(tr, params)

        # æ¸²æŸ“è§†é¢‘ä¸»é¢˜å’Œæç¤ºè¯
        render_video_details(tr)

        # æ¸²æŸ“è„šæœ¬æ“ä½œæŒ‰é’®
        render_script_buttons(tr, params)


def render_script_file(tr, params):
    """æ¸²æŸ“è„šæœ¬æ–‡ä»¶é€‰æ‹©"""
    script_list = [
        (tr("None"), ""), 
        (tr("Auto Generate"), "auto"), 
        (tr("Short Generate"), "short"),
        (tr("Upload Script"), "upload_script")  # æ–°å¢ä¸Šä¼ è„šæœ¬é€‰é¡¹
    ]

    # è·å–å·²æœ‰è„šæœ¬æ–‡ä»¶
    suffix = "*.json"
    script_dir = utils.script_dir()
    files = glob.glob(os.path.join(script_dir, suffix))
    file_list = []

    for file in files:
        file_list.append({
            "name": os.path.basename(file),
            "file": file,
            "ctime": os.path.getctime(file)
        })

    file_list.sort(key=lambda x: x["ctime"], reverse=True)
    for file in file_list:
        display_name = file['file'].replace(config.root_dir, "")
        script_list.append((display_name, file['file']))

    # æ‰¾åˆ°ä¿å­˜çš„è„šæœ¬æ–‡ä»¶åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•
    saved_script_path = st.session_state.get('video_clip_json_path', '')
    selected_index = 0
    for i, (_, path) in enumerate(script_list):
        if path == saved_script_path:
            selected_index = i
            break

    selected_script_index = st.selectbox(
        tr("Script Files"),
        index=selected_index,
        options=range(len(script_list)),
        format_func=lambda x: script_list[x][0]
    )

    script_path = script_list[selected_script_index][1]
    st.session_state['video_clip_json_path'] = script_path
    params.video_clip_json_path = script_path

    # å¤„ç†è„šæœ¬ä¸Šä¼ 
    if script_path == "upload_script":
        uploaded_file = st.file_uploader(
            tr("Upload Script File"),
            type=["json"],
            accept_multiple_files=False,
        )

        if uploaded_file is not None:
            try:
                # è¯»å–ä¸Šä¼ çš„JSONå†…å®¹å¹¶éªŒè¯æ ¼å¼
                script_content = uploaded_file.read().decode('utf-8')
                json_data = json.loads(script_content)
                
                # ä¿å­˜åˆ°è„šæœ¬ç›®å½•
                script_file_path = os.path.join(script_dir, uploaded_file.name)
                file_name, file_extension = os.path.splitext(uploaded_file.name)
                
                # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨,æ·»åŠ æ—¶é—´æˆ³
                if os.path.exists(script_file_path):
                    timestamp = time.strftime("%Y%m%d%H%M%S")
                    file_name_with_timestamp = f"{file_name}_{timestamp}"
                    script_file_path = os.path.join(script_dir, file_name_with_timestamp + file_extension)

                # å†™å…¥æ–‡ä»¶
                with open(script_file_path, "w", encoding='utf-8') as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)
                
                # æ›´æ–°çŠ¶æ€
                st.success(tr("Script Uploaded Successfully"))
                st.session_state['video_clip_json_path'] = script_file_path
                params.video_clip_json_path = script_file_path
                time.sleep(1)
                st.rerun()
                
            except json.JSONDecodeError:
                st.error(tr("Invalid JSON format"))
            except Exception as e:
                st.error(f"{tr('Upload failed')}: {str(e)}")


def render_video_file(tr, params):
    """æ¸²æŸ“è§†é¢‘æ–‡ä»¶é€‰æ‹©"""
    video_list = [(tr("None"), ""), (tr("Upload Local Files"), "upload_local")]

    # è·å–å·²æœ‰è§†é¢‘æ–‡ä»¶
    for suffix in ["*.mp4", "*.mov", "*.avi", "*.mkv"]:
        video_files = glob.glob(os.path.join(utils.video_dir(), suffix))
        for file in video_files:
            display_name = file.replace(config.root_dir, "")
            video_list.append((display_name, file))

    selected_video_index = st.selectbox(
        tr("Video File"),
        index=0,
        options=range(len(video_list)),
        format_func=lambda x: video_list[x][0]
    )

    video_path = video_list[selected_video_index][1]
    st.session_state['video_origin_path'] = video_path
    params.video_origin_path = video_path

    if video_path == "upload_local":
        uploaded_file = st.file_uploader(
            tr("Upload Local Files"),
            type=["mp4", "mov", "avi", "flv", "mkv"],
            accept_multiple_files=False,
        )

        if uploaded_file is not None:
            video_file_path = os.path.join(utils.video_dir(), uploaded_file.name)
            file_name, file_extension = os.path.splitext(uploaded_file.name)

            if os.path.exists(video_file_path):
                timestamp = time.strftime("%Y%m%d%H%M%S")
                file_name_with_timestamp = f"{file_name}_{timestamp}"
                video_file_path = os.path.join(utils.video_dir(), file_name_with_timestamp + file_extension)

            with open(video_file_path, "wb") as f:
                f.write(uploaded_file.read())
                st.success(tr("File Uploaded Successfully"))
                st.session_state['video_origin_path'] = video_file_path
                params.video_origin_path = video_file_path
                time.sleep(1)
                st.rerun()


def render_video_details(tr):
    """æ¸²æŸ“è§†é¢‘ä¸»é¢˜å’Œæç¤ºè¯"""
    video_theme = st.text_input(tr("Video Theme"))
    custom_prompt = st.text_area(
        tr("Generation Prompt"),
        value=st.session_state.get('video_plot', ''),
        help=tr("Custom prompt for LLM, leave empty to use default prompt"),
        height=180
    )
    st.session_state['video_theme'] = video_theme
    st.session_state['custom_prompt'] = custom_prompt
    return video_theme, custom_prompt


def render_script_buttons(tr, params):
    """æ¸²æŸ“è„šæœ¬æ“ä½œæŒ‰é’®"""
    # æ–°å¢ä¸‰ä¸ªè¾“å…¥æ¡†ï¼Œæ”¾åœ¨åŒä¸€è¡Œ
    input_cols = st.columns(3)
    
    with input_cols[0]:
        skip_seconds = st.number_input(
            "skip_seconds",
            min_value=0,
            value=st.session_state.get('skip_seconds', config.frames.get('skip_seconds', 0)),
            help=tr("Skip the first few seconds"),
            key="skip_seconds_input"
        )
        st.session_state['skip_seconds'] = skip_seconds
        
    with input_cols[1]:
        threshold = st.number_input(
            "threshold",
            min_value=0,
            value=st.session_state.get('threshold', config.frames.get('threshold', 30)),
            help=tr("Difference threshold"),
            key="threshold_input"
        )
        st.session_state['threshold'] = threshold
        
    with input_cols[2]:
        vision_batch_size = st.number_input(
            "vision_batch_size",
            min_value=1,
            max_value=20,
            value=st.session_state.get('vision_batch_size', config.frames.get('vision_batch_size', 5)),
            help=tr("Vision processing batch size"),
            key="vision_batch_size_input"
        )
        st.session_state['vision_batch_size'] = vision_batch_size

    # ç”Ÿæˆ/åŠ è½½æŒ‰é’®
    script_path = st.session_state.get('video_clip_json_path', '')
    if script_path == "auto":
        button_name = tr("Generate Video Script")
    elif script_path == "short":
        button_name = tr("Generate Short Video Script")
    elif script_path.endswith("json"):
        button_name = tr("Load Video Script")
    else:
        button_name = tr("Please Select Script File")

    if st.button(button_name, key="script_action", disabled=not script_path):
        if script_path == "auto":
            generate_script_docu(tr, params)
        elif script_path == "short":
            generate_script_short(tr, params)
        else:
            load_script(tr, script_path)

    # è§†é¢‘è„šæœ¬ç¼–è¾‘åŒº
    video_clip_json_details = st.text_area(
        tr("Video Script"),
        value=json.dumps(st.session_state.get('video_clip_json', []), indent=2, ensure_ascii=False),
        height=180
    )

    # æ“ä½œæŒ‰é’®è¡Œ
    button_cols = st.columns(3)
    with button_cols[0]:
        if st.button(tr("Check Format"), key="check_format", use_container_width=True):
            check_script_format(tr, video_clip_json_details)

    with button_cols[1]:
        if st.button(tr("Save Script"), key="save_script", use_container_width=True):
            save_script(tr, video_clip_json_details)

    with button_cols[2]:
        script_valid = st.session_state.get('script_format_valid', False)
        if st.button(tr("Crop Video"), key="crop_video", disabled=not script_valid, use_container_width=True):
            crop_video(tr, params)


def check_script_format(tr, script_content):
    """æ£€æŸ¥è„šæœ¬æ ¼å¼"""
    try:
        result = check_script.check_format(script_content)
        if result.get('success'):
            st.success(tr("Script format check passed"))
            st.session_state['script_format_valid'] = True
        else:
            st.error(f"{tr('Script format check failed')}: {result.get('message')}")
            st.session_state['script_format_valid'] = False
    except Exception as e:
        st.error(f"{tr('Script format check error')}: {str(e)}")
        st.session_state['script_format_valid'] = False


def load_script(tr, script_path):
    """åŠ è½½è„šæœ¬æ–‡ä»¶"""
    try:
        with open(script_path, 'r', encoding='utf-8') as f:
            script = f.read()
            script = utils.clean_model_output(script)
            st.session_state['video_clip_json'] = json.loads(script)
            st.success(tr("Script loaded successfully"))
            st.rerun()
    except Exception as e:
        logger.error(f"åŠ è½½è„šæœ¬æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯\n{traceback.format_exc()}")
        st.error(f"{tr('Failed to load script')}: {str(e)}")


def save_script(tr, video_clip_json_details):
    """ä¿å­˜è§†é¢‘è„šæœ¬"""
    if not video_clip_json_details:
        st.error(tr("è¯·è¾“å…¥è§†é¢‘è„šæœ¬"))
        st.stop()

    with st.spinner(tr("Save Script")):
        script_dir = utils.script_dir()
        timestamp = time.strftime("%Y-%m%d-%H%M%S")
        save_path = os.path.join(script_dir, f"{timestamp}.json")

        try:
            data = json.loads(video_clip_json_details)
            with open(save_path, 'w', encoding='utf-8') as file:
                json.dump(data, file, ensure_ascii=False, indent=4)
                st.session_state['video_clip_json'] = data
                st.session_state['video_clip_json_path'] = save_path

                # æ›´æ–°é…ç½®
                config.app["video_clip_json_path"] = save_path

                # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
                st.success(tr("Script saved successfully"))

                # å¼ºåˆ¶é‡æ–°åŠ è½½é¡µé¢æ›´æ–°é€‰æ‹©æ¡†
                time.sleep(0.5)  # ç»™ä¸€ç‚¹æ—¶é—´è®©ç”¨æˆ·çœ‹åˆ°æˆåŠŸæ¶ˆæ¯
                st.rerun()

        except Exception as err:
            st.error(f"{tr('Failed to save script')}: {str(err)}")
            st.stop()


def crop_video(tr, params):
    """è£å‰ªè§†é¢‘"""
    progress_bar = st.progress(0)
    status_text = st.empty()

    def update_progress(progress):
        progress_bar.progress(progress)
        status_text.text(f"å‰ªè¾‘è¿›åº¦: {progress}%")

    try:
        utils.cut_video(params, update_progress)
        time.sleep(0.5)
        progress_bar.progress(100)
        status_text.text("å‰ªå®Œæˆï¼")
        st.success("è§†é¢‘å‰ªè¾‘æˆåŠŸå®Œæˆï¼")
    except Exception as e:
        st.error(f"å‰ªè¾‘è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
    finally:
        time.sleep(2)
        progress_bar.empty()
        status_text.empty()


def get_script_params():
    """è·å–è„šæœ¬å‚æ•°"""
    return {
        'video_language': st.session_state.get('video_language', ''),
        'video_clip_json_path': st.session_state.get('video_clip_json_path', ''),
        'video_origin_path': st.session_state.get('video_origin_path', ''),
        'video_name': st.session_state.get('video_name', ''),
        'video_plot': st.session_state.get('video_plot', '')
    }
</file>

<file path="webui/components/subtitle_settings.py">
import streamlit as st
from app.config import config
from webui.utils.cache import get_fonts_cache
import os


def render_subtitle_panel(tr):
    """æ¸²æŸ“å­—å¹•è®¾ç½®é¢æ¿"""
    with st.container(border=True):
        st.write(tr("Subtitle Settings"))

        # å¯ç”¨å­—å¹•é€‰é¡¹
        enable_subtitles = st.checkbox(tr("Enable Subtitles"), value=True)
        st.session_state['subtitle_enabled'] = enable_subtitles

        if enable_subtitles:
            render_font_settings(tr)
            render_position_settings(tr)
            render_style_settings(tr)


def render_font_settings(tr):
    """æ¸²æŸ“å­—ä½“è®¾ç½®"""
    # è·å–å­—ä½“åˆ—è¡¨
    font_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "resource", "fonts")
    font_names = get_fonts_cache(font_dir)

    # è·å–ä¿å­˜çš„å­—ä½“è®¾ç½®
    saved_font_name = config.ui.get("font_name", "")
    saved_font_name_index = 0
    if saved_font_name in font_names:
        saved_font_name_index = font_names.index(saved_font_name)

    # å­—ä½“é€‰æ‹©
    font_name = st.selectbox(
        tr("Font"),
        options=font_names,
        index=saved_font_name_index
    )
    config.ui["font_name"] = font_name
    st.session_state['font_name'] = font_name

    # å­—ä½“å¤§å° å’Œ å­—å¹•å¤§å°
    font_cols = st.columns([0.3, 0.7])
    with font_cols[0]:
        saved_text_fore_color = config.ui.get("text_fore_color", "#FFFFFF")
        text_fore_color = st.color_picker(
            tr("Font Color"),
            saved_text_fore_color
        )
        config.ui["text_fore_color"] = text_fore_color
        st.session_state['text_fore_color'] = text_fore_color

    with font_cols[1]:
        saved_font_size = config.ui.get("font_size", 60)
        font_size = st.slider(
            tr("Font Size"),
            min_value=20,
            max_value=100,
            value=saved_font_size
        )
        config.ui["font_size"] = font_size
        st.session_state['font_size'] = font_size


def render_position_settings(tr):
    """æ¸²æŸ“ä½ç½®è®¾ç½®"""
    subtitle_positions = [
        (tr("Top"), "top"),
        (tr("Center"), "center"),
        (tr("Bottom"), "bottom"),
        (tr("Custom"), "custom"),
    ]

    selected_index = st.selectbox(
        tr("Position"),
        index=2,
        options=range(len(subtitle_positions)),
        format_func=lambda x: subtitle_positions[x][0],
    )

    subtitle_position = subtitle_positions[selected_index][1]
    st.session_state['subtitle_position'] = subtitle_position

    # è‡ªå®šä¹‰ä½ç½®å¤„ç†
    if subtitle_position == "custom":
        custom_position = st.text_input(
            tr("Custom Position (% from top)"),
            value="70.0"
        )
        try:
            custom_position_value = float(custom_position)
            if custom_position_value < 0 or custom_position_value > 100:
                st.error(tr("Please enter a value between 0 and 100"))
            else:
                st.session_state['custom_position'] = custom_position_value
        except ValueError:
            st.error(tr("Please enter a valid number"))


def render_style_settings(tr):
    """æ¸²æŸ“æ ·å¼è®¾ç½®"""
    stroke_cols = st.columns([0.3, 0.7])

    with stroke_cols[0]:
        stroke_color = st.color_picker(
            tr("Stroke Color"),
            value="#000000"
        )
        st.session_state['stroke_color'] = stroke_color

    with stroke_cols[1]:
        stroke_width = st.slider(
            tr("Stroke Width"),
            min_value=0.0,
            max_value=10.0,
            value=1.0,
            step=0.01
        )
        st.session_state['stroke_width'] = stroke_width


def get_subtitle_params():
    """è·å–å­—å¹•å‚æ•°"""
    return {
        'subtitle_enabled': st.session_state.get('subtitle_enabled', True),
        'font_name': st.session_state.get('font_name', ''),
        'font_size': st.session_state.get('font_size', 60),
        'text_fore_color': st.session_state.get('text_fore_color', '#FFFFFF'),
        'position': st.session_state.get('subtitle_position', 'bottom'),
        'custom_position': st.session_state.get('custom_position', 70.0),
        'stroke_color': st.session_state.get('stroke_color', '#000000'),
        'stroke_width': st.session_state.get('stroke_width', 1.5),
    }
</file>

<file path="webui/components/system_settings.py">
import streamlit as st
import os
import shutil
from loguru import logger

from app.utils.utils import storage_dir


def clear_directory(dir_path, tr):
    """æ¸…ç†æŒ‡å®šç›®å½•"""
    if os.path.exists(dir_path):
        try:
            for item in os.listdir(dir_path):
                item_path = os.path.join(dir_path, item)
                try:
                    if os.path.isfile(item_path):
                        os.unlink(item_path)
                    elif os.path.isdir(item_path):
                        shutil.rmtree(item_path)
                except Exception as e:
                    logger.error(f"Failed to delete {item_path}: {e}")
            st.success(tr("Directory cleared"))
            logger.info(f"Cleared directory: {dir_path}")
        except Exception as e:
            st.error(f"{tr('Failed to clear directory')}: {str(e)}")
            logger.error(f"Failed to clear directory {dir_path}: {e}")
    else:
        st.warning(tr("Directory does not exist"))

def render_system_panel(tr):
    """æ¸²æŸ“ç³»ç»Ÿè®¾ç½®é¢æ¿"""
    with st.expander(tr("System settings"), expanded=False):
        col1, col2, col3 = st.columns(3)
                
        with col1:
            if st.button(tr("Clear frames"), use_container_width=True):
                clear_directory(os.path.join(storage_dir(), "temp/keyframes"), tr)
                
        with col2:
            if st.button(tr("Clear clip videos"), use_container_width=True):
                clear_directory(os.path.join(storage_dir(), "temp/clip_video"), tr)
                
        with col3:
            if st.button(tr("Clear tasks"), use_container_width=True):
                clear_directory(os.path.join(storage_dir(), "tasks"), tr)
</file>

<file path="webui/components/video_settings.py">
import streamlit as st
from app.models.schema import VideoClipParams, VideoAspect


def render_video_panel(tr):
    """æ¸²æŸ“è§†é¢‘é…ç½®é¢æ¿"""
    with st.container(border=True):
        st.write(tr("Video Settings"))
        params = VideoClipParams()
        render_video_config(tr, params)


def render_video_config(tr, params):
    """æ¸²æŸ“è§†é¢‘é…ç½®"""
    # è§†é¢‘æ¯”ä¾‹
    video_aspect_ratios = [
        (tr("Portrait"), VideoAspect.portrait.value),
        (tr("Landscape"), VideoAspect.landscape.value),
    ]
    selected_index = st.selectbox(
        tr("Video Ratio"),
        options=range(len(video_aspect_ratios)),
        format_func=lambda x: video_aspect_ratios[x][0],
    )
    params.video_aspect = VideoAspect(video_aspect_ratios[selected_index][1])
    st.session_state['video_aspect'] = params.video_aspect.value

    # è§†é¢‘ç”»è´¨
    video_qualities = [
        ("4K (2160p)", "2160p"),
        ("2K (1440p)", "1440p"),
        ("Full HD (1080p)", "1080p"),
        ("HD (720p)", "720p"),
        ("SD (480p)", "480p"),
    ]
    quality_index = st.selectbox(
        tr("Video Quality"),
        options=range(len(video_qualities)),
        format_func=lambda x: video_qualities[x][0],
        index=2  # é»˜è®¤é€‰æ‹© 1080p
    )
    st.session_state['video_quality'] = video_qualities[quality_index][1]

    # åŸå£°éŸ³é‡
    params.original_volume = st.slider(
        tr("Original Volume"),
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        step=0.01,
        help=tr("Adjust the volume of the original audio")
    )
    st.session_state['original_volume'] = params.original_volume


def get_video_params():
    """è·å–è§†é¢‘å‚æ•°"""
    return {
        'video_aspect': st.session_state.get('video_aspect', VideoAspect.portrait.value),
        'video_quality': st.session_state.get('video_quality', '1080p'),
        'original_volume': st.session_state.get('original_volume', 0.7)
    }
</file>

<file path="webui/config/settings.py">
import os
import tomli
from loguru import logger
from typing import Dict, Any, Optional
from dataclasses import dataclass

@dataclass
class WebUIConfig:
    """WebUIé…ç½®ç±»"""
    # UIé…ç½®
    ui: Dict[str, Any] = None
    # ä»£ç†é…ç½®
    proxy: Dict[str, str] = None
    # åº”ç”¨é…ç½®
    app: Dict[str, Any] = None
    # Azureé…ç½®
    azure: Dict[str, str] = None
    # é¡¹ç›®ç‰ˆæœ¬
    project_version: str = "0.1.0"
    # é¡¹ç›®æ ¹ç›®å½•
    root_dir: str = None
    # Gemini API Key
    gemini_api_key: str = ""
    # æ¯æ‰¹å¤„ç†çš„å›¾ç‰‡æ•°é‡
    vision_batch_size: int = 5
    # æç¤ºè¯
    vision_prompt: str = """..."""
    # Narrato API é…ç½®
    narrato_api_url: str = "http://127.0.0.1:8000/api/v1/video/analyze"
    narrato_api_key: str = ""
    narrato_batch_size: int = 10
    narrato_vision_model: str = "gemini-1.5-flash"
    narrato_llm_model: str = "qwen-plus"
    
    def __post_init__(self):
        """åˆå§‹åŒ–é»˜è®¤å€¼"""
        self.ui = self.ui or {}
        self.proxy = self.proxy or {}
        self.app = self.app or {}
        self.azure = self.azure or {}
        self.root_dir = self.root_dir or os.path.dirname(os.path.dirname(os.path.dirname(__file__)))

def load_config(config_path: Optional[str] = None) -> WebUIConfig:
    """åŠ è½½é…ç½®æ–‡ä»¶
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    Returns:
        WebUIConfig: é…ç½®å¯¹è±¡
    """
    try:
        if config_path is None:
            config_path = os.path.join(
                os.path.dirname(os.path.dirname(__file__)),
                ".streamlit",
                "webui.toml"
            )
        
        # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¤ºä¾‹é…ç½®
        if not os.path.exists(config_path):
            example_config = os.path.join(
                os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                "config.example.toml"
            )
            if os.path.exists(example_config):
                config_path = example_config
            else:
                logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
                return WebUIConfig()
        
        # è¯»å–é…ç½®æ–‡ä»¶
        with open(config_path, "rb") as f:
            config_dict = tomli.load(f)
            
        # åˆ›å»ºé…ç½®å¯¹è±¡
        config = WebUIConfig(
            ui=config_dict.get("ui", {}),
            proxy=config_dict.get("proxy", {}),
            app=config_dict.get("app", {}),
            azure=config_dict.get("azure", {}),
            project_version=config_dict.get("project_version", "0.1.0")
        )
        
        return config
    
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return WebUIConfig()

def save_config(config: WebUIConfig, config_path: Optional[str] = None) -> bool:
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    Args:
        config: é…ç½®å¯¹è±¡
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    Returns:
        bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    try:
        if config_path is None:
            config_path = os.path.join(
                os.path.dirname(os.path.dirname(__file__)),
                ".streamlit",
                "webui.toml"
            )
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(config_path), exist_ok=True)
        
        # è½¬æ¢ä¸ºå­—å…¸
        config_dict = {
            "ui": config.ui,
            "proxy": config.proxy,
            "app": config.app,
            "azure": config.azure,
            "project_version": config.project_version
        }
        
        # ä¿å­˜é…ç½®
        with open(config_path, "w", encoding="utf-8") as f:
            import tomli_w
            tomli_w.dump(config_dict, f)
        
        return True
    
    except Exception as e:
        logger.error(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return False

def get_config() -> WebUIConfig:
    """è·å–å…¨å±€é…ç½®å¯¹è±¡
    Returns:
        WebUIConfig: é…ç½®å¯¹è±¡
    """
    if not hasattr(get_config, "_config"):
        get_config._config = load_config()
    return get_config._config

def update_config(config_dict: Dict[str, Any]) -> bool:
    """æ›´æ–°é…ç½®
    Args:
        config_dict: é…ç½®å­—å…¸
    Returns:
        bool: æ˜¯å¦æ›´æ–°æˆåŠŸ
    """
    try:
        config = get_config()
        
        # æ›´æ–°é…ç½®
        if "ui" in config_dict:
            config.ui.update(config_dict["ui"])
        if "proxy" in config_dict:
            config.proxy.update(config_dict["proxy"])
        if "app" in config_dict:
            config.app.update(config_dict["app"])
        if "azure" in config_dict:
            config.azure.update(config_dict["azure"])
        if "project_version" in config_dict:
            config.project_version = config_dict["project_version"]
        
        # ä¿å­˜é…ç½®
        return save_config(config)
    
    except Exception as e:
        logger.error(f"æ›´æ–°é…ç½®å¤±è´¥: {e}")
        return False

# å¯¼å‡ºå…¨å±€é…ç½®å¯¹è±¡
config = get_config()
</file>

<file path="webui/i18n/__init__.py">
# ç©ºæ–‡ä»¶ï¼Œç”¨äºæ ‡è®°åŒ…
</file>

<file path="webui/i18n/en.json">
{
  "Language": "English",
  "Translation": {
    "Video Script Configuration": "**Video Script Configuration**",
    "Video Script Generate": "Generate Video Script",
    "Video Subject": "Video Subject (Given a keyword, :red[AI auto-generates] video script)",
    "Script Language": "Language of the generated video script (Usually, AI automatically outputs according to the language of the input subject)",
    "Script Files": "Script Files",
    "Generate Video Script and Keywords": "Click to use AI to generate **Video Script** and **Video Keywords** based on the **subject**",
    "Auto Detect": "Auto Detect",
    "Auto Generate": "Auto Generate",
    "Video Script": "Video Script (:blue[â‘ Optional, use AI to generate â‘¡Proper punctuation helps in generating subtitles])",
    "Save Script": "Save Script",
    "Crop Video": "Crop Video",
    "Video File": "Video File (:blue[1ï¸âƒ£Supports uploading video files (limit 2G) 2ï¸âƒ£For large files, it is recommended to directly import them into the ./resource/videos directory])",
    "Plot Description": "Plot Description (:blue[Can be obtained from https://www.tvmao.com/])",
    "Generate Video Keywords": "Click to use AI to generate **Video Keywords** based on the **script**",
    "Please Enter the Video Subject": "Please enter the video script first",
    "Generating Video Script and Keywords": "AI is generating the video script and keywords...",
    "Generating Video Keywords": "AI is generating the video keywords...",
    "Video Keywords": "Video Keywords (:blue[Long videos work better in conjunction with plot descriptions.])",
    "Video Settings": "**Video Settings**",
    "Video Concat Mode": "Video Concatenation Mode",
    "Random": "Random Concatenation (Recommended)",
    "Sequential": "Sequential Concatenation",
    "Video Ratio": "Video Ratio",
    "Portrait": "Portrait 9:16 (TikTok Video)",
    "Landscape": "Landscape 16:9 (Xigua Video)",
    "Clip Duration": "Maximum Clip Duration (Seconds) (**Not the total length of the video**, refers to the length of each **composite segment**)",
    "Number of Videos Generated Simultaneously": "Number of Videos Generated Simultaneously",
    "Audio Settings": "**Audio Settings**",
    "Speech Synthesis": "Speech Synthesis Voice (:red[**Keep consistent with the script language**. Note: V2 version performs better, but requires an API KEY])",
    "Speech Region": "Service Region (:red[Required, [Click to Get](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Key": "API Key (:red[Required, either Key 1 or Key 2 is acceptable [Click to Get](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Volume": "Speech Volume (1.0 represents 100%)",
    "Speech Rate": "Speech Rate (1.0 represents 1x speed)",
    "Male": "Male",
    "Female": "Female",
    "Background Music": "Background Music",
    "No Background Music": "No Background Music",
    "Random Background Music": "Random Background Music",
    "Custom Background Music": "Custom Background Music",
    "Custom Background Music File": "Please enter the file path of the custom background music",
    "Background Music Volume": "Background Music Volume (0.2 represents 20%, background sound should not be too loud)",
    "Subtitle Settings": "**Subtitle Settings**",
    "Enable Subtitles": "Enable Subtitles (If unchecked, the following settings will not take effect)",
    "Font": "Subtitle Font",
    "Position": "Subtitle Position",
    "Top": "Top",
    "Center": "Center",
    "Bottom": "Bottom (Recommended)",
    "Custom": "Custom Position (70, represents 70% from the top)",
    "Font Size": "Subtitle Size",
    "Font Color": "Subtitle Color",
    "Stroke Color": "Stroke Color",
    "Stroke Width": "Stroke Width",
    "Generate Video": "Generate Video",
    "Video Script and Subject Cannot Both Be Empty": "Video Subject and Video Script cannot both be empty",
    "Generating Video": "Generating video, please wait...",
    "Start Generating Video": "Start Generating Video",
    "Video Generation Completed": "Video Generation Completed",
    "Video Generation Failed": "Video Generation Failed",
    "You can download the generated video from the following links": "You can download the generated video from the following links",
    "Basic Settings": "**Basic Settings** (:blue[Click to expand])",
    "Language": "Interface Language",
    "Pexels API Key": "Pexels API Key ([Click to Get](https://www.pexels.com/api/)) :red[Recommended]",
    "Pixabay API Key": "Pixabay API Key ([Click to Get](https://pixabay.com/api/docs/#api_search_videos)) :red[Optional, if Pexels is unavailable, then choose Pixabay]",
    "LLM Provider": "LLM Provider",
    "API Key": "API Key (:red[Required, must be applied from the LLM provider's backend])",
    "Base Url": "Base Url (Optional)",
    "Account ID": "Account ID (Obtained from the URL of the Cloudflare dashboard)",
    "Model Name": "Model Name (:blue[Confirm the authorized model name from the LLM provider's backend])",
    "Please Enter the LLM API Key": "Please enter the **LLM API Key**",
    "Please Enter the Pexels API Key": "Please enter the **Pexels API Key**",
    "Please Enter the Pixabay API Key": "Please enter the **Pixabay API Key**",
    "Get Help": "One-stop AI video commentary + automated editing tool\uD83C\uDF89\uD83C\uDF89\uD83C\uDF89\n\nFor any questions or suggestions, you can join the **community channel** for help or discussion: https://github.com/linyqh/NarratoAI/wiki",
    "Video Source": "Video Source",
    "TikTok": "TikTok (Support is coming soon)",
    "Bilibili": "Bilibili (Support is coming soon)",
    "Xiaohongshu": "Xiaohongshu (Support is coming soon)",
    "Local file": "Local file",
    "Play Voice": "Play Synthesized Voice",
    "Voice Example": "This is a sample text for testing voice synthesis",
    "Synthesizing Voice": "Synthesizing voice, please wait...",
    "TTS Provider": "TTS Provider",
    "Hide Log": "Hide Log",
    "Upload Local Files": "Upload Local Files",
    "File Uploaded Successfully": "File Uploaded Successfully"
  }
}
</file>

<file path="webui/i18n/zh.json">
{
  "Language": "ç®€ä½“ä¸­æ–‡",
  "Translation": {
    "Video Script Configuration": "**è§†é¢‘è„šæœ¬é…ç½®**",
    "Generate Video Script": "AIç”Ÿæˆç”»é¢è§£è¯´è„šæœ¬",
    "Video Subject": "è§†é¢‘ä¸»é¢˜ï¼ˆç»™å®šä¸€ä¸ªå…³é”®è¯ï¼Œ:red[AIè‡ªåŠ¨ç”Ÿæˆ]è§†é¢‘æ–‡æ¡ˆï¼‰",
    "Script Language": "ç”Ÿæˆè§†é¢‘è„šæœ¬çš„è¯­è¨€ï¼ˆä¸€èˆ¬æƒ…å†µAIä¼šè‡ªåŠ¨æ ¹æ®ä½ è¾“å…¥çš„ä¸»é¢˜è¯­è¨€è¾“å‡ºï¼‰",
    "Script Files": "è„šæœ¬æ–‡ä»¶",
    "Generate Video Script and Keywords": "ç‚¹å‡»ä½¿ç”¨AIæ ¹æ®**ä¸»é¢˜**ç”Ÿæˆ ã€è§†é¢‘æ–‡æ¡ˆã€‘ å’Œ ã€è§†é¢‘å…³é”®è¯ã€‘",
    "Auto Detect": "è‡ªåŠ¨æ£€æµ‹",
    "Video Theme": "è§†é¢‘ä¸»é¢˜",
    "Generation Prompt": "è‡ªå®šä¹‰æç¤ºè¯",
    "Save Script": "ä¿å­˜è„šæœ¬",
    "Crop Video": "è£å‰ªè§†é¢‘",
    "Video File": "è§†é¢‘æ–‡ä»¶ï¼ˆ:blue[1ï¸âƒ£æ”¯æŒä¸Šä¼ è§†é¢‘æ–‡ä»¶(é™åˆ¶2G) 2ï¸âƒ£å¤§æ–‡ä»¶å»ºè®®ç›´æ¥å¯¼å…¥ ./resource/videos ç›®å½•]ï¼‰",
    "Plot Description": "å‰§æƒ…æè¿° (:blue[å¯ä» https://www.tvmao.com/ è·å–])",
    "Generate Video Keywords": "ç‚¹å‡»ä½¿ç”¨AIæ ¹æ®**æ–‡æ¡ˆ**ç”Ÿæˆã€è§†é¢‘å…³é”®ã€‘",
    "Please Enter the Video Subject": "è¯·å…ˆå¡«å†™è§†é¢‘æ–‡æ¡ˆ",
    "Generating Video Script and Keywords": "AIæ­£åœ¨ç”Ÿæˆè§†é¢‘æ–‡æ¡ˆå’Œå…³é”®è¯...",
    "Generating Video Keywords": "AIæ­£åœ¨ç”Ÿæˆè§†é¢‘å…³é”®è¯...",
    "Video Keywords": "è§†é¢‘å…³é”®è¯ï¼ˆ:blue[å¯¹äºé•¿è§†é¢‘é…åˆå‰§æƒ…æè¿°æ•ˆæœæ›´å¥½]ï¼‰",
    "Video Settings": "**è§†é¢‘è®¾ç½®**",
    "Video Concat Mode": "è§†é¢‘æ‹¼æ¥æ¨¡å¼",
    "Random": "éšæœºæ‹¼æ¥ï¼ˆæ¨èï¼‰",
    "Sequential": "é¡ºåºæ‹¼æ¥",
    "Video Ratio": "è§†é¢‘æ¯”ä¾‹",
    "Portrait": "ç«–å± 9:16ï¼ˆæŠ–éŸ³è§†é¢‘ï¼‰",
    "Landscape": "æ¨ªå± 16:9ï¼ˆè¥¿ç“œè§†é¢‘ï¼‰",
    "Clip Duration": "è§†é¢‘ç‰‡æ®µæœ€å¤§æ—¶é•¿(ç§’)ï¼ˆ**ä¸æ˜¯è§†é¢‘æ€»é•¿åº¦**ï¼Œæ˜¯æŒ‡æ¯ä¸ª**åˆæˆç‰‡æ®µ**çš„é•¿åº¦ï¼‰",
    "Number of Videos Generated Simultaneously": "åŒæ—¶ç”Ÿæˆè§†é¢‘æ•°é‡",
    "Audio Settings": "**éŸ³é¢‘è®¾ç½®**",
    "Speech Synthesis": "æœ—è¯»å£°éŸ³ï¼ˆ:red[**ä¸æ–‡æ¡ˆè¯­è¨€ä¿æŒä¸€è‡´**ã€‚æ³¨æ„ï¼šV2ç‰ˆæ•ˆæœæ›´å¥½ï¼Œä½†æ˜¯éœ€è¦API KEY]ï¼‰",
    "Speech Region": "æœåŠ¡åŒºåŸŸ (:red[å¿…å¡«ï¼Œ[ç‚¹å‡»è·å–](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Key": "API Key (:red[å¿…å¡«ï¼Œå¯†é’¥1 æˆ– å¯†é’¥2 å‡å¯ [ç‚¹å‡»è·å–](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Volume": "æœ—è¯»éŸ³é‡ï¼ˆ1.0è¡¨ç¤º100%ï¼‰",
    "Speech Rate": "æœ—è¯»é€Ÿåº¦ï¼ˆ1.0è¡¨ç¤º1å€é€Ÿï¼‰",
    "Male": "ç”·æ€§",
    "Female": "å¥³æ€§",
    "Background Music": "èƒŒæ™¯éŸ³ä¹",
    "No Background Music": "æ— èƒŒæ™¯éŸ³ä¹",
    "Random Background Music": "éšæœºèƒŒæ™¯éŸ³ä¹",
    "Custom Background Music": "è‡ªå®šä¹‰èƒŒæ™¯éŸ³ä¹",
    "Custom Background Music File": "è¯·è¾“å…¥è‡ªå®šä¹‰èƒŒæ™¯éŸ³ä¹çš„æ–‡ä»¶è·¯å¾„",
    "Background Music Volume": "èƒŒæ™¯éŸ³ä¹éŸ³é‡ï¼ˆ0.2è¡¨ç¤º20%ï¼ŒèƒŒæ™¯å£°éŸ³ä¸å®œè¿‡é«˜ï¼‰",
    "Subtitle Settings": "**å­—å¹•è®¾ç½®**",
    "Enable Subtitles": "å¯ç”¨å­—å¹•ï¼ˆè‹¥å–æ¶ˆå‹¾é€‰ï¼Œä¸‹é¢çš„è®¾ç½®éƒ½å°†ä¸ç”Ÿæ•ˆï¼‰",
    "Font": "å­—å¹•å­—ä½“",
    "Position": "å­—å¹•ä½ç½®",
    "Top": "é¡¶éƒ¨",
    "Center": "ä¸­é—´",
    "Bottom": "åº•éƒ¨ï¼ˆæ¨èï¼‰",
    "Custom": "è‡ªå®šä¹‰ä½ç½®ï¼ˆ70ï¼Œè¡¨ç¤ºç¦»é¡¶éƒ¨70%çš„ä½ç½®ï¼‰",
    "Font Size": "å­—å¹•å¤§å°",
    "Font Color": "å­—å¹•é¢œè‰²",
    "Stroke Color": "æè¾¹é¢œè‰²",
    "Stroke Width": "æè¾¹ç²—ç»†",
    "Generate Video": "ç”Ÿæˆè§†é¢‘",
    "Video Script and Subject Cannot Both Be Empty": "è§†é¢‘ä¸»é¢˜ å’Œ è§†é¢‘æ–‡æ¡ˆï¼Œä¸èƒ½åŒæ—¶ä¸ºç©º",
    "Generating Video": "æ­£åœ¨ç”Ÿæˆè§†é¢‘ï¼Œè¯·ç¨å€™...",
    "Start Generating Video": "å¼€å§‹ç”Ÿæˆè§†é¢‘",
    "Video Generation Completed": "è§†é¢‘ç”Ÿæˆå®Œæˆ",
    "Video Generation Failed": "è§†é¢‘ç”Ÿæˆå¤±è´¥",
    "You can download the generated video from the following links": "ä½ å¯ä»¥ä»ä»¥ä¸‹é“¾æ¥ä¸‹è½½ç”Ÿæˆçš„è§†é¢‘",
    "Basic Settings": "**åŸºç¡€è®¾ç½®** (:blue[ç‚¹å‡»å±•å¼€])",
    "Pixabay API Key": "Pixabay API Key ([ç‚¹å‡»è·å–](https://pixabay.com/api/docs/#api_search_videos)) :red[å¯ä»¥ä¸ç”¨é…ç½®ï¼Œå¦‚æœ Pexels æ— æ³•ä½¿ç”¨ï¼Œå†é€‰æ‹©Pixabay]",
    "Video LLM Provider": "è§†é¢‘è½¬å½•å¤§æ¨¡å‹",
    "LLM Provider": "å¤§è¯­è¨€æ¨¡å‹",
    "API Key": "API Key (:red[å¿…å¡«ï¼Œéœ€è¦åˆ°å¤§æ¨¡å‹æä¾›å•†çš„åå°ç”³è¯·])",
    "Base Url": "Base Url (å¯é€‰)",
    "Model Name": "æ¨¡å‹åç§° (:blue[éœ€è¦åˆ°å¤§æ¨¡å‹æä¾›å•†çš„åå°ç¡®è®¤è¢«æˆæƒçš„æ¨¡å‹åç§°])",
    "Please Enter the LLM API Key": "è¯·å…ˆå¡«å†™å¤§æ¨¡å‹ **API Key**",
    "Please Enter the Pixabay API Key": "è¯·å…ˆå¡«å†™ **Pixabay API Key**",
    "Get Help": "ä¸€ç«™å¼ AI å½±è§†è§£è¯´+è‡ªåŠ¨åŒ–å‰ªè¾‘å·¥å…·\uD83C\uDF89\uD83C\uDF89\uD83C\uDF89\n\næœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œå¯ä»¥åŠ å…¥ **ç¤¾åŒºé¢‘é“** æ±‚åŠ©æˆ–è®¨è®ºï¼šhttps://github.com/linyqh/NarratoAI/wiki",
    "Video Source": "è§†é¢‘æ¥æº",
    "TikTok": "æŠ–éŸ³ (TikTok æ”¯æŒä¸­ï¼Œæ•¬è¯·æœŸå¾…)",
    "Bilibili": "å“”å“©å“”å“© (Bilibili æ”¯æŒä¸­ï¼Œæ•¬è¯·æœŸå¾…)",
    "Xiaohongshu": "å°çº¢ä¹¦ (Xiaohongshu æ”¯æŒä¸­ï¼Œæ•¬è¯·æœŸå¾…)",
    "Local file": "æœ¬åœ°æ–‡ä»¶",
    "Play Voice": "è¯•å¬è¯­éŸ³åˆæˆ",
    "Voice Example": "è¿™æ˜¯ä¸€æ®µæµ‹è¯•è¯­éŸ³åˆæˆçš„ç¤ºä¾‹æ–‡æœ¬",
    "Synthesizing Voice": "è¯­éŸ³åˆæˆä¸­ï¼Œè¯·ç¨å€™...",
    "TTS Provider": "è¯­éŸ³åˆæˆæä¾›å•†",
    "Hide Log": "éšè—æ—¥å¿—",
    "Upload Local Files": "ä¸Šä¼ æœ¬åœ°æ–‡ä»¶",
    "Video Check": "è§†é¢‘å®¡æŸ¥",
    "File Uploaded Successfully": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
    "timestamp": "æ—¶é—´æˆ³",
    "Picture description": "å›¾ç‰‡æè¿°",
    "Narration": "è§†é¢‘æ–‡æ¡ˆ",
    "Rebuild": "é‡æ–°ç”Ÿæˆ",
    "Load Video Script": "åŠ è½½è§†é¢‘è„šæœ¬",
    "Speech Pitch": "è¯­è°ƒ",
    "Please Select Script File": "è¯·é€‰æ‹©è„šæœ¬æ–‡ä»¶",
    "Check Format": "è„šæœ¬æ ¼å¼æ£€æŸ¥",
    "Script Loaded Successfully": "è„šæœ¬åŠ è½½æˆåŠŸ",
    "Script format check passed": "è„šæœ¬æ ¼å¼æ£€æŸ¥é€šè¿‡",
    "Script format check failed": "è„šæœ¬æ ¼å¼æ£€æŸ¥å¤±è´¥",
    "Failed to Load Script": "åŠ è½½è„šæœ¬å¤±è´¥",
    "Failed to Save Script": "ä¿å­˜è„šæœ¬å¤±è´¥",
    "Script saved successfully": "è„šæœ¬ä¿å­˜æˆåŠŸ",
    "Video Script": "è§†é¢‘è„šæœ¬",
    "Video Quality": "è§†é¢‘è´¨é‡",
    "Custom prompt for LLM, leave empty to use default prompt": "è‡ªå®šä¹‰æç¤ºè¯ï¼Œç•™ç©ºåˆ™ä½¿ç”¨é»˜è®¤æç¤ºè¯",
    "Proxy Settings": "ä»£ç†è®¾ç½®",
    "HTTP_PROXY": "HTTP ä»£ç†",
    "HTTPs_PROXY": "HTTPS ä»£ç†",
    "Vision Model Settings": "è§†é¢‘åˆ†ææ¨¡å‹è®¾ç½®",
    "Vision Model Provider": "è§†é¢‘åˆ†ææ¨¡å‹æä¾›å•†",
    "Vision API Key": "è§†é¢‘åˆ†æ API å¯†é’¥",
    "Vision Base URL": "è§†é¢‘åˆ†ææ¥å£åœ°å€",
    "Vision Model Name": "è§†é¢‘åˆ†ææ¨¡å‹åç§°",
    "Narrato Additional Settings": "Narrato é™„åŠ è®¾ç½®",
    "Narrato API Key": "Narrato API å¯†é’¥",
    "Narrato API URL": "Narrato API åœ°å€",
    "Text Generation Model Settings": "æ–‡æ¡ˆç”Ÿæˆæ¨¡å‹è®¾ç½®",
    "LLM Model Name": "å¤§è¯­è¨€æ¨¡å‹åç§°",
    "LLM Model API Key": "å¤§è¯­è¨€æ¨¡å‹ API å¯†é’¥",
    "Batch Size": "æ‰¹å¤„ç†å¤§å°",
    "Text Model Provider": "æ–‡æ¡ˆç”Ÿæˆæ¨¡å‹æä¾›å•†",
    "Text API Key": "æ–‡æ¡ˆç”Ÿæˆ API å¯†é’¥",
    "Text Base URL": "æ–‡æ¡ˆç”Ÿæˆæ¥å£åœ°å€",
    "Text Model Name": "æ–‡æ¡ˆç”Ÿæˆæ¨¡å‹åç§°",
    "Account ID": "è´¦æˆ· ID",
    "Skip the first few seconds": "è·³è¿‡å¼€å¤´å¤šå°‘ç§’",
    "Difference threshold": "å·®å¼‚é˜ˆå€¼",
    "Vision processing batch size": "è§†è§‰å¤„ç†æ‰¹æ¬¡å¤§å°",
    "Test Connection": "æµ‹è¯•è¿æ¥",
    "gemini model is available": "Gemini æ¨¡å‹å¯ç”¨",
    "gemini model is not available": "Gemini æ¨¡å‹ä¸å¯ç”¨",
    "NarratoAPI is available": "NarratoAPI å¯ç”¨",
    "NarratoAPI is not available": "NarratoAPI ä¸å¯ç”¨",
    "Unsupported provider": "ä¸æ”¯æŒçš„æä¾›å•†",
    "0: Keep the audio only, 1: Keep the original sound only, 2: Keep the original sound and audio": "0: ä»…ä¿ç•™éŸ³é¢‘ï¼Œ1: ä»…ä¿ç•™åŸå£°ï¼Œ2: ä¿ç•™åŸå£°å’ŒéŸ³é¢‘",
    "Text model is not available": "æ–‡æ¡ˆç”Ÿæˆæ¨¡å‹ä¸å¯ç”¨",
    "Text model is available": "æ–‡æ¡ˆç”Ÿæˆæ¨¡å‹å¯ç”¨",
    "Upload Script": "ä¸Šä¼ è„šæœ¬",
    "Upload Script File": "ä¸Šä¼ è„šæœ¬æ–‡ä»¶",
    "Script Uploaded Successfully": "è„šæœ¬ä¸Šä¼ æˆåŠŸ",
    "Invalid JSON format": "æ— æ•ˆçš„JSONæ ¼å¼",
    "Upload failed": "ä¸Šä¼ å¤±è´¥",
    "Video Subtitle Merge": "**åˆå¹¶è§†é¢‘ä¸å­—å¹•**",
    "Upload Video and Subtitle Files": "ä¸Šä¼ è§†é¢‘å’Œå­—å¹•æ–‡ä»¶",
    "Matched File Pairs": "å·²åŒ¹é…çš„æ–‡ä»¶å¯¹",
    "Merge All Files": "åˆå¹¶æ‰€æœ‰æ–‡ä»¶",
    "Merge Function Not Implemented": "åˆå¹¶åŠŸèƒ½å¾…å®ç°",
    "No Matched Pairs Found": "æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶å¯¹",
    "Missing Subtitle": "ç¼ºå°‘å¯¹åº”çš„å­—å¹•æ–‡ä»¶",
    "Missing Video": "ç¼ºå°‘å¯¹åº”çš„è§†é¢‘æ–‡ä»¶",
    "All Uploaded Files": "æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶",
    "Order": "æ’åºåºå·",
    "Reorder": "é‡æ–°æ’åº",
    "Merging files...": "æ­£åœ¨åˆå¹¶æ–‡ä»¶...",
    "Merge completed!": "åˆå¹¶å®Œæˆï¼",
    "Download Merged Video": "ä¸‹è½½åˆå¹¶åçš„è§†é¢‘",
    "Download Merged Subtitle": "ä¸‹è½½åˆå¹¶åçš„å­—å¹•",
    "Error during merge": "åˆå¹¶è¿‡ç¨‹ä¸­å‡ºé”™",
    "Failed to generate merged video.": "ç”Ÿæˆåˆå¹¶è§†é¢‘å¤±è´¥ã€‚",
    "Failed to generate merged subtitle.": "ç”Ÿæˆåˆå¹¶å­—å¹•å¤±è´¥ã€‚",
    "Error reading merged video file": "è¯»å–åˆå¹¶åçš„è§†é¢‘æ–‡ä»¶æ—¶å‡ºé”™",
    "Error reading merged subtitle file": "è¯»å–åˆå¹¶åçš„å­—å¹•æ–‡ä»¶æ—¶å‡ºé”™",
    "Error processing video files. Please check if the videos are valid MP4 files.": "å¤„ç†è§†é¢‘æ–‡ä»¶æ—¶å‡ºé”™ã€‚è¯·æ£€æŸ¥è§†é¢‘æ˜¯å¦ä¸ºæœ‰æ•ˆçš„MP4æ–‡ä»¶ã€‚",
    "Error processing subtitle files. Please check if the subtitles are valid SRT files.": "å¤„ç†å­—å¹•æ–‡ä»¶æ—¶å‡ºé”™ã€‚è¯·æ£€æŸ¥å­—å¹•æ˜¯å¦ä¸ºæœ‰æ•ˆçš„SRTæ–‡ä»¶ã€‚",
    "Preview Merged Video": "é¢„è§ˆåˆå¹¶åçš„è§†é¢‘",
    "Video Path": "è§†é¢‘è·¯å¾„",
    "Subtitle Path": "å­—å¹•è·¯å¾„",
    "Enable Proxy": "å¯ç”¨ä»£ç†",
    "QwenVL model is available": "QwenVL æ¨¡å‹å¯ç”¨",
    "QwenVL model is not available": "QwenVL æ¨¡å‹ä¸å¯ç”¨",
    "System settings": "ç³»ç»Ÿè®¾ç½®",
    "Clear Cache": "æ¸…ç†ç¼“å­˜",
    "Cache cleared": "ç¼“å­˜æ¸…ç†å®Œæˆ",
    "storage directory does not exist": "storageç›®å½•ä¸å­˜åœ¨",
    "Failed to clear cache": "æ¸…ç†ç¼“å­˜å¤±è´¥",
    "Clear frames": "æ¸…ç†å…³é”®å¸§",
    "Clear clip videos": "æ¸…ç†è£å‰ªè§†é¢‘",
    "Clear tasks": "æ¸…ç†ä»»åŠ¡",
    "Directory cleared": "ç›®å½•æ¸…ç†å®Œæˆ",
    "Directory does not exist": "ç›®å½•ä¸å­˜åœ¨",
    "Failed to clear directory": "æ¸…ç†ç›®å½•å¤±è´¥",
    "Subtitle Preview": "å­—å¹•é¢„è§ˆ",
    "One-Click Transcribe": "ä¸€é”®è½¬å½•",
    "Transcribing...": "æ­£åœ¨è½¬å½•ä¸­...",
    "Transcription Complete!": "è½¬å½•å®Œæˆï¼",
    "Transcription Failed. Please try again.": "è½¬å½•å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚",
    "API rate limit exceeded. Please wait about an hour and try again.": "API è°ƒç”¨æ¬¡æ•°å·²è¾¾åˆ°é™åˆ¶ï¼Œè¯·ç­‰å¾…çº¦ä¸€å°æ—¶åå†è¯•ã€‚",
    "Resources exhausted. Please try again later.": "èµ„æºå·²è€—å°½ï¼Œè¯·ç¨åå†è¯•ã€‚",
    "Transcription Failed": "è½¬å½•å¤±è´¥",
    "Mergeable Files": "å¯åˆå¹¶æ–‡ä»¶æ•°",
    "Subtitle Content": "å­—å¹•å†…å®¹",
    "Merge Result Preview": "åˆå¹¶ç»“æœé¢„è§ˆ",
    "Short Generate": "çŸ­å‰§æ··å‰ª (é«˜ç‡ƒå‰ªè¾‘)",
    "Generate Short Video Script": "AIç”ŸæˆçŸ­å‰§æ··å‰ªè„šæœ¬",
    "Adjust the volume of the original audio": "è°ƒæ•´åŸå§‹éŸ³é¢‘çš„éŸ³é‡",
    "Original Volume": "è§†é¢‘éŸ³é‡",
    "Auto Generate": "çºªå½•ç‰‡è§£è¯´ (ç”»é¢è§£è¯´)"
  }
}
</file>

<file path="webui/tools/base.py">
import os
import requests
import streamlit as st
from loguru import logger
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from app.config import config
from app.utils import gemini_analyzer, qwenvl_analyzer


def create_vision_analyzer(provider, api_key, model, base_url):
    """
    åˆ›å»ºè§†è§‰åˆ†æå™¨å®ä¾‹
    
    Args:
        provider: æä¾›å•†åç§° ('gemini' æˆ– 'qwenvl')
        api_key: APIå¯†é’¥
        model: æ¨¡å‹åç§°
        base_url: APIåŸºç¡€URL
        
    Returns:
        VisionAnalyzer æˆ– QwenAnalyzer å®ä¾‹
    """
    if provider == 'gemini':
        return gemini_analyzer.VisionAnalyzer(model_name=model, api_key=api_key)
    elif provider == 'qwenvl':
        # åªä¼ å…¥å¿…è¦çš„å‚æ•°
        return qwenvl_analyzer.QwenAnalyzer(
            model_name=model, 
            api_key=api_key,
            base_url=base_url
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è§†è§‰åˆ†ææä¾›å•†: {provider}")


def get_batch_timestamps(batch_files, prev_batch_files=None):
    """
    è§£æä¸€æ‰¹æ–‡ä»¶çš„æ—¶é—´æˆ³èŒƒå›´,æ”¯æŒæ¯«ç§’çº§ç²¾åº¦

    Args:
        batch_files: å½“å‰æ‰¹æ¬¡çš„æ–‡ä»¶åˆ—è¡¨
        prev_batch_files: ä¸Šä¸€ä¸ªæ‰¹æ¬¡çš„æ–‡ä»¶åˆ—è¡¨,ç”¨äºå¤„ç†å•å¼ å›¾ç‰‡çš„æƒ…å†µ

    Returns:
        tuple: (first_timestamp, last_timestamp, timestamp_range)
        æ—¶é—´æˆ³æ ¼å¼: HH:MM:SS,mmm (æ—¶:åˆ†:ç§’,æ¯«ç§’)
        ä¾‹å¦‚: 00:00:50,100 è¡¨ç¤º50ç§’100æ¯«ç§’

    ç¤ºä¾‹æ–‡ä»¶åæ ¼å¼:
        keyframe_001253_000050100.jpg
        å…¶ä¸­ 000050100 è¡¨ç¤º 00:00:50,100 (50ç§’100æ¯«ç§’)
    """
    if not batch_files:
        logger.warning("Empty batch files")
        return "00:00:00,000", "00:00:00,000", "00:00:00,000-00:00:00,000"

    def get_frame_files():
        """è·å–é¦–å¸§å’Œå°¾å¸§æ–‡ä»¶å"""
        if len(batch_files) == 1 and prev_batch_files and prev_batch_files:
            # å•å¼ å›¾ç‰‡æƒ…å†µ:ä½¿ç”¨ä¸Šä¸€æ‰¹æ¬¡æœ€åä¸€å¸§ä½œä¸ºé¦–å¸§
            first = os.path.basename(prev_batch_files[-1])
            last = os.path.basename(batch_files[0])
            logger.debug(f"å•å¼ å›¾ç‰‡æ‰¹æ¬¡,ä½¿ç”¨ä¸Šä¸€æ‰¹æ¬¡æœ€åä¸€å¸§ä½œä¸ºé¦–å¸§: {first}")
        else:
            first = os.path.basename(batch_files[0])
            last = os.path.basename(batch_files[-1])
        return first, last

    def extract_time(filename):
        """ä»æ–‡ä»¶åæå–æ—¶é—´ä¿¡æ¯"""
        try:
            # æå–ç±»ä¼¼ 000050100 çš„æ—¶é—´æˆ³éƒ¨åˆ†
            time_str = filename.split('_')[2].replace('.jpg', '')
            if len(time_str) < 9:  # å¤„ç†æ—§æ ¼å¼
                time_str = time_str.ljust(9, '0')
            return time_str
        except (IndexError, AttributeError) as e:
            logger.warning(f"Invalid filename format: {filename}, error: {e}")
            return "000000000"

    def format_timestamp(time_str):
        """
        å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸º HH:MM:SS,mmm æ ¼å¼

        Args:
            time_str: 9ä½æ•°å­—å­—ç¬¦ä¸²,æ ¼å¼ä¸º HHMMSSMMM
                     ä¾‹å¦‚: 000010000 è¡¨ç¤º 00æ—¶00åˆ†10ç§’000æ¯«ç§’
                          000043039 è¡¨ç¤º 00æ—¶00åˆ†43ç§’039æ¯«ç§’

        Returns:
            str: HH:MM:SS,mmm æ ¼å¼çš„æ—¶é—´æˆ³
        """
        try:
            if len(time_str) < 9:
                logger.warning(f"Invalid timestamp format: {time_str}")
                return "00:00:00,000"

            # ä»æ—¶é—´æˆ³ä¸­æå–æ—¶ã€åˆ†ã€ç§’å’Œæ¯«ç§’
            hours = int(time_str[0:2])  # å‰2ä½ä½œä¸ºå°æ—¶
            minutes = int(time_str[2:4])  # ç¬¬3-4ä½ä½œä¸ºåˆ†é’Ÿ
            seconds = int(time_str[4:6])  # ç¬¬5-6ä½ä½œä¸ºç§’æ•°
            milliseconds = int(time_str[6:])  # æœ€å3ä½ä½œä¸ºæ¯«ç§’

            return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"

        except ValueError as e:
            logger.warning(f"æ—¶é—´æˆ³æ ¼å¼è½¬æ¢å¤±è´¥: {time_str}, error: {e}")
            return "00:00:00,000"

    # è·å–é¦–å¸§å’Œå°¾å¸§æ–‡ä»¶å
    first_frame, last_frame = get_frame_files()

    # ä»æ–‡ä»¶åä¸­æå–æ—¶é—´ä¿¡æ¯
    first_time = extract_time(first_frame)
    last_time = extract_time(last_frame)

    # è½¬æ¢ä¸ºæ ‡å‡†æ—¶é—´æˆ³æ ¼å¼
    first_timestamp = format_timestamp(first_time)
    last_timestamp = format_timestamp(last_time)
    timestamp_range = f"{first_timestamp}-{last_timestamp}"

    # logger.debug(f"è§£ææ—¶é—´æˆ³: {first_frame} -> {first_timestamp}, {last_frame} -> {last_timestamp}")
    return first_timestamp, last_timestamp, timestamp_range


def get_batch_files(keyframe_files, result, batch_size=5):
    """
    è·å–å½“å‰æ‰¹æ¬¡çš„å›¾ç‰‡æ–‡ä»¶
    """
    batch_start = result['batch_index'] * batch_size
    batch_end = min(batch_start + batch_size, len(keyframe_files))
    return keyframe_files[batch_start:batch_end]


def chekc_video_config(video_params):
    """
    æ£€æŸ¥è§†é¢‘åˆ†æé…ç½®
    """
    headers = {
        'accept': 'application/json',
        'Content-Type': 'application/json'
    }
    session = requests.Session()
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[500, 502, 503, 504]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    try:
        session.post(
            f"{config.app.get('narrato_api_url')}/video/config",
            headers=headers,
            json=video_params,
            timeout=30,
            verify=True
        )
        return True
    except Exception as e:
        return False
</file>

<file path="webui/tools/generate_script_docu.py">
# çºªå½•ç‰‡è„šæœ¬ç”Ÿæˆ
import os
import json
import time
import asyncio
import traceback
import requests
import streamlit as st
from loguru import logger
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from app.config import config
from app.utils.script_generator import ScriptProcessor
from app.utils import utils, video_processor, video_processor_v2, qwenvl_analyzer
from webui.tools.base import create_vision_analyzer, get_batch_files, get_batch_timestamps, chekc_video_config


def generate_script_docu(tr, params):
    """
    ç”Ÿæˆ çºªå½•ç‰‡ è§†é¢‘è„šæœ¬
    """
    progress_bar = st.progress(0)
    status_text = st.empty()

    def update_progress(progress: float, message: str = ""):
        progress_bar.progress(progress)
        if message:
            status_text.text(f"{progress}% - {message}")
        else:
            status_text.text(f"è¿›åº¦: {progress}%")

    try:
        with st.spinner("æ­£åœ¨ç”Ÿæˆè„šæœ¬..."):
            if not params.video_origin_path:
                st.error("è¯·å…ˆé€‰æ‹©è§†é¢‘æ–‡ä»¶")
                return

            # ===================æå–é”®å¸§===================
            update_progress(10, "æ­£åœ¨æå–å…³é”®å¸§...")

            # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºå­˜å‚¨å…³é”®å¸§
            keyframes_dir = os.path.join(utils.temp_dir(), "keyframes")
            video_hash = utils.md5(params.video_origin_path + str(os.path.getmtime(params.video_origin_path)))
            video_keyframes_dir = os.path.join(keyframes_dir, video_hash)

            # æ£€æŸ¥æ˜¯å¦å·²ç»æå–è¿‡å…³é”®å¸§
            keyframe_files = []
            if os.path.exists(video_keyframes_dir):
                # å–å·²æœ‰çš„å…³é”®å¸§æ–‡ä»¶
                for filename in sorted(os.listdir(video_keyframes_dir)):
                    if filename.endswith('.jpg'):
                        keyframe_files.append(os.path.join(video_keyframes_dir, filename))

                if keyframe_files:
                    logger.info(f"ä½¿ç”¨å·²ç¼“å­˜çš„å…³é”®å¸§: {video_keyframes_dir}")
                    st.info(f"ä½¿ç”¨å·²ç¼“å­˜çš„å…³é”®å¸§ï¼Œå¦‚éœ€é‡æ–°æå–è¯·åˆ é™¤ç›®å½•: {video_keyframes_dir}")
                    update_progress(20, f"ä½¿ç”¨å·²ç¼“å­˜å…³é”®å¸§ï¼Œå…± {len(keyframe_files)} å¸§")

            # å¦‚æœæ²¡æœ‰ç¼“å­˜çš„å…³é”®å¸§ï¼Œåˆ™è¿›è¡Œæå–
            if not keyframe_files:
                try:
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    os.makedirs(video_keyframes_dir, exist_ok=True)

                    # åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨
                    if config.frames.get("version") == "v2":
                        processor = video_processor_v2.VideoProcessor(params.video_origin_path)
                        # å¤„ç†è§†é¢‘å¹¶æå–å…³é”®å¸§
                        processor.process_video_pipeline(
                            output_dir=video_keyframes_dir,
                            skip_seconds=st.session_state.get('skip_seconds'),
                            threshold=st.session_state.get('threshold')
                        )
                    else:
                        processor = video_processor.VideoProcessor(params.video_origin_path)
                        # å¤„ç†è§†é¢‘å¹¶æå–å…³é”®å¸§
                        processor.process_video(
                            output_dir=video_keyframes_dir,
                            skip_seconds=0
                        )

                    # è·å–æ‰€æœ‰å…³é”®æ–‡ä»¶è·¯å¾„
                    for filename in sorted(os.listdir(video_keyframes_dir)):
                        if filename.endswith('.jpg'):
                            keyframe_files.append(os.path.join(video_keyframes_dir, filename))

                    if not keyframe_files:
                        raise Exception("æœªæå–åˆ°ä»»ä½•å…³é”®å¸§")

                    update_progress(20, f"å…³é”®å¸§æå–å®Œæˆï¼Œå…± {len(keyframe_files)} å¸§")

                except Exception as e:
                    # å¦‚æœæå–å¤±è´¥ï¼Œæ¸…ç†åˆ›å»ºçš„ç›®å½•
                    try:
                        if os.path.exists(video_keyframes_dir):
                            import shutil
                            shutil.rmtree(video_keyframes_dir)
                    except Exception as cleanup_err:
                        logger.error(f"æ¸…ç†å¤±è´¥çš„å…³é”®å¸§ç›®å½•æ—¶å‡ºé”™: {cleanup_err}")

                    raise Exception(f"å…³é”®å¸§æå–å¤±è´¥: {str(e)}")

            # æ ¹æ®ä¸åŒçš„ LLM æä¾›å•†å¤„ç†
            vision_llm_provider = st.session_state.get('vision_llm_providers').lower()
            logger.debug(f"Vision LLM æä¾›å•†: {vision_llm_provider}")

            try:
                # ===================åˆå§‹åŒ–è§†è§‰åˆ†æå™¨===================
                update_progress(30, "æ­£åœ¨åˆå§‹åŒ–è§†è§‰åˆ†æå™¨...")

                # ä»é…ç½®ä¸­è·å–ç›¸å…³é…ç½®
                if vision_llm_provider == 'gemini':
                    vision_api_key = st.session_state.get('vision_gemini_api_key')
                    vision_model = st.session_state.get('vision_gemini_model_name')
                    vision_base_url = st.session_state.get('vision_gemini_base_url')
                elif vision_llm_provider == 'qwenvl':
                    vision_api_key = st.session_state.get('vision_qwenvl_api_key')
                    vision_model = st.session_state.get('vision_qwenvl_model_name', 'qwen-vl-max-latest')
                    vision_base_url = st.session_state.get('vision_qwenvl_base_url')
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„è§†è§‰åˆ†ææä¾›å•†: {vision_llm_provider}")

                # åˆ›å»ºè§†è§‰åˆ†æå™¨å®ä¾‹
                analyzer = create_vision_analyzer(
                    provider=vision_llm_provider,
                    api_key=vision_api_key,
                    model=vision_model,
                    base_url=vision_base_url
                )

                update_progress(40, "æ­£åœ¨åˆ†æå…³é”®å¸§...")

                # ===================åˆ›å»ºå¼‚æ­¥äº‹ä»¶å¾ªç¯===================
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

                # æ‰§è¡Œå¼‚æ­¥åˆ†æ
                vision_batch_size = st.session_state.get('vision_batch_size') or config.frames.get("vision_batch_size")
                results = loop.run_until_complete(
                    analyzer.analyze_images(
                        images=keyframe_files,
                        prompt=config.app.get('vision_analysis_prompt'),
                        batch_size=vision_batch_size
                    )
                )
                loop.close()

                # ===================å¤„ç†åˆ†æç»“æœ===================
                update_progress(60, "æ­£åœ¨æ•´ç†åˆ†æç»“æœ...")

                # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„æç»“æœ
                frame_analysis = ""
                prev_batch_files = None

                for result in results:
                    if 'error' in result:
                        logger.warning(f"æ‰¹æ¬¡ {result['batch_index']} å¤„ç†å‡ºç°è­¦å‘Š: {result['error']}")

                    # è·å–å½“å‰æ‰¹æ¬¡çš„æ–‡ä»¶åˆ—è¡¨ keyframe_001136_000045.jpg å°† 000045 ç²¾åº¦æå‡åˆ° æ¯«ç§’
                    batch_files = get_batch_files(keyframe_files, result, vision_batch_size)
                    logger.debug(f"æ‰¹æ¬¡ {result['batch_index']} å¤„ç†å®Œæˆï¼Œå…± {len(batch_files)} å¼ å›¾ç‰‡")
                    # logger.debug(batch_files)

                    first_timestamp, last_timestamp, _ = get_batch_timestamps(batch_files, prev_batch_files)
                    logger.debug(f"å¤„ç†æ—¶é—´æˆ³: {first_timestamp}-{last_timestamp}")

                    # æ·»åŠ å¸¦æ—¶é—´æˆ³çš„åˆ†æç»“æœ
                    frame_analysis += f"\n=== {first_timestamp}-{last_timestamp} ===\n"
                    frame_analysis += result['response']
                    frame_analysis += "\n"

                    # æ›´æ–°ä¸Šä¸€ä¸ªæ‰¹æ¬¡çš„æ–‡ä»¶
                    prev_batch_files = batch_files

                if not frame_analysis.strip():
                    raise Exception("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å¸§åˆ†æç»“æœ")

                # ä¿å­˜åˆ†æç»“æœ
                analysis_path = os.path.join(utils.temp_dir(), "frame_analysis.txt")
                with open(analysis_path, 'w', encoding='utf-8') as f:
                    f.write(frame_analysis)

                update_progress(70, "æ­£åœ¨ç”Ÿæˆè„šæœ¬...")

                # ä»é…ç½®ä¸­è·å–æ–‡æœ¬ç”Ÿæˆç›¸å…³é…ç½®
                text_provider = config.app.get('text_llm_provider', 'gemini').lower()
                text_api_key = config.app.get(f'text_{text_provider}_api_key')
                text_model = config.app.get(f'text_{text_provider}_model_name')
                text_base_url = config.app.get(f'text_{text_provider}_base_url')

                # æ„å»ºå¸§å†…å®¹åˆ—è¡¨
                frame_content_list = []
                prev_batch_files = None

                for i, result in enumerate(results):
                    if 'error' in result:
                        continue

                    batch_files = get_batch_files(keyframe_files, result, vision_batch_size)
                    _, _, timestamp_range = get_batch_timestamps(batch_files, prev_batch_files)

                    frame_content = {
                        "timestamp": timestamp_range,
                        "picture": result['response'],
                        "narration": "",
                        "OST": 2
                    }
                    frame_content_list.append(frame_content)

                    logger.debug(f"æ·»åŠ å¸§å†…å®¹: æ—¶é—´èŒƒå›´={timestamp_range}, åˆ†æç»“æœé•¿åº¦={len(result['response'])}")

                    # æ›´æ–°ä¸Šä¸€ä¸ªæ‰¹æ¬¡çš„æ–‡ä»¶
                    prev_batch_files = batch_files

                if not frame_content_list:
                    raise Exception("æ²¡æœ‰æœ‰æ•ˆçš„å¸§å†…å®¹å¯ä»¥å¤„ç†")

                # ===================å¼€å§‹ç”Ÿæˆæ–‡æ¡ˆ===================
                update_progress(80, "æ­£åœ¨ç”Ÿæˆæ–‡æ¡ˆ...")
                # æ ¡éªŒé…ç½®
                api_params = {
                    "vision_api_key": vision_api_key,
                    "vision_model_name": vision_model,
                    "vision_base_url": vision_base_url or "",
                    "text_api_key": text_api_key,
                    "text_model_name": text_model,
                    "text_base_url": text_base_url or ""
                }
                chekc_video_config(api_params)
                custom_prompt = st.session_state.get('custom_prompt', '')
                processor = ScriptProcessor(
                    model_name=text_model,
                    api_key=text_api_key,
                    prompt=custom_prompt,
                    base_url=text_base_url or "",
                    video_theme=st.session_state.get('video_theme', '')
                )

                # å¤„ç†å¸§å†…å®¹ç”Ÿæˆè„šæœ¬
                script_result = processor.process_frames(frame_content_list)

                # ç»“æœè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                script = json.dumps(script_result, ensure_ascii=False, indent=2)

            except Exception as e:
                logger.exception(f"å¤§æ¨¡å‹å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯\n{traceback.format_exc()}")
                raise Exception(f"åˆ†æå¤±è´¥: {str(e)}")

            if script is None:
                st.error("ç”Ÿæˆè„šæœ¬å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
                st.stop()
            logger.info(f"è„šæœ¬ç”Ÿæˆå®Œæˆ")
            if isinstance(script, list):
                st.session_state['video_clip_json'] = script
            elif isinstance(script, str):
                st.session_state['video_clip_json'] = json.loads(script)
            update_progress(80, "è„šæœ¬ç”Ÿæˆå®Œæˆ")

        time.sleep(0.1)
        progress_bar.progress(100)
        status_text.text("è„šæœ¬ç”Ÿæˆå®Œæˆï¼")
        st.success("è§†é¢‘è„šæœ¬ç”ŸæˆæˆåŠŸï¼")

    except Exception as err:
        st.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(err)}")
        logger.exception(f"ç”Ÿæˆè„šæœ¬æ—¶å‘ç”Ÿé”™è¯¯\n{traceback.format_exc()}")
    finally:
        time.sleep(2)
        progress_bar.empty()
        status_text.empty()
</file>

<file path="webui/tools/generate_script_short.py">
import os
import json
import time
import asyncio
import traceback
import requests
import streamlit as st
from loguru import logger

from app.config import config
from webui.tools.base import chekc_video_config


def generate_script_short(tr, params):
    """
    ç”Ÿæˆ çºªå½•ç‰‡ è§†é¢‘è„šæœ¬
    """
    progress_bar = st.progress(0)
    status_text = st.empty()

    def update_progress(progress: float, message: str = ""):
        progress_bar.progress(progress)
        if message:
            status_text.text(f"{progress}% - {message}")
        else:
            status_text.text(f"è¿›åº¦: {progress}%")

    try:
        with st.spinner("æ­£åœ¨ç”Ÿæˆè„šæœ¬..."):
            text_provider = config.app.get('text_llm_provider', 'gemini').lower()
            text_api_key = config.app.get(f'text_{text_provider}_api_key')
            text_model = config.app.get(f'text_{text_provider}_model_name')
            text_base_url = config.app.get(f'text_{text_provider}_base_url')
            vision_api_key = st.session_state.get(f'vision_{text_provider}_api_key', "")
            vision_model = st.session_state.get(f'vision_{text_provider}_model_name', "")
            vision_base_url = st.session_state.get(f'vision_{text_provider}_base_url', "")
            narrato_api_key = config.app.get('narrato_api_key')

            update_progress(20, "å¼€å§‹å‡†å¤‡ç”Ÿæˆè„šæœ¬")

            srt_path = params.video_origin_path.replace(".mp4", ".srt").replace("videos", "srt").replace("video", "subtitle")
            if not os.path.exists(srt_path):
                logger.error(f"{srt_path} æ–‡ä»¶ä¸å­˜åœ¨è¯·æ£€æŸ¥æˆ–é‡æ–°è½¬å½•")
                st.error(f"{srt_path} æ–‡ä»¶ä¸å­˜åœ¨è¯·æ£€æŸ¥æˆ–é‡æ–°è½¬å½•")
                st.stop()

            api_params = {
                "vision_api_key": vision_api_key,
                "vision_model_name": vision_model,
                "vision_base_url": vision_base_url or "",
                "text_api_key": text_api_key,
                "text_model_name": text_model,
                "text_base_url": text_base_url or ""
            }
            chekc_video_config(api_params)
            from app.services.SDP.generate_script_short import generate_script
            script = generate_script(
                srt_path=srt_path,
                output_path="resource/scripts/merged_subtitle.json",
                api_key=text_api_key,
                model_name=text_model,
                base_url=text_base_url,
                narrato_api_key=narrato_api_key,
                bert_path="app/models/bert/",
            )

            if script is None:
                st.error("ç”Ÿæˆè„šæœ¬å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
                st.stop()
            logger.info(f"è„šæœ¬ç”Ÿæˆå®Œæˆ {json.dumps(script, ensure_ascii=False, indent=4)}")
            if isinstance(script, list):
                st.session_state['video_clip_json'] = script
            elif isinstance(script, str):
                st.session_state['video_clip_json'] = json.loads(script)
            update_progress(80, "è„šæœ¬ç”Ÿæˆå®Œæˆ")

        time.sleep(0.1)
        progress_bar.progress(100)
        status_text.text("è„šæœ¬ç”Ÿæˆå®Œæˆï¼")
        st.success("è§†é¢‘è„šæœ¬ç”ŸæˆæˆåŠŸï¼")

    except Exception as err:
        progress_bar.progress(100)
        st.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(err)}")
        logger.exception(f"ç”Ÿæˆè„šæœ¬æ—¶å‘ç”Ÿé”™è¯¯\n{traceback.format_exc()}")
</file>

<file path="webui/utils/__init__.py">
from .performance import monitor_performance, PerformanceMonitor
from .cache import *
from .file_utils import *

__all__ = [
    'monitor_performance',
    'PerformanceMonitor'
]
</file>

<file path="webui/utils/cache.py">
import streamlit as st
import os
import glob
from app.utils import utils

def get_fonts_cache(font_dir):
    if 'fonts_cache' not in st.session_state:
        fonts = []
        for root, dirs, files in os.walk(font_dir):
            for file in files:
                if file.endswith(".ttf") or file.endswith(".ttc"):
                    fonts.append(file)
        fonts.sort()
        st.session_state['fonts_cache'] = fonts
    return st.session_state['fonts_cache']

def get_video_files_cache():
    if 'video_files_cache' not in st.session_state:
        video_files = []
        for suffix in ["*.mp4", "*.mov", "*.avi", "*.mkv"]:
            video_files.extend(glob.glob(os.path.join(utils.video_dir(), suffix)))
        st.session_state['video_files_cache'] = video_files[::-1]
    return st.session_state['video_files_cache']

def get_songs_cache(song_dir):
    if 'songs_cache' not in st.session_state:
        songs = []
        for root, dirs, files in os.walk(song_dir):
            for file in files:
                if file.endswith(".mp3"):
                    songs.append(file)
        st.session_state['songs_cache'] = songs
    return st.session_state['songs_cache']
</file>

<file path="webui/utils/file_utils.py">
import os
import glob
import time
import platform
import shutil
from uuid import uuid4
from loguru import logger
from app.utils import utils

def open_task_folder(root_dir, task_id):
    """æ‰“å¼€ä»»åŠ¡æ–‡ä»¶å¤¹
    Args:
        root_dir: é¡¹ç›®æ ¹ç›®å½•
        task_id: ä»»åŠ¡ID
    """
    try:
        sys = platform.system()
        path = os.path.join(root_dir, "storage", "tasks", task_id)
        if os.path.exists(path):
            if sys == 'Windows':
                os.system(f"start {path}")
            if sys == 'Darwin':
                os.system(f"open {path}")
            if sys == 'Linux':
                os.system(f"xdg-open {path}")
    except Exception as e:
        logger.error(f"æ‰“å¼€ä»»åŠ¡æ–‡ä»¶å¤¹å¤±è´¥: {e}")

def cleanup_temp_files(temp_dir, max_age=3600):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    Args:
        temp_dir: ä¸´æ—¶æ–‡ä»¶ç›®å½•
        max_age: æ–‡ä»¶æœ€å¤§ä¿å­˜æ—¶é—´(ç§’)
    """
    if os.path.exists(temp_dir):
        for file in os.listdir(temp_dir):
            file_path = os.path.join(temp_dir, file)
            try:
                if os.path.getctime(file_path) < time.time() - max_age:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                    elif os.path.isdir(file_path):
                        shutil.rmtree(file_path)
                    logger.debug(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")

def get_file_list(directory, file_types=None, sort_by='ctime', reverse=True):
    """è·å–æŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶åˆ—è¡¨
    Args:
        directory: ç›®å½•è·¯å¾„
        file_types: æ–‡ä»¶ç±»å‹åˆ—è¡¨ï¼Œå¦‚ ['.mp4', '.mov']
        sort_by: æ’åºæ–¹å¼ï¼Œæ”¯æŒ 'ctime'(åˆ›å»ºæ—¶é—´), 'mtime'(ä¿®æ”¹æ—¶é—´), 'size'(æ–‡ä»¶å¤§å°), 'name'(æ–‡ä»¶å)
        reverse: æ˜¯å¦å€’åºæ’åº
    Returns:
        list: æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
    """
    if not os.path.exists(directory):
        return []
    
    files = []
    if file_types:
        for file_type in file_types:
            files.extend(glob.glob(os.path.join(directory, f"*{file_type}")))
    else:
        files = glob.glob(os.path.join(directory, "*"))
    
    file_list = []
    for file_path in files:
        try:
            file_stat = os.stat(file_path)
            file_info = {
                "name": os.path.basename(file_path),
                "path": file_path,
                "size": file_stat.st_size,
                "ctime": file_stat.st_ctime,
                "mtime": file_stat.st_mtime
            }
            file_list.append(file_info)
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {file_path}, é”™è¯¯: {e}")
    
    # æ’åº
    if sort_by in ['ctime', 'mtime', 'size', 'name']:
        file_list.sort(key=lambda x: x.get(sort_by, ''), reverse=reverse)
    
    return file_list

def save_uploaded_file(uploaded_file, save_dir, allowed_types=None):
    """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    Args:
        uploaded_file: StreamlitUploadedFileå¯¹è±¡
        save_dir: ä¿å­˜ç›®å½•
        allowed_types: å…è®¸çš„æ–‡ä»¶ç±»å‹åˆ—è¡¨ï¼Œå¦‚ ['.mp4', '.mov']
    Returns:
        str: ä¿å­˜åçš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        
        file_name, file_extension = os.path.splitext(uploaded_file.name)
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if allowed_types and file_extension.lower() not in allowed_types:
            logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}")
            return None
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
        save_path = os.path.join(save_dir, uploaded_file.name)
        if os.path.exists(save_path):
            timestamp = time.strftime("%Y%m%d%H%M%S")
            new_file_name = f"{file_name}_{timestamp}{file_extension}"
            save_path = os.path.join(save_dir, new_file_name)
        
        # ä¿å­˜æ–‡ä»¶
        with open(save_path, "wb") as f:
            f.write(uploaded_file.read())
        
        logger.info(f"æ–‡ä»¶ä¿å­˜æˆåŠŸ: {save_path}")
        return save_path
    
    except Exception as e:
        logger.error(f"ä¿å­˜ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {e}")
        return None

def create_temp_file(prefix='tmp', suffix='', directory=None):
    """åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    Args:
        prefix: æ–‡ä»¶åå‰ç¼€
        suffix: æ–‡ä»¶æ‰©å±•å
        directory: ä¸´æ—¶æ–‡ä»¶ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨ç³»ç»Ÿä¸´æ—¶ç›®å½•
    Returns:
        str: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    """
    try:
        if directory is None:
            directory = utils.storage_dir("temp", create=True)
        
        if not os.path.exists(directory):
            os.makedirs(directory)
        
        temp_file = os.path.join(directory, f"{prefix}-{str(uuid4())}{suffix}")
        return temp_file
    
    except Exception as e:
        logger.error(f"åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        return None

def get_file_size(file_path, format='MB'):
    """è·å–æ–‡ä»¶å¤§å°
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        format: è¿”å›æ ¼å¼ï¼Œæ”¯æŒ 'B', 'KB', 'MB', 'GB'
    Returns:
        float: æ–‡ä»¶å¤§å°
    """
    try:
        size_bytes = os.path.getsize(file_path)
        
        if format.upper() == 'B':
            return size_bytes
        elif format.upper() == 'KB':
            return size_bytes / 1024
        elif format.upper() == 'MB':
            return size_bytes / (1024 * 1024)
        elif format.upper() == 'GB':
            return size_bytes / (1024 * 1024 * 1024)
        else:
            return size_bytes
    
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return 0

def ensure_directory(directory):
    """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    Args:
        directory: ç›®å½•è·¯å¾„
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
        return True
    except Exception as e:
        logger.error(f"åˆ›å»ºç›®å½•å¤±è´¥: {directory}, é”™è¯¯: {e}")
        return False

def create_zip(files: list, zip_path: str, base_dir: str = None, folder_name: str = "demo") -> bool:
    """
    åˆ›å»ºzipæ–‡ä»¶
    Args:
        files: è¦æ‰“åŒ…çš„æ–‡ä»¶åˆ—è¡¨
        zip_path: zipæ–‡ä»¶ä¿å­˜è·¯å¾„
        base_dir: åŸºç¡€ç›®å½•ï¼Œç”¨äºä¿æŒç›®å½•ç»“æ„
        folder_name: zipè§£å‹åçš„æ–‡ä»¶å¤¹åç§°ï¼Œé»˜è®¤ä¸ºframes
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        import zipfile
        
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(zip_path), exist_ok=True)
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file in files:
                if not os.path.exists(file):
                    logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file}")
                    continue
                    
                # è®¡ç®—æ–‡ä»¶åœ¨zipä¸­çš„è·¯å¾„ï¼Œæ·»åŠ folder_nameä½œä¸ºå‰ç¼€ç›®å½•
                if base_dir:
                    arcname = os.path.join(folder_name, os.path.relpath(file, base_dir))
                else:
                    arcname = os.path.join(folder_name, os.path.basename(file))
                
                try:
                    zipf.write(file, arcname)
                except Exception as e:
                    logger.error(f"æ·»åŠ æ–‡ä»¶åˆ°zipå¤±è´¥: {file}, é”™è¯¯: {e}")
                    continue

        return True
        
    except Exception as e:
        logger.error(f"åˆ›å»ºzipæ–‡ä»¶å¤±è´¥: {e}")
        return False
</file>

<file path="webui/utils/merge_video.py">
"""
åˆå¹¶è§†é¢‘å’Œå­—å¹•æ–‡ä»¶
"""
from moviepy.editor import VideoFileClip, concatenate_videoclips
import pysrt
import os


def get_video_duration(video_path):
    """è·å–è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰"""
    video = VideoFileClip(video_path)
    duration = video.duration
    video.close()
    return duration


def adjust_subtitle_timing(subtitle_path, time_offset):
    """è°ƒæ•´å­—å¹•æ—¶é—´æˆ³"""
    subs = pysrt.open(subtitle_path)

    # ä¸ºæ¯ä¸ªå­—å¹•é¡¹æ·»åŠ æ—¶é—´åç§»
    for sub in subs:
        sub.start.hours += int(time_offset / 3600)
        sub.start.minutes += int((time_offset % 3600) / 60)
        sub.start.seconds += int(time_offset % 60)
        sub.start.milliseconds += int((time_offset * 1000) % 1000)

        sub.end.hours += int(time_offset / 3600)
        sub.end.minutes += int((time_offset % 3600) / 60)
        sub.end.seconds += int(time_offset % 60)
        sub.end.milliseconds += int((time_offset * 1000) % 1000)

    return subs


def merge_videos_and_subtitles(video_paths, subtitle_paths, output_video_path, output_subtitle_path):
    """åˆå¹¶è§†é¢‘å’Œå­—å¹•æ–‡ä»¶"""
    if len(video_paths) != len(subtitle_paths):
        raise ValueError("è§†é¢‘æ–‡ä»¶æ•°é‡ä¸å­—å¹•æ–‡ä»¶æ•°é‡ä¸åŒ¹é…")

    # 1. åˆå¹¶è§†é¢‘
    video_clips = []
    accumulated_duration = 0
    merged_subs = pysrt.SubRipFile()

    try:
        # å¤„ç†æ‰€æœ‰è§†é¢‘å’Œå­—å¹•
        for i, (video_path, subtitle_path) in enumerate(zip(video_paths, subtitle_paths)):
            # æ·»åŠ è§†é¢‘
            print(f"å¤„ç†è§†é¢‘ {i + 1}/{len(video_paths)}: {video_path}")
            video_clip = VideoFileClip(video_path)
            video_clips.append(video_clip)

            # å¤„ç†å­—å¹•
            print(f"å¤„ç†å­—å¹• {i + 1}/{len(subtitle_paths)}: {subtitle_path}")
            if i == 0:
                # ç¬¬ä¸€ä¸ªå­—å¹•æ–‡ä»¶ç›´æ¥è¯»å–
                current_subs = pysrt.open(subtitle_path)
            else:
                # åç»­å­—å¹•æ–‡ä»¶éœ€è¦è°ƒæ•´æ—¶é—´æˆ³
                current_subs = adjust_subtitle_timing(subtitle_path, accumulated_duration)

            # åˆå¹¶å­—å¹•
            merged_subs.extend(current_subs)

            # æ›´æ–°ç´¯è®¡æ—¶é•¿
            accumulated_duration += video_clip.duration

        # åˆ¤æ–­è§†é¢‘æ˜¯å¦å­˜åœ¨ï¼Œè‹¥å·²ç»å­˜åœ¨ä¸é‡å¤åˆå¹¶
        if not os.path.exists(output_video_path):
            print("åˆå¹¶è§†é¢‘ä¸­...")
            final_video = concatenate_videoclips(video_clips)

            # ä¿å­˜åˆå¹¶åçš„è§†é¢‘
            print("ä¿å­˜åˆå¹¶åçš„è§†é¢‘...")
            final_video.write_videofile(output_video_path, audio_codec='aac')

        # ä¿å­˜åˆå¹¶åçš„å­—å¹•
        print("ä¿å­˜åˆå¹¶åçš„å­—å¹•...")
        merged_subs.save(output_subtitle_path, encoding='utf-8')

        print("åˆå¹¶å®Œæˆ")

    finally:
        # æ¸…ç†èµ„æº
        for clip in video_clips:
            clip.close()


def main():
    # ç¤ºä¾‹ç”¨æ³•
    video_paths = [
        "temp/1.mp4",
        "temp/2.mp4",
        "temp/3.mp4",
        "temp/4.mp4",
        "temp/5.mp4",
    ]

    subtitle_paths = [
        "temp/1.srt",
        "temp/2.srt",
        "temp/3.srt",
        "temp/4.srt",
        "temp/5.srt",
    ]

    output_video_path = "temp/merged_video.mp4"
    output_subtitle_path = "temp/merged_subtitle.srt"

    merge_videos_and_subtitles(video_paths, subtitle_paths, output_video_path, output_subtitle_path)


if __name__ == "__main__":
    main()
</file>

<file path="webui/utils/performance.py">
import psutil
import os
from loguru import logger
import torch

class PerformanceMonitor:
    @staticmethod
    def monitor_memory():
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        
        logger.debug(f"Memory usage: {memory_info.rss / 1024 / 1024:.2f} MB")
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024
            logger.debug(f"GPU Memory usage: {gpu_memory:.2f} MB")
    
    @staticmethod
    def cleanup_resources():
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        import gc
        gc.collect()
        
        PerformanceMonitor.monitor_memory()

def monitor_performance(func):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        try:
            PerformanceMonitor.monitor_memory()
            result = func(*args, **kwargs)
            return result
        finally:
            PerformanceMonitor.cleanup_resources()
    return wrapper
</file>

<file path="webui/utils/vision_analyzer.py">
import logging
from typing import List, Dict, Any, Optional
from app.utils import gemini_analyzer, qwenvl_analyzer

logger = logging.getLogger(__name__)

class VisionAnalyzer:
    def __init__(self):
        self.provider = None
        self.api_key = None
        self.model = None
        self.base_url = None
        self.analyzer = None
        
    def initialize_gemini(self, api_key: str, model: str, base_url: str) -> None:
        """
        åˆå§‹åŒ–Geminiè§†è§‰åˆ†æå™¨
        
        Args:
            api_key: Gemini APIå¯†é’¥
            model: æ¨¡å‹åç§°
            base_url: APIåŸºç¡€URL
        """
        self.provider = 'gemini'
        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        self.analyzer = gemini_analyzer.VisionAnalyzer(
            model_name=model,
            api_key=api_key
        )

    def initialize_qwenvl(self, api_key: str, model: str, base_url: str) -> None:
        """
        åˆå§‹åŒ–QwenVLè§†è§‰åˆ†æå™¨
        
        Args:
            api_key: é˜¿é‡Œäº‘APIå¯†é’¥
            model: æ¨¡å‹åç§°
            base_url: APIåŸºç¡€URL
        """
        self.provider = 'qwenvl'
        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        self.analyzer = qwenvl_analyzer.QwenAnalyzer(
            model_name=model,
            api_key=api_key
        )
        
    async def analyze_images(self, images: List[str], prompt: str, batch_size: int = 5) -> Dict[str, Any]:
        """
        åˆ†æå›¾ç‰‡å†…å®¹
        
        Args:
            images: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            prompt: åˆ†ææç¤ºè¯
            batch_size: æ¯æ‰¹å¤„ç†çš„å›¾ç‰‡æ•°é‡ï¼Œé»˜è®¤ä¸º5
            
        Returns:
            Dict: åˆ†æç»“æœ
        """
        if not self.analyzer:
            raise ValueError("æœªåˆå§‹åŒ–è§†è§‰åˆ†æå™¨")
            
        return await self.analyzer.analyze_images(
            images=images,
            prompt=prompt,
            batch_size=batch_size
        )

def create_vision_analyzer(provider: str, **kwargs) -> VisionAnalyzer:
    """
    åˆ›å»ºè§†è§‰åˆ†æå™¨å®ä¾‹
    
    Args:
        provider: æä¾›å•†åç§° ('gemini' æˆ– 'qwenvl')
        **kwargs: æä¾›å•†ç‰¹å®šçš„é…ç½®å‚æ•°
        
    Returns:
        VisionAnalyzer: é…ç½®å¥½çš„è§†è§‰åˆ†æå™¨å®ä¾‹
    """
    analyzer = VisionAnalyzer()
    
    if provider.lower() == 'gemini':
        analyzer.initialize_gemini(
            api_key=kwargs.get('api_key'),
            model=kwargs.get('model'),
            base_url=kwargs.get('base_url')
        )
    elif provider.lower() == 'qwenvl':
        analyzer.initialize_qwenvl(
            api_key=kwargs.get('api_key'),
            model=kwargs.get('model'),
            base_url=kwargs.get('base_url')
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è§†è§‰åˆ†ææä¾›å•†: {provider}")
        
    return analyzer
</file>

<file path="webui/__init__.py">
"""
NarratoAI WebUI Package
"""
from webui.config.settings import config
from webui.components import (
    basic_settings,
    video_settings,
    audio_settings,
    subtitle_settings
)
from webui.utils import cache, file_utils, performance

__all__ = [
    'config',
    'basic_settings',
    'video_settings',
    'audio_settings',
    'subtitle_settings',
    'cache',
    'file_utils',
    'performance'
]
</file>

<file path=".dockerignore">
# Exclude common Python files and directories
venv/
__pycache__/
*.pyc
*.pyo
*.pyd
*.pyz
*.pyw
*.pyi
*.egg-info/

# Exclude development and local files
.env
.env.*
*.log
*.db

# Exclude version control system files
.git/
.gitignore
.svn/

storage/
config.toml
</file>

<file path=".gitignore">
.DS_Store
/config.toml
/storage/
/.idea/
/app/services/__pycache__
/app/__pycache__/
/app/config/__pycache__/
/app/models/__pycache__/
/app/utils/__pycache__/
/*/__pycache__/*
.vscode
/**/.streamlit
__pycache__
logs/

node_modules
# VuePress é»˜è®¤ä¸´æ—¶æ–‡ä»¶ç›®å½•
/sites/docs/.vuepress/.temp
# VuePress é»˜è®¤ç¼“å­˜ç›®å½•
/sites/docs/.vuepress/.cache
# VuePress é»˜è®¤æ„å»ºç”Ÿæˆçš„é™æ€æ–‡ä»¶ç›®å½•
/sites/docs/.vuepress/dist
# æ¨¡å‹ç›®å½•
/models/
./models/*
resource/scripts/*.json
resource/videos/*.mp4
resource/songs/*.mp3
resource/songs/*.flac
resource/fonts/*.ttc
resource/fonts/*.ttf
resource/fonts/*.otf
resource/srt/*.srt
app/models/faster-whisper-large-v2/*
app/models/bert/*
</file>

<file path="changelog.py">
from git_changelog.cli import build_and_render

# è¿è¡Œè¿™æ®µè„šæœ¬è‡ªåŠ¨ç”ŸæˆCHANGELOG.mdæ–‡ä»¶

build_and_render(
    repository=".",
    output="CHANGELOG.md",
    convention="angular",
    provider="github",
    template="keepachangelog",
    parse_trailers=True,
    parse_refs=False,
    sections=["build", "deps", "feat", "fix", "refactor"],
    versioning="pep440",
    bump="1.1.2",  # æŒ‡å®šbumpç‰ˆæœ¬
    in_place=True,
)
</file>

<file path="config.example.toml">
[app]
    project_version="0.5.2"
    # æ”¯æŒè§†é¢‘ç†è§£çš„å¤§æ¨¡å‹æä¾›å•†
    #   gemini
    #   NarratoAPI
    #   qwen2-vl (å¾…å¢åŠ )
    vision_llm_provider="gemini"
    vision_analysis_prompt = "ä½ æ˜¯èµ„æ·±è§†é¢‘å†…å®¹åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ†æè§†é¢‘ç”»é¢ä¿¡æ¯ï¼Œåˆ†æä¸‹é¢è§†é¢‘ç”»é¢å†…å®¹ï¼Œåªè¾“å‡ºå®¢è§‚çš„ç”»é¢æè¿°ä¸è¦ç»™ä»»ä½•æ€»ç»“æˆ–è¯„ä»·"

    ########## Vision Gemini API Key
    vision_gemini_api_key = ""
    vision_gemini_model_name = "gemini-1.5-flash"

    ########## Vision Qwen API Key
    vision_qwenvl_api_key = ""
    vision_qwenvl_model_name = "qwen-vl-max-latest"
    vision_qwenvl_base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

    ########### Vision NarratoAPI Key
    narrato_api_key = "ggyY91BAO-_ULvAqKum3XexcyN1G3dP86DEzvjZDcrg"
    narrato_api_url = "https://narratoinsight.scsmtech.cn/api/v1"
    narrato_vision_model = "gemini-1.5-flash"
    narrato_vision_key = ""
    narrato_llm_model = "gpt-4o"
    narrato_llm_key = ""

    # ç”¨äºç”Ÿæˆæ–‡æ¡ˆçš„å¤§æ¨¡å‹æ”¯æŒçš„æä¾›å•† (Supported providers):
    #   openai (é»˜è®¤)
    #   moonshot (æœˆä¹‹æš—é¢)
    #   oneapi
    #   g4f
    #   azure
    #   qwen (é€šä¹‰åƒé—®)
    #   gemini
    text_llm_provider="openai"

    ########## OpenAI API Key
    # Get your API key at https://platform.openai.com/api-keys
    text_openai_api_key = ""
    text_openai_base_url = "https://api.openai.com/v1"
    text_openai_model_name = "gpt-4o-mini"

    ########## Moonshot API Key
    # Visit https://platform.moonshot.cn/console/api-keys to get your API key.
    text_moonshot_api_key=""
    text_moonshot_base_url = "https://api.moonshot.cn/v1"
    text_moonshot_model_name = "moonshot-v1-8k"

    ########## G4F
    # Visit https://github.com/xtekky/gpt4free to get more details
    # Supported model list: https://github.com/xtekky/gpt4free/blob/main/g4f/models.py
    text_g4f_model_name = "gpt-3.5-turbo"

    ########## Azure API Key
    # Visit https://learn.microsoft.com/zh-cn/azure/ai-services/openai/ to get more details
    # API documentation: https://learn.microsoft.com/zh-cn/azure/ai-services/openai/reference
    text_azure_api_key = ""
    text_azure_base_url=""
    text_azure_model_name="gpt-35-turbo" # replace with your model deployment name
    text_azure_api_version = "2024-02-15-preview"

    ########## Gemini API Key
    text_gemini_api_key=""
    text_gemini_model_name = "gemini-1.5-flash"

    ########## Qwen API Key
    # Visit https://dashscope.console.aliyun.com/apiKey to get your API key
    # Visit below links to get more details
    # https://tongyi.aliyun.com/qianwen/
    # https://help.aliyun.com/zh/dashscope/developer-reference/model-introduction
    text_qwen_api_key = ""
    text_qwen_model_name = "qwen-plus-1127"
    text_qwen_base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

    ########## DeepSeek API Key
    # ä½¿ç”¨ ç¡…åŸºæµåŠ¨ ç¬¬ä¸‰æ–¹ API Keyï¼Œä½¿ç”¨æ‰‹æœºå·æ³¨å†Œï¼šhttps://cloud.siliconflow.cn/i/pyOKqFCV
    text_deepseek_api_key = ""
    text_deepseek_base_url = "https://api.siliconflow.cn/v1"
    text_deepseek_model_name = "deepseek-ai/DeepSeek-V3"

    # å­—å¹•æä¾›å•†ã€å¯é€‰ï¼Œæ”¯æŒ whisper å’Œ faster-whisper-large-v2"whisper"
    # é»˜è®¤ä¸º faster-whisper-large-v2 æ¨¡å‹åœ°å€ï¼šhttps://huggingface.co/guillaumekln/faster-whisper-large-v2
    subtitle_provider = "faster-whisper-large-v2"
    subtitle_enabled = true

    # ImageMagick
    # å®‰è£…åï¼Œå°†è‡ªåŠ¨æ£€æµ‹åˆ° ImageMagickï¼ŒWindows é™¤å¤–ï¼
    # ä¾‹å¦‚ï¼Œåœ¨ Windows ä¸Š "C:\Program Files (x86)\ImageMagick-7.1.1-Q16-HDRI\magick.exe"
    # ä¸‹è½½ä½ç½® https://imagemagick.org/archive/binaries/ImageMagick-7.1.1-29-Q16-x64-static.exe
    # imagemagick_path = "C:\\Program Files (x86)\\ImageMagick-7.1.1-Q16\\magick.exe"

    # FFMPEG
    #
    # é€šå¸¸æƒ…å†µä¸‹ï¼Œffmpeg ä¼šè¢«è‡ªåŠ¨ä¸‹è½½ï¼Œå¹¶ä¸”ä¼šè¢«è‡ªåŠ¨æ£€æµ‹åˆ°ã€‚
    # ä½†æ˜¯å¦‚æœä½ çš„ç¯å¢ƒæœ‰é—®é¢˜ï¼Œæ— æ³•è‡ªåŠ¨ä¸‹è½½ï¼Œå¯èƒ½ä¼šé‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š
    #   RuntimeError: No ffmpeg exe could be found.
    #   Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.
    # æ­¤æ—¶ä½ å¯ä»¥æ‰‹åŠ¨ä¸‹è½½ ffmpeg å¹¶è®¾ç½® ffmpeg_pathï¼Œä¸‹è½½åœ°å€ï¼šhttps://www.gyan.dev/ffmpeg/builds/

    # ffmpeg_path = "C:\\Users\\harry\\Downloads\\ffmpeg.exe"
    #########################################################################################

    # å½“è§†é¢‘ç”ŸæˆæˆåŠŸåï¼ŒAPIæœåŠ¡æä¾›çš„è§†é¢‘ä¸‹è½½æ¥å…¥ç‚¹ï¼Œé»˜è®¤ä¸ºå½“å‰æœåŠ¡çš„åœ°å€å’Œç›‘å¬ç«¯å£
    # æ¯”å¦‚ http://127.0.0.1:8080/tasks/6357f542-a4e1-46a1-b4c9-bf3bd0df5285/final-1.mp4
    # å¦‚æœä½ éœ€è¦ä½¿ç”¨åŸŸåå¯¹å¤–æä¾›æœåŠ¡ï¼ˆä¸€èˆ¬ä¼šç”¨nginxåšä»£ç†ï¼‰ï¼Œåˆ™å¯ä»¥è®¾ç½®ä¸ºä½ çš„åŸŸå
    # æ¯”å¦‚ https://xxxx.com/tasks/6357f542-a4e1-46a1-b4c9-bf3bd0df5285/final-1.mp4
    # endpoint="https://xxxx.com"

    # When the video is successfully generated, the API service provides a download endpoint for the video, defaulting to the service's current address and listening port.
    # For example, http://127.0.0.1:8080/tasks/6357f542-a4e1-46a1-b4c9-bf3bd0df5285/final-1.mp4
    # If you need to provide the service externally using a domain name (usually done with nginx as a proxy), you can set it to your domain name.
    # For example, https://xxxx.com/tasks/6357f542-a4e1-46a1-b4c9-bf3bd0df5285/final-1.mp4
    # endpoint="https://xxxx.com"
    endpoint=""


    # Video material storage location
    # material_directory = ""                    # Indicates that video materials will be downloaded to the default folder, the default folder is ./storage/cache_videos under the current project
    # material_directory = "/user/harry/videos"  # Indicates that video materials will be downloaded to a specified folder
    # material_directory = "task"                # Indicates that video materials will be downloaded to the current task's folder, this method does not allow sharing of already downloaded video materials

    # è§†é¢‘ç´ æå­˜æ”¾ä½ç½®
    # material_directory = ""                    #è¡¨ç¤ºå°†è§†é¢‘ç´ æä¸‹è½½åˆ°é»˜è®¤çš„æ–‡ä»¶å¤¹ï¼Œé»˜è®¤æ–‡ä»¶å¤¹ä¸ºå½“å‰é¡¹ç›®ä¸‹çš„ ./storage/cache_videos
    # material_directory = "/user/harry/videos"  #è¡¨ç¤ºå°†è§†é¢‘ç´ æä¸‹è½½åˆ°æŒ‡å®šçš„æ–‡ä»¶å¤¹ä¸­
    # material_directory = "task"                #è¡¨ç¤ºå°†è§†é¢‘ç´ æä¸‹è½½åˆ°å½“å‰ä»»åŠ¡çš„æ–‡ä»¶å¤¹ä¸­ï¼Œè¿™ç§æ–¹å¼æ— æ³•å…±äº«å·²ç»ä¸‹è½½çš„è§†é¢‘ç´ æ

    material_directory = ""

    # ç”¨äºä»»åŠ¡çš„çŠ¶æ€ç®¡ç†
    enable_redis = false
    redis_host = "localhost"
    redis_port = 6379
    redis_db = 0
    redis_password = ""

    # æ–‡ç”Ÿè§†é¢‘æ—¶çš„æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
    max_concurrent_tasks = 5

    # webuiç•Œé¢æ˜¯å¦æ˜¾ç¤ºé…ç½®é¡¹
    hide_config = false


[whisper]
    # Only effective when subtitle_provider is "whisper"

    # Run on GPU with FP16
    # model = WhisperModel(model_size, device="cuda", compute_type="float16")

    # Run on GPU with INT8
    # model = WhisperModel(model_size, device="cuda", compute_type="int8_float16")

    # Run on CPU with INT8
    # model = WhisperModel(model_size, device="cpu", compute_type="int8")

    # recommended model_size: "large-v3"
    model_size="faster-whisper-large-v2"
    # å¦‚æœè¦ä½¿ç”¨ GPUï¼Œè¯·è®¾ç½® device=â€œcudaâ€
    device="CPU"
    compute_type="int8"


[proxy]
    ### Use a proxy to access the Pexels API
    ### Format: "http://<username>:<password>@<proxy>:<port>"
    ### Example: "http://user:pass@proxy:1234"
    ### Doc: https://requests.readthedocs.io/en/latest/user/advanced/#proxies

    http = "http://127.0.0.1:7890"
    https = "http://127.0.0.1:7890"

[azure]
    # Azure Speech API Key
    # Get your API key at https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices
    speech_key=""
    speech_region=""

[frames]
    skip_seconds = 0
    # thresholdï¼ˆå·®å¼‚é˜ˆå€¼ï¼‰ç”¨äºåˆ¤æ–­ä¸¤ä¸ªè¿ç»­å¸§ä¹‹é—´æ˜¯å¦å‘ç”Ÿäº†åœºæ™¯åˆ‡æ¢
    # è¾ƒå°çš„é˜ˆå€¼ï¼ˆå¦‚ 20ï¼‰ï¼šæ›´æ•æ„Ÿï¼Œèƒ½æ•æ‰åˆ°ç»†å¾®çš„åœºæ™¯å˜åŒ–ï¼Œä½†å¯èƒ½ä¼šè¯¯åˆ¤ï¼Œå…³é”®å¸§å›¾ç‰‡æ›´å¤š
    # è¾ƒå¤§çš„é˜ˆå€¼ï¼ˆå¦‚ 40ï¼‰ï¼šæ›´ä¿å®ˆï¼Œåªæ•æ‰æ˜æ˜¾çš„åœºæ™¯åˆ‡æ¢ï¼Œä½†å¯èƒ½ä¼šæ¼æ‰æ¸å˜åœºæ™¯ï¼Œå…³é”®å¸§å›¾ç‰‡æ›´å°‘
    # é»˜è®¤å€¼ 30ï¼šåœ¨å®è·µä¸­æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¹³è¡¡çš„é€‰æ‹©
    threshold = 30
    version = "v2"
    # å¤§æ¨¡å‹å•æ¬¡å¤„ç†çš„å…³é”®å¸§æ•°é‡
    vision_batch_size = 5
</file>

<file path="docker-compose.yml">
x-common: &common
  build:
    context: .
    dockerfile: Dockerfile
  image: linyq1/narratoai:latest
  volumes:
    - ./:/NarratoAI
  environment:
    - VPN_PROXY_URL=http://host.docker.internal:7890
    - PYTHONUNBUFFERED=1
    - PYTHONMALLOC=malloc
    - OPENCV_OPENCL_RUNTIME=disabled
    - OPENCV_CPU_DISABLE=0
  restart: always
  mem_limit: 4g
  mem_reservation: 2g
  memswap_limit: 6g
  cpus: 2.0
  cpu_shares: 1024

services:
  webui:
    <<: *common
    container_name: webui
    ports:
      - "8501:8501"
    command: ["webui"]
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "3"
    tmpfs:
      - /tmp:size=1G
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
</file>

<file path="docker-entrypoint.sh">
#!/bin/bash
set -e

if [ "$1" = "webui" ]; then
    exec streamlit run webui.py --browser.serverAddress=127.0.0.1 --server.enableCORS=True --browser.gatherUsageStats=False
else
    exec "$@"
fi
</file>

<file path="Dockerfile">
# æ„å»ºé˜¶æ®µ
FROM python:3.10-slim-bullseye as builder

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /build

# å®‰è£…æ„å»ºä¾èµ–
RUN apt-get update && apt-get install -y \
    git \
    git-lfs \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# é¦–å…ˆå®‰è£… PyTorchï¼ˆå› ä¸ºå®ƒæ˜¯æœ€å¤§çš„ä¾èµ–ï¼‰
RUN pip install --no-cache-dir torch torchvision torchaudio

# ç„¶åå®‰è£…å…¶ä»–ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# è¿è¡Œé˜¶æ®µ
FROM python:3.10-slim-bullseye

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /NarratoAI

# ä»builderé˜¶æ®µå¤åˆ¶è™šæ‹Ÿç¯å¢ƒ
COPY --from=builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apt-get update && apt-get install -y \
    imagemagick \
    ffmpeg \
    wget \
    git-lfs \
    && rm -rf /var/lib/apt/lists/* \
    && sed -i '/<policy domain="path" rights="none" pattern="@\*"/d' /etc/ImageMagick-6/policy.xml

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH="/NarratoAI" \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

# è®¾ç½®ç›®å½•æƒé™
RUN chmod 777 /NarratoAI

# å®‰è£…git lfs
RUN git lfs install

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8501 8080

# ä½¿ç”¨è„šæœ¬ä½œä¸ºå…¥å£ç‚¹
COPY docker-entrypoint.sh /usr/local/bin/
RUN chmod +x /usr/local/bin/docker-entrypoint.sh
ENTRYPOINT ["docker-entrypoint.sh"]
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 linyq

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="main.py">
import os
import uvicorn
from loguru import logger

from app.config import config

if __name__ == "__main__":
    logger.info(
        "start server, docs: http://127.0.0.1:" + str(config.listen_port) + "/docs"
    )
    os.environ["HTTP_PROXY"] = config.proxy.get("http")
    os.environ["HTTPS_PROXY"] = config.proxy.get("https")
    uvicorn.run(
        app="app.asgi:app",
        host=config.listen_host,
        port=config.listen_port,
        reload=config.reload_debug,
        log_level="warning",
    )
</file>

<file path="README-cn.md">
<div align="center">
<h1 align="center" style="font-size: 2cm;"> NarratoAI ğŸ˜ğŸ“½ï¸ </h1>
<h3 align="center">An all-in-one AI-powered tool for film commentary and automated video editing.ğŸ¬ğŸï¸ </h3>


<h3>ğŸ“– English | <a href="README.md">ç®€ä½“ä¸­æ–‡</a> | <a href="README-ja.md">æ—¥æœ¬èª</a> </h3>
<div align="center">

[//]: # (  <a href="https://trendshift.io/repositories/8731" target="_blank"><img src="https://trendshift.io/api/badge/repositories/8731" alt="harry0703%2FNarratoAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>)
</div>
<br>
NarratoAI is an automated video narration tool that provides an all-in-one solution for script writing, automated video editing, voice-over, and subtitle generation, powered by LLM to enhance efficient content creation.
<br>

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&labelColor=orange)](https://github.com/linyqh/NarratoAI)
[![GitHub license](https://img.shields.io/github/license/linyqh/NarratoAI?style=for-the-badge)](https://github.com/linyqh/NarratoAI/blob/main/LICENSE)
[![GitHub issues](https://img.shields.io/github/issues/linyqh/NarratoAI?style=for-the-badge)](https://github.com/linyqh/NarratoAI/issues)
[![GitHub stars](https://img.shields.io/github/stars/linyqh/NarratoAI?style=for-the-badge)](https://github.com/linyqh/NarratoAI/stargazers)

<a href="https://github.com/linyqh/NarratoAI/wiki" target="_blank">ğŸ’¬ Join the open source community to get project updates and the latest news.</a>

<h3>Home</h3>

![](docs/index-en.png)

<h3>Video Review Interface</h3>

![](docs/check-en.png)

</div>

## Future Plans ğŸ¥³ 
- [x] Windows Integration Pack Release
- [ ] Optimized the story generation process and improved the generation effect 
- [ ] Support local large model MiniCPM-V 
- [ ] Support local large model Qwen2-VL 
- [ ] ...

## System Requirements ğŸ“¦

- Recommended minimum: CPU with 4 cores or more, 8GB RAM or more, GPU is not required
- Windows 10 or MacOS 11.0 or above

## Quick Start ğŸš€
### 1. Apply for Google AI Studio Account
1. Visit https://aistudio.google.com/app/prompts/new_chat to apply for an account.
2. Click `Get API Key` to request an API Key.
3. Enter the obtained API Key into the `gemini_api_key` setting in the `config.example.toml` file.

### 2. Configure Proxy VPN
> The method to configure VPN is not restricted, as long as you can access Google's network. Here, `clash` is used as an example.
1. Note the port of the clash service, usually `http://127.0.0.1:7890`.
2. If the port is not `7890`, modify the `VPN_PROXY_URL` in the `docker-compose.yml` file to your proxy address.
   ```yaml
   environment:
     - "VPN_PROXY_URL=http://host.docker.internal:7890" # Change to your proxy port; host.docker.internal represents the IP of the physical machine.
    ```

3. (Optional) Or modify the `proxy` settings in the `config.example.toml` file.
   ```toml
   [proxy]
    ### Use a proxy to access the Pexels API
    ### Format: "http://<username>:<password>@<proxy>:<port>"
    ### Example: "http://user:pass@proxy:1234"
    ### Doc: https://requests.readthedocs.io/en/latest/user/advanced/#proxies

    http = "http://xx.xx.xx.xx:7890"
    https = "http://xx.xx.xx.xx:7890"
   ```


### 3. Get Started ğŸ“¥ with the Modpack (for Windows users)
NarratoAI Modpack v0.1.2 is released ğŸš€ 

Hurry up and follow the WeChat public account [NarratoAIåŠ©æ‰‹] and reply to the keyword [æ•´åˆåŒ…] to get the latest download link! Give it a try! 

Note: 
- Currently only available for Windows, Mac version is in development, Linux version will be available in a future release.



### 4. Get started ğŸ³ with docker (for Mac and Linux users)
#### â‘  clone project, Start Docker
```shell
git clone https://github.com/linyqh/NarratoAI.git
cd NarratoAI
docker-compose up
```
#### â‘¡ Access the Web Interface

Open your browser and go to http://127.0.0.1:8501

#### â‘¢ Access the API Documentation

Open your browser and go to http://127.0.0.1:8080/docs or http://127.0.0.1:8080/redoc

## Usage
#### 1. Basic Configuration, Select Model, Enter API Key, and Choose Model
> Currently, only the `Gemini` model is supported. Other modes will be added in future updates. Contributions are welcome via [PR](https://github.com/linyqh/NarratoAI/pulls) to join in the development ğŸ‰ğŸ‰ğŸ‰
<div align="center">
  <img src="docs/img001-en.png" alt="001" width="1000"/>
</div>

#### 2. Select the Video for Narration and Click to Generate Video Script
> A demo video is included in the platform. To use your own video, place the mp4 file in the `resource/videos` directory and refresh your browser.
> Note: The filename can be anything, but it must not contain Chinese characters, special characters, spaces, backslashes, etc.
<div align="center">
  <img src="docs/img002-en.png" alt="002" width="400"/>
</div>

#### 3. Save the Script and Start Editing
> After saving the script, refresh the browser, and the newly generated `.json` script file will appear in the script file dropdown. Select the json file and video to start editing.
<div align="center">
  <img src="docs/img003-en.png" alt="003" width="400"/>
</div>

#### 4. Review the Video; if there are segments that don't meet the rules, click to regenerate or manually edit them.
<div align="center">
  <img src="docs/img004-en.png" alt="003" width="1000"/>
</div>

#### 5. Configure Basic Video Parameters
<div align="center">
  <img src="docs/img005-en.png" alt="003" width="700"/>
</div>

#### 6. Start Generating
<div align="center">
  <img src="docs/img006-en.png" alt="003" width="1000"/>
</div>

#### 7. Video Generation Complete
<div align="center">
  <img src="docs/img007-en.png" alt="003" width="1000"/>
</div>

## Development ğŸ’»
1. Install Dependencies
```shell
conda create -n narratoai python=3.10
conda activate narratoai
cd narratoai
pip install -r requirements.txt
```
2. Install ImageMagick
###### Windows:

- Download https://imagemagick.org/archive/binaries/ImageMagick-7.1.1-38-Q16-x64-static.exe
- Install the downloaded ImageMagick, ensuring you do not change the installation path
- Update `imagemagick_path` in the `config.toml` file to your actual installation path (typically `C:\Program Files\ImageMagick-7.1.1-Q16\magick.exe`)

###### MacOS:

```shell
brew install imagemagick
````

###### Ubuntu

```shell
sudo apt-get install imagemagick
```

###### CentOS

```shell
sudo yum install ImageMagick
```

3. initiate webui
```shell
streamlit run ./webui/Main.py --browser.serverAddress=127.0.0.1 --server.enableCORS=True --browser.gatherUsageStats=False
```
4. Access http://127.0.0.1:8501

## Feedback & Suggestions ğŸ“¢

### ğŸ‘ 1. You can submit [issues](https://github.com/linyqh/NarratoAI/issues) or [pull requests](https://github.com/linyqh/NarratoAI/pulls) 

### ğŸ’¬ 2. [Join the open source community exchange group]((https://github.com/linyqh/NarratoAI/wiki))

### ğŸ‘‰ 3. [frequently asked questions](https://thread-marsupial-df8.notion.site/105866888dab80988650fa063b1df4eb)

## Reference Projects ğŸ“š
- https://github.com/FujiwaraChoki/MoneyPrinter
- https://github.com/harry0703/MoneyPrinterTurbo

This project was refactored based on the above projects with the addition of video narration features. Thanks to the original authors for their open-source spirit ğŸ¥³ğŸ¥³ğŸ¥³ 

## License ğŸ“

Click to view the [`LICENSE`](LICENSE) file

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=linyqh/NarratoAI&type=Date)](https://star-history.com/#linyqh/NarratoAI&Date)
</file>

<file path="README-ja.md">
<div align="center">
<h1 align="center" style="font-size: 2cm;"> NarratoAI ğŸ˜ğŸ“½ï¸ </h1>
<h3 align="center">ä¸€ä½“å‹AIæ˜ ç”»è§£èª¬ãŠã‚ˆã³è‡ªå‹•ãƒ“ãƒ‡ã‚ªç·¨é›†ãƒ„ãƒ¼ãƒ«ğŸ¬ğŸï¸ </h3>

<h3>ğŸ“– <a href="README-cn.md">ç®€ä½“ä¸­æ–‡</a> | <a href="README.md">English</a> | æ—¥æœ¬èª </h3>
<div align="center">

[//]: # (  <a href="https://trendshift.io/repositories/8731" target="_blank"><img src="https://trendshift.io/api/badge/repositories/8731" alt="harry0703%2FNarratoAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>)
</div>
<br>
NarratoAIã¯ã€LLMã‚’æ´»ç”¨ã—ã¦ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆã€è‡ªå‹•ãƒ“ãƒ‡ã‚ªç·¨é›†ã€ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€å­—å¹•ç”Ÿæˆã®ä¸€ä½“å‹ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æä¾›ã™ã‚‹è‡ªå‹•åŒ–ãƒ“ãƒ‡ã‚ªãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
<br>

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&labelColor=orange)](https://github.com/linyqh/NarratoAI)
[![GitHub license](https://img.shields.io/github/license/linyqh/NarratoAI?style=for-the-badge)](https://github.com/linyqh/NarratoAI/blob/main/LICENSE)
[![GitHub issues](https://img.shields.io/github/issues/linyqh/NarratoAI?style=for-the-badge)](https://github.com/linyqh/NarratoAI/issues)
[![GitHub stars](https://img.shields.io/github/stars/linyqh/NarratoAI?style=for-the-badge)](https://github.com/linyqh/NarratoAI/stargazers)

<a href="https://discord.gg/uVAJftcm" target="_blank">ğŸ’¬ Discordã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«å‚åŠ ã—ã¦ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æœ€æ–°æƒ…å ±ã‚’å…¥æ‰‹ã—ã¾ã—ã‚‡ã†ã€‚</a>

<h2><a href="https://p9mf6rjv3c.feishu.cn/wiki/SP8swLLZki5WRWkhuFvc2CyInDg?from=from_copylink" target="_blank">ğŸ‰ğŸ‰ğŸ‰ å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ğŸ‰ğŸ‰ğŸ‰</a> </h2>
<h3>ãƒ›ãƒ¼ãƒ </h3>

![](docs/index-zh.png)

<h3>ãƒ“ãƒ‡ã‚ªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹</h3>

![](docs/check-zh.png)

</div>

## æœ€æ–°æƒ…å ±
- 2024.11.24 Discordã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£é–‹è¨­ï¼šhttps://discord.gg/uVAJftcm
- 2024.11.11 ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«ç§»è¡Œã€å‚åŠ ã‚’æ­“è¿ã—ã¾ã™ï¼ [å…¬å¼ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«å‚åŠ ](https://github.com/linyqh/NarratoAI/wiki)
- 2024.11.10 å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¬é–‹ã€è©³ç´°ã¯ [å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://p9mf6rjv3c.feishu.cn/wiki/SP8swLLZki5WRWkhuFvc2CyInDg) ã‚’å‚ç…§
- 2024.11.10 æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³v0.3.5ãƒªãƒªãƒ¼ã‚¹ï¼›ãƒ“ãƒ‡ã‚ªç·¨é›†ãƒ—ãƒ­ã‚»ã‚¹ã®æœ€é©åŒ–

## ä»Šå¾Œã®è¨ˆç”» ğŸ¥³
- [x] Windowsçµ±åˆãƒ‘ãƒƒã‚¯ãƒªãƒªãƒ¼ã‚¹
- [x] ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã®æœ€é©åŒ–ã€ç”ŸæˆåŠ¹æœã®å‘ä¸Š
- [x] ãƒãƒ¼ã‚¸ãƒ§ãƒ³0.3.5çµ±åˆãƒ‘ãƒƒã‚¯ãƒªãƒªãƒ¼ã‚¹
- [ ] ã‚¢ãƒªãƒãƒQwen2-VLå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ãƒ“ãƒ‡ã‚ªç†è§£ã‚µãƒãƒ¼ãƒˆ
- [ ] çŸ­ç·¨ãƒ‰ãƒ©ãƒã®è§£èª¬ã‚µãƒãƒ¼ãƒˆ
- [ ] ...

## ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ ğŸ“¦

- æ¨å¥¨æœ€ä½ï¼šCPU 4ã‚³ã‚¢ä»¥ä¸Šã€ãƒ¡ãƒ¢ãƒª8GBä»¥ä¸Šã€GPUã¯å¿…é ˆã§ã¯ã‚ã‚Šã¾ã›ã‚“
- Windows 10ã¾ãŸã¯MacOS 11.0ä»¥ä¸Š

## ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¨ææ¡ˆ ğŸ“¢

ğŸ‘ 1. [issue](https://github.com/linyqh/NarratoAI/issues)ã¾ãŸã¯[pull request](https://github.com/linyqh/NarratoAI/pulls)ã‚’æå‡ºã§ãã¾ã™

ğŸ’¬ 2. [ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£äº¤æµã‚°ãƒ«ãƒ¼ãƒ—ã«å‚åŠ ](https://github.com/linyqh/NarratoAI/wiki)

ğŸ“· 3. å…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã€NarratoAIåŠ©æ‰‹ã€‘ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ã—ã¦æœ€æ–°æƒ…å ±ã‚’å…¥æ‰‹

## å‚è€ƒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ ğŸ“š
- https://github.com/FujiwaraChoki/MoneyPrinter
- https://github.com/harry0703/MoneyPrinterTurbo

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ä¸Šè¨˜ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’åŸºã«ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã•ã‚Œã€æ˜ ç”»è§£èª¬æ©Ÿèƒ½ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ä½œè€…ã«æ„Ÿè¬ã—ã¾ã™ ğŸ¥³ğŸ¥³ğŸ¥³ 

## ä½œè€…ã«ã‚³ãƒ¼ãƒ’ãƒ¼ã‚’ä¸€æ¯ãŠã”ã‚‹ â˜•ï¸
<div style="display: flex; justify-content: space-between;">
  <img src="https://github.com/user-attachments/assets/5038ccfb-addf-4db1-9966-99415989fd0c" alt="Image 1" style="width: 350px; height: 350px; margin: auto;"/>
  <img src="https://github.com/user-attachments/assets/07d4fd58-02f0-425c-8b59-2ab94b4f09f8" alt="Image 2" style="width: 350px; height: 350px; margin: auto;"/>
</div>

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ ğŸ“

[`LICENSE`](LICENSE) ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦è¡¨ç¤º

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=linyqh/NarratoAI&type=Date)](https://star-history.com/#linyqh/NarratoAI&Date)
</file>

<file path="README.md">
<div align="center">
<h1 align="center" style="font-size: 2cm;"> NarratoAI ğŸ˜ğŸ“½ï¸ </h1>
<h3 align="center">ä¸€ç«™å¼ AI å½±è§†è§£è¯´+è‡ªåŠ¨åŒ–å‰ªè¾‘å·¥å…·ğŸ¬ğŸï¸ </h3>


<h3>ğŸ“– <a href="README-cn.md">English</a> | ç®€ä½“ä¸­æ–‡ | <a href="README-ja.md">æ—¥æœ¬èª</a> </h3>
<div align="center">

[//]: # (  <a href="https://trendshift.io/repositories/8731" target="_blank"><img src="https://trendshift.io/api/badge/repositories/8731" alt="harry0703%2FNarratoAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>)
</div>
<br>
NarratoAI æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–å½±è§†è§£è¯´å·¥å…·ï¼ŒåŸºäºLLMå®ç°æ–‡æ¡ˆæ’°å†™ã€è‡ªåŠ¨åŒ–è§†é¢‘å‰ªè¾‘ã€é…éŸ³å’Œå­—å¹•ç”Ÿæˆçš„ä¸€ç«™å¼æµç¨‹ï¼ŒåŠ©åŠ›é«˜æ•ˆå†…å®¹åˆ›ä½œã€‚
<br>

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&labelColor=orange)](https://github.com/linyqh/NarratoAI)
[![GitHub license](https://img.shields.io/github/license/linyqh/NarratoAI?style=for-the-badge)](https://github.com/linyqh/NarratoAI/blob/main/LICENSE)
[![GitHub issues](https://img.shields.io/github/issues/linyqh/NarratoAI?style=for-the-badge)](https://github.com/linyqh/NarratoAI/issues)
[![GitHub stars](https://img.shields.io/github/stars/linyqh/NarratoAI?style=for-the-badge)](https://github.com/linyqh/NarratoAI/stargazers)

<a href="https://discord.com/invite/V2pbAqqQNb" target="_blank">ğŸ’¬ åŠ å…¥ discord å¼€æºç¤¾åŒºï¼Œè·å–é¡¹ç›®åŠ¨æ€å’Œæœ€æ–°èµ„è®¯ã€‚</a>

<h2><a href="https://p9mf6rjv3c.feishu.cn/wiki/SP8swLLZki5WRWkhuFvc2CyInDg?from=from_copylink" target="_blank">ğŸ‰ğŸ‰ğŸ‰ å®˜æ–¹æ–‡æ¡£ ğŸ‰ğŸ‰ğŸ‰</a> </h2>
<h3>é¦–é¡µ</h3>

![](docs/index-zh.png)

<h3>è§†é¢‘å®¡æŸ¥ç•Œé¢</h3>

![](docs/check-zh.png)

</div>

## æœ€æ–°èµ„è®¯
- 2025.03.06 å‘å¸ƒæ–°ç‰ˆæœ¬ 0.5.2ï¼Œæ”¯æŒ DeepSeek R1 å’Œ DeepSeek V3 æ¨¡å‹è¿›è¡ŒçŸ­å‰§æ··å‰ª
- 2024.12.16 å‘å¸ƒæ–°ç‰ˆæœ¬ 0.3.9ï¼Œæ”¯æŒé˜¿é‡Œ Qwen2-VL æ¨¡å‹ç†è§£è§†é¢‘ï¼›æ”¯æŒçŸ­å‰§æ··å‰ª
- 2024.11.24 å¼€é€š discord ç¤¾ç¾¤ï¼šhttps://discord.com/invite/V2pbAqqQNb
- 2024.11.11 è¿ç§»å¼€æºç¤¾ç¾¤ï¼Œæ¬¢è¿åŠ å…¥ï¼ [åŠ å…¥å®˜æ–¹ç¤¾ç¾¤](https://github.com/linyqh/NarratoAI/wiki)
- 2024.11.10 å‘å¸ƒå®˜æ–¹æ–‡æ¡£ï¼Œè¯¦æƒ…å‚è§ [å®˜æ–¹æ–‡æ¡£](https://p9mf6rjv3c.feishu.cn/wiki/SP8swLLZki5WRWkhuFvc2CyInDg)
- 2024.11.10 å‘å¸ƒæ–°ç‰ˆæœ¬ v0.3.5ï¼›ä¼˜åŒ–è§†é¢‘å‰ªè¾‘æµç¨‹ï¼Œ

## é‡ç£…ç¦åˆ© ğŸ‰
å³æ—¥èµ·å…¨é¢æ”¯æŒDeepSeekæ¨¡å‹ï¼æ³¨å†Œå³äº«2000ä¸‡å…è´¹Tokenï¼ˆä»·å€¼14å…ƒå¹³å°é…é¢ï¼‰ï¼Œå‰ªè¾‘10åˆ†é’Ÿè§†é¢‘ä»…éœ€0.1å…ƒï¼  

ğŸ”¥ å¿«é€Ÿé¢†ç¦åˆ©ï¼š  
1ï¸âƒ£ ç‚¹å‡»é“¾æ¥æ³¨å†Œï¼šhttps://cloud.siliconflow.cn/i/pyOKqFCV  
2ï¸âƒ£ ä½¿ç”¨æ‰‹æœºå·ç™»å½•ï¼Œ**åŠ¡å¿…å¡«å†™é‚€è¯·ç ï¼špyOKqFCV**  
3ï¸âƒ£ é¢†å–14å…ƒé…é¢ï¼Œæé€Ÿä½“éªŒé«˜æ€§ä»·æ¯”AIå‰ªè¾‘  

ğŸ’¡ å°æˆæœ¬å¤§åˆ›ä½œï¼š  
ç¡…åŸºæµåŠ¨API Keyä¸€é”®æ¥å…¥ï¼Œæ™ºèƒ½å‰ªè¾‘æ•ˆç‡ç¿»å€ï¼  
ï¼ˆæ³¨ï¼šé‚€è¯·ç ä¸ºç¦åˆ©é¢†å–å”¯ä¸€å‡­è¯ï¼Œæ³¨å†Œåè‡ªåŠ¨åˆ°è´¦ï¼‰  

ç«‹å³è¡ŒåŠ¨ï¼Œç”¨ã€ŒpyOKqFCVã€è§£é”ä½ çš„AIç”Ÿäº§åŠ›ï¼

ğŸ˜Š æ›´æ–°æ­¥éª¤ï¼š
æ•´åˆåŒ…ï¼šç‚¹å‡» update.bat ä¸€é”®æ›´æ–°è„šæœ¬
ä»£ç æ„å»ºï¼šä½¿ç”¨ git pull æ‹‰å»æœ€æ–°ä»£ç 

## å…¬å‘Š ğŸ“¢
_**æ³¨æ„âš ï¸ï¼šè¿‘æœŸåœ¨ x (æ¨ç‰¹) ä¸Šå‘ç°æœ‰äººå†’å……ä½œè€…åœ¨ pump.fun å¹³å°ä¸Šå‘è¡Œä»£å¸ï¼ è¿™æ˜¯éª—å­ï¼ï¼ï¼ ä¸è¦è¢«å‰²äº†éŸ­èœ
ï¼ï¼ï¼ç›®å‰ NarratoAI æ²¡æœ‰åœ¨ x(æ¨ç‰¹) ä¸Šåšä»»ä½•å®˜æ–¹å®£ä¼ ï¼Œæ³¨æ„ç”„åˆ«**_

ä¸‹é¢æ˜¯æ­¤äºº x(æ¨ç‰¹) é¦–é¡µæˆªå›¾

<img src="https://github.com/user-attachments/assets/c492ab99-52cd-4ba2-8695-1bd2073ecf12" alt="Screenshot_20250109_114131_Samsung Internet" style="width:30%; height:auto;">

## æœªæ¥è®¡åˆ’ ğŸ¥³
- [x] windows æ•´åˆåŒ…å‘å¸ƒ
- [x] ä¼˜åŒ–å‰§æƒ…ç”Ÿæˆæµç¨‹ï¼Œæå‡ç”Ÿæˆæ•ˆæœ
- [x] å‘å¸ƒ 0.3.5 æ•´åˆåŒ…
- [x] æ”¯æŒé˜¿é‡Œ Qwen2-VL å¤§æ¨¡å‹ç†è§£è§†é¢‘
- [x] æ”¯æŒçŸ­å‰§æ··å‰ª
  - [x] ä¸€é”®åˆå¹¶ç´ æ
  - [x] ä¸€é”®è½¬å½•
  - [x] ä¸€é”®æ¸…ç†ç¼“å­˜
- [ ] æ”¯æŒå¯¼å‡ºå‰ªæ˜ è‰ç¨¿
- [ ] æ”¯æŒçŸ­å‰§è§£è¯´
- [ ] ä¸»è§’äººè„¸åŒ¹é…
- [ ] æ”¯æŒæ ¹æ®å£æ’­ï¼Œæ–‡æ¡ˆï¼Œè§†é¢‘ç´ æè‡ªåŠ¨åŒ¹é…
- [ ] ...

## é…ç½®è¦æ±‚ ğŸ“¦

- å»ºè®®æœ€ä½ CPU 4æ ¸æˆ–ä»¥ä¸Šï¼Œå†…å­˜ 8G æˆ–ä»¥ä¸Šï¼Œæ˜¾å¡éå¿…é¡»
- Windows 10 æˆ– MacOS 11.0 ä»¥ä¸Šç³»ç»Ÿ
- [Python 3.10+](https://www.python.org/downloads/)

## åé¦ˆå»ºè®® ğŸ“¢

ğŸ‘ 1. å¯ä»¥æäº¤ [issue](https://github.com/linyqh/NarratoAI/issues)æˆ–è€… [pull request](https://github.com/linyqh/NarratoAI/pulls)

ğŸ’¬ 2. [åŠ å…¥å¼€æºç¤¾åŒºäº¤æµç¾¤](https://github.com/linyqh/NarratoAI/wiki)

ğŸ“· 3. å…³æ³¨å…¬ä¼—å·ã€NarratoAIåŠ©æ‰‹ã€‘ï¼ŒæŒæ¡æœ€æ–°èµ„è®¯

## å‚è€ƒé¡¹ç›® ğŸ“š
- https://github.com/FujiwaraChoki/MoneyPrinter
- https://github.com/harry0703/MoneyPrinterTurbo

è¯¥é¡¹ç›®åŸºäºä»¥ä¸Šé¡¹ç›®é‡æ„è€Œæ¥ï¼Œå¢åŠ äº†å½±è§†è§£è¯´åŠŸèƒ½ï¼Œæ„Ÿè°¢å¤§ä½¬çš„å¼€æºç²¾ç¥ ğŸ¥³ğŸ¥³ğŸ¥³ 

## è¯·ä½œè€…å–ä¸€æ¯å’–å•¡ â˜•ï¸
<div style="display: flex; justify-content: space-between;">
  <img src="https://github.com/user-attachments/assets/5038ccfb-addf-4db1-9966-99415989fd0c" alt="Image 1" style="width: 350px; height: 350px; margin: auto;"/>
  <img src="https://github.com/user-attachments/assets/07d4fd58-02f0-425c-8b59-2ab94b4f09f8" alt="Image 2" style="width: 350px; height: 350px; margin: auto;"/>
</div>

## è®¸å¯è¯ ğŸ“

ç‚¹å‡»æŸ¥çœ‹ [`LICENSE`](LICENSE) æ–‡ä»¶

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=linyqh/NarratoAI&type=Date)](https://star-history.com/#linyqh/NarratoAI&Date)
</file>

<file path="release-notes.md">
# Release Notes

## Latest Changes

* Dev-0.3.9. PR [#73](https://github.com/linyqh/NarratoAI/pull/73) by [@linyqh](https://github.com/linyqh).
* 0.3.9 ç‰ˆæœ¬å‘å¸ƒ. PR [#71](https://github.com/linyqh/NarratoAI/pull/71) by [@linyqh](https://github.com/linyqh).
* docs: add Japanese README. PR [#66](https://github.com/linyqh/NarratoAI/pull/66) by [@eltociear](https://github.com/eltociear).
* docs: æµ‹è¯• release 2. PR [#62](https://github.com/linyqh/NarratoAI/pull/62) by [@linyqh](https://github.com/linyqh).
* docs: æµ‹è¯• release. PR [#61](https://github.com/linyqh/NarratoAI/pull/61) by [@linyqh](https://github.com/linyqh).
* docs: æµ‹è¯•commit. PR [#60](https://github.com/linyqh/NarratoAI/pull/60) by [@linyqh](https://github.com/linyqh).
* Dev. PR [#59](https://github.com/linyqh/NarratoAI/pull/59) by [@linyqh](https://github.com/linyqh).
* 0.2.0æ–°ç‰ˆé¢„å‘å¸ƒ. PR [#37](https://github.com/linyqh/NarratoAI/pull/37) by [@linyqh](https://github.com/linyqh).
* v0.3.6. PR [#58](https://github.com/linyqh/NarratoAI/pull/58) by [@linyqh](https://github.com/linyqh).
* 0.3.4 ä¿®æ”¹å„ç§bug. PR [#49](https://github.com/linyqh/NarratoAI/pull/49) by [@linyqh](https://github.com/linyqh).
</file>

<file path="requirements.txt">
requests~=2.31.0
moviepy==2.0.0.dev2
faster-whisper~=1.0.1
uvicorn~=0.27.1
fastapi~=0.115.4
tomli~=2.0.1
streamlit~=1.40.0
loguru~=0.7.2
aiohttp~=3.10.10
urllib3~=2.2.1
pydantic~=2.6.3
g4f~=0.3.0.4
dashscope~=1.15.0
google.generativeai>=0.8.3
python-multipart~=0.0.9
redis==5.0.3
opencv-python~=4.10.0.84
# for azure speech
# https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/9-more-realistic-ai-voices-for-conversations-now-generally/ba-p/4099471
azure-cognitiveservices-speech~=1.37.0
git-changelog~=2.5.2
watchdog==5.0.2
pydub==0.25.1
psutil>=5.9.0
opencv-python~=4.10.0.84
scikit-learn~=1.5.2
google-generativeai~=0.8.3
pillow==10.3.0
python-dotenv~=1.0.1
openai~=1.53.0
tqdm>=4.66.6
tenacity>=9.0.0
tiktoken==0.8.0
yt-dlp==2024.11.18
pysrt==1.1.2
httpx==0.27.2
transformers==4.47.0
edge-tts==6.1.19
</file>

<file path="video_pipeline.py">
import requests
import json
import os
import time
from typing import Dict, Any

class VideoPipeline:
    def __init__(self, base_url: str = "http://127.0.0.1:8080"):
        self.base_url = base_url
        
    def download_video(self, url: str, resolution: str = "1080p", 
                      output_format: str = "mp4", rename: str = None) -> Dict[str, Any]:
        """ä¸‹è½½è§†é¢‘çš„ç¬¬ä¸€æ­¥"""
        endpoint = f"{self.base_url}/api/v2/youtube/download"
        payload = {
            "url": url,
            "resolution": resolution,
            "output_format": output_format,
            "rename": rename or time.strftime("%Y-%m-%d")
        }
        
        response = requests.post(endpoint, json=payload)
        response.raise_for_status()
        return response.json()
    
    def generate_script(self, video_path: str, skip_seconds: int = 0,
                       threshold: int = 30, vision_batch_size: int = 10,
                       vision_llm_provider: str = "gemini") -> Dict[str, Any]:
        """ç”Ÿæˆè„šæœ¬çš„ç¬¬äºŒæ­¥"""
        endpoint = f"{self.base_url}/api/v2/scripts/generate"
        payload = {
            "video_path": video_path,
            "skip_seconds": skip_seconds,
            "threshold": threshold,
            "vision_batch_size": vision_batch_size,
            "vision_llm_provider": vision_llm_provider
        }
        
        response = requests.post(endpoint, json=payload)
        response.raise_for_status()
        return response.json()
    
    def crop_video(self, video_path: str, script: list) -> Dict[str, Any]:
        """å‰ªè¾‘è§†é¢‘çš„ç¬¬ä¸‰æ­¥"""
        endpoint = f"{self.base_url}/api/v2/scripts/crop"
        payload = {
            "video_origin_path": video_path,
            "video_script": script
        }
        
        response = requests.post(endpoint, json=payload)
        response.raise_for_status()
        return response.json()
    
    def generate_final_video(self, task_id: str, video_path: str, 
                           script_path: str, script: list, subclip_videos: Dict[str, str], voice_name: str) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆè§†é¢‘çš„ç¬¬å››æ­¥"""
        endpoint = f"{self.base_url}/api/v2/scripts/start-subclip"
        
        request_data = {
            "video_clip_json": script,
            "video_clip_json_path": script_path,
            "video_origin_path": video_path,
            "video_aspect": "16:9",
            "video_language": "zh-CN",
            "voice_name": voice_name,
            "voice_volume": 1,
            "voice_rate": 1.2,
            "voice_pitch": 1,
            "bgm_name": "random",
            "bgm_type": "random",
            "bgm_file": "",
            "bgm_volume": 0.3,
            "subtitle_enabled": True,
            "subtitle_position": "bottom",
            "font_name": "STHeitiMedium.ttc",
            "text_fore_color": "#FFFFFF",
            "text_background_color": "transparent",
            "font_size": 75,
            "stroke_color": "#000000",
            "stroke_width": 1.5,
            "custom_position": 70,
            "n_threads": 8
        }
        
        payload = {
            "request": request_data,
            "subclip_videos": subclip_videos
        }
        
        params = {"task_id": task_id}
        response = requests.post(endpoint, params=params, json=payload)
        response.raise_for_status()
        return response.json()
    
    def save_script_to_json(self, script: list, script_path: str) -> str:
        """ä¿å­˜è„šæœ¬åˆ°jsonæ–‡ä»¶"""        
        try:
            with open(script_path, 'w', encoding='utf-8') as f:
                json.dump(script, f, ensure_ascii=False, indent=2)
            print(f"è„šæœ¬å·²ä¿å­˜åˆ°: {script_path}")
            return script_path
        except Exception as e:
            print(f"ä¿å­˜è„šæœ¬å¤±è´¥: {str(e)}")
            raise
    
    def run_pipeline(self, task_id: str, script_name: str, youtube_url: str, video_name: str="null", skip_seconds: int = 0, threshold: int = 30, vision_batch_size: int = 10, vision_llm_provider: str = "gemini", voice_name: str = "zh-CN-YunjianNeural") -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„pipeline"""
        try:
            current_path = os.path.dirname(os.path.abspath(__file__))
            video_path = os.path.join(current_path, "resource", "videos", f"{video_name}.mp4")
            # åˆ¤æ–­è§†é¢‘æ˜¯å¦å­˜åœ¨
            if not os.path.exists(video_path):
                # 1. ä¸‹è½½è§†é¢‘
                print(f"è§†é¢‘ä¸å­˜åœ¨, å¼€å§‹ä¸‹è½½è§†é¢‘: {video_path}")
                download_result = self.download_video(url=youtube_url, resolution="1080p", output_format="mp4", rename=video_name)
                video_path = download_result["output_path"]
            else:
                print(f"è§†é¢‘å·²å­˜åœ¨: {video_path}")
            
            # 2. åˆ¤æ–­script_nameæ˜¯å¦å­˜åœ¨
            # 2.1.1 æ‹¼æ¥è„šæœ¬è·¯å¾„ NarratoAI/resource/scripts
            script_path = os.path.join(current_path, "resource", "scripts", script_name)
            if os.path.exists(script_path):
                script = json.load(open(script_path, "r", encoding="utf-8"))
            else:
                # 2.1.2 ç”Ÿæˆè„šæœ¬
                print("å¼€å§‹ç”Ÿæˆè„šæœ¬...")
                script_result = self.generate_script(video_path=video_path, skip_seconds=skip_seconds, threshold=threshold, vision_batch_size=vision_batch_size, vision_llm_provider=vision_llm_provider)
                script = script_result["script"]
            
            # 2.2 ä¿å­˜è„šæœ¬åˆ°jsonæ–‡ä»¶
            print("ä¿å­˜è„šæœ¬åˆ°jsonæ–‡ä»¶...")
            self.save_script_to_json(script=script, script_path=script_path)
            
            # 3. å‰ªè¾‘è§†é¢‘
            print("å¼€å§‹å‰ªè¾‘è§†é¢‘...")
            crop_result = self.crop_video(video_path=video_path, script=script)
            subclip_videos = crop_result["subclip_videos"]
            
            # 4. ç”Ÿæˆæœ€ç»ˆè§†é¢‘
            print("å¼€å§‹ç”Ÿæˆæœ€ç»ˆè§†é¢‘...")
            self.generate_final_video(
                task_id=task_id,
                video_path=video_path,
                script_path=script_path,
                script=script,
                subclip_videos=subclip_videos,
                voice_name=voice_name
            )
            
            return {
                "status": "ç­‰å¾…å¼‚æ­¥ç”Ÿæˆè§†é¢‘",
                "path": os.path.join(current_path, "storage", "tasks", task_id)
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    pipeline = VideoPipeline()
    result = pipeline.run_pipeline(
        task_id="test_111901",
        script_name="test.json",
        youtube_url="https://www.youtube.com/watch?v=vLJ7Yed6FQ4",
        video_name="2024-11-19-01",
        skip_seconds=50,
        threshold=35,
        vision_batch_size=10,
        vision_llm_provider="gemini",
        voice_name="zh-CN-YunjianNeural",
    )
    print(result)
</file>

<file path="webui.py">
import streamlit as st
import os
import sys
from uuid import uuid4
from app.config import config
from webui.components import basic_settings, video_settings, audio_settings, subtitle_settings, script_settings, review_settings, merge_settings, system_settings
from webui.utils import cache, file_utils
from app.utils import utils
from app.models.schema import VideoClipParams, VideoAspect
from webui.utils.performance import PerformanceMonitor

# åˆå§‹åŒ–é…ç½® - å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤
st.set_page_config(
    page_title="NarratoAI",
    page_icon="ğŸ“½ï¸",
    layout="wide",
    initial_sidebar_state="auto",
    menu_items={
        "Report a bug": "https://github.com/linyqh/NarratoAI/issues",
        'About': f"# NarratoAI:sunglasses: ğŸ“½ï¸ \n #### Version: v{config.project_version} \n "
                 f"è‡ªåŠ¨åŒ–å½±è§†è§£è¯´è§†é¢‘è¯¦æƒ…è¯·ç§»æ­¥ï¼šhttps://github.com/linyqh/NarratoAI"
    },
)

# è®¾ç½®é¡µé¢æ ·å¼
hide_streamlit_style = """
<style>#root > div:nth-child(1) > div > div > div > div > section > div {padding-top: 6px; padding-bottom: 10px; padding-left: 20px; padding-right: 20px;}</style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

def init_log():
    """åˆå§‹åŒ–æ—¥å¿—é…ç½®"""
    from loguru import logger
    logger.remove()
    _lvl = "DEBUG"

    def format_record(record):
        # å¢åŠ æ›´å¤šéœ€è¦è¿‡æ»¤çš„è­¦å‘Šæ¶ˆæ¯
        ignore_messages = [
            "Examining the path of torch.classes raised",
            "torch.cuda.is_available()",
            "CUDA initialization"
        ]
        
        for msg in ignore_messages:
            if msg in record["message"]:
                return ""
            
        file_path = record["file"].path
        relative_path = os.path.relpath(file_path, config.root_dir)
        record["file"].path = f"./{relative_path}"
        record['message'] = record['message'].replace(config.root_dir, ".")

        _format = '<green>{time:%Y-%m-%d %H:%M:%S}</> | ' + \
                  '<level>{level}</> | ' + \
                  '"{file.path}:{line}":<blue> {function}</> ' + \
                  '- <level>{message}</>' + "\n"
        return _format

    # ä¼˜åŒ–æ—¥å¿—è¿‡æ»¤å™¨
    def log_filter(record):
        ignore_messages = [
            "Examining the path of torch.classes raised",
            "torch.cuda.is_available()",
            "CUDA initialization"
        ]
        return not any(msg in record["message"] for msg in ignore_messages)

    logger.add(
        sys.stdout,
        level=_lvl,
        format=format_record,
        colorize=True,
        filter=log_filter
    )

def init_global_state():
    """åˆå§‹åŒ–å…¨å±€çŠ¶æ€"""
    if 'video_clip_json' not in st.session_state:
        st.session_state['video_clip_json'] = []
    if 'video_plot' not in st.session_state:
        st.session_state['video_plot'] = ''
    if 'ui_language' not in st.session_state:
        st.session_state['ui_language'] = config.ui.get("language", utils.get_system_locale())
    if 'subclip_videos' not in st.session_state:
        st.session_state['subclip_videos'] = {}

def tr(key):
    """ç¿»è¯‘å‡½æ•°"""
    i18n_dir = os.path.join(os.path.dirname(__file__), "webui", "i18n")
    locales = utils.load_locales(i18n_dir)
    loc = locales.get(st.session_state['ui_language'], {})
    return loc.get("Translation", {}).get(key, key)

def render_generate_button():
    """æ¸²æŸ“ç”ŸæˆæŒ‰é’®å’Œå¤„ç†é€»è¾‘"""
    if st.button(tr("Generate Video"), use_container_width=True, type="primary"):
        try:
            from app.services import task as tm
            import torch
            
            # é‡ç½®æ—¥å¿—å®¹å™¨å’Œè®°å½•
            log_container = st.empty()
            log_records = []

            def log_received(msg):
                with log_container:
                    log_records.append(msg)
                    st.code("\n".join(log_records))

            from loguru import logger
            logger.add(log_received)

            config.save_config()
            task_id = st.session_state.get('task_id')

            if not task_id:
                st.error(tr("è¯·å…ˆè£å‰ªè§†é¢‘"))
                return
            if not st.session_state.get('video_clip_json_path'):
                st.error(tr("è„šæœ¬æ–‡ä»¶ä¸èƒ½ä¸ºç©º"))
                return
            if not st.session_state.get('video_origin_path'):
                st.error(tr("è§†é¢‘æ–‡ä»¶ä¸èƒ½ä¸ºç©º"))
                return

            st.toast(tr("ç”Ÿæˆè§†é¢‘"))
            logger.info(tr("å¼€å§‹ç”Ÿæˆè§†é¢‘"))

            # è·å–æ‰€æœ‰å‚æ•°
            script_params = script_settings.get_script_params()
            video_params = video_settings.get_video_params()
            audio_params = audio_settings.get_audio_params()
            subtitle_params = subtitle_settings.get_subtitle_params()

            # åˆå¹¶æ‰€æœ‰å‚æ•°
            all_params = {
                **script_params,
                **video_params,
                **audio_params,
                **subtitle_params
            }

            # åˆ›å»ºå‚æ•°å¯¹è±¡
            params = VideoClipParams(**all_params)

            result = tm.start_subclip(
                task_id=task_id,
                params=params,
                subclip_path_videos=st.session_state['subclip_videos']
            )

            video_files = result.get("videos", [])
            st.success(tr("è§†ç”Ÿæˆå®Œæˆ"))
            
            try:
                if video_files:
                    player_cols = st.columns(len(video_files) * 2 + 1)
                    for i, url in enumerate(video_files):
                        player_cols[i * 2 + 1].video(url)
            except Exception as e:
                logger.error(f"æ’­æ”¾è§†é¢‘å¤±è´¥: {e}")

            file_utils.open_task_folder(config.root_dir, task_id)
            logger.info(tr("è§†é¢‘ç”Ÿæˆå®Œæˆ"))

        finally:
            PerformanceMonitor.cleanup_resources()

def main():
    """ä¸»å‡½æ•°"""
    init_log()
    init_global_state()
    utils.init_resources()
    
    st.title(f"NarratoAI :sunglasses:ğŸ“½ï¸")
    st.write(tr("Get Help"))
    
    # æ¸²æŸ“åŸºç¡€è®¾ç½®é¢æ¿
    basic_settings.render_basic_settings(tr)
    # æ¸²æŸ“åˆå¹¶è®¾ç½®
    merge_settings.render_merge_settings(tr)

    # æ¸²æŸ“ä¸»é¢æ¿
    panel = st.columns(3)
    with panel[0]:
        script_settings.render_script_panel(tr)
    with panel[1]:
        video_settings.render_video_panel(tr)
        audio_settings.render_audio_panel(tr)
    with panel[2]:
        subtitle_settings.render_subtitle_panel(tr)
        # æ¸²æŸ“ç³»ç»Ÿè®¾ç½®é¢æ¿
        system_settings.render_system_panel(tr)
    
    # æ¸²æŸ“è§†é¢‘å®¡æŸ¥é¢æ¿
    review_settings.render_review_panel(tr)
    
    # æ¸²æŸ“ç”ŸæˆæŒ‰é’®å’Œå¤„ç†é€»è¾‘
    render_generate_button()

if __name__ == "__main__":
    main()
</file>

<file path="webui.txt">
@echo off
set CURRENT_DIR=%CD%
echo ***** Current directory: %CURRENT_DIR% *****
set PYTHONPATH=%CURRENT_DIR%

set "vpn_proxy_url=%http://127.0.0.1:7890%"

:: ä½¿ç”¨VPNä»£ç†è¿›è¡Œä¸€äº›æ“ä½œï¼Œä¾‹å¦‚é€šè¿‡ä»£ç†ä¸‹è½½æ–‡ä»¶
set "http_proxy=%vpn_proxy_url%"
set "https_proxy=%vpn_proxy_url%"

@echo off
setlocal enabledelayedexpansion

rem åˆ›å»ºé“¾æ¥å’Œè·¯å¾„çš„æ•°ç»„
set "urls_paths[0]=https://zenodo.org/records/13293144/files/MicrosoftYaHeiBold.ttc|.\resource\fonts"
set "urls_paths[1]=https://zenodo.org/records/13293144/files/MicrosoftYaHeiNormal.ttc|.\resource\fonts"
set "urls_paths[2]=https://zenodo.org/records/13293144/files/STHeitiLight.ttc|.\resource\fonts"
set "urls_paths[3]=https://zenodo.org/records/13293144/files/STHeitiMedium.ttc|.\resource\fonts"
set "urls_paths[4]=https://zenodo.org/records/13293144/files/UTM%20Kabel%20KT.ttf|.\resource\fonts"
set "urls_paths[5]=https://zenodo.org/records/14167125/files/test.mp4|.\resource\videos"
set "urls_paths[6]=https://zenodo.org/records/13293150/files/output000.mp3|.\resource\songs"
set "urls_paths[7]=https://zenodo.org/records/13293150/files/output001.mp3|.\resource\songs"
set "urls_paths[8]=https://zenodo.org/records/13293150/files/output002.mp3|.\resource\songs"
set "urls_paths[9]=https://zenodo.org/records/13293150/files/output003.mp3|.\resource\songs"
set "urls_paths[10]=https://zenodo.org/records/13293150/files/output004.mp3|.\resource\songs"
set "urls_paths[11]=https://zenodo.org/records/13293150/files/output005.mp3|.\resource\songs"
set "urls_paths[12]=https://zenodo.org/records/13293150/files/output006.mp3|.\resource\songs"
set "urls_paths[13]=https://zenodo.org/records/13293150/files/output007.mp3|.\resource\songs"
set "urls_paths[14]=https://zenodo.org/records/13293150/files/output008.mp3|.\resource\songs"
set "urls_paths[15]=https://zenodo.org/records/13293150/files/output009.mp3|.\resource\songs"
set "urls_paths[16]=https://zenodo.org/records/13293150/files/output010.mp3|.\resource\songs"

rem å¾ªç¯ä¸‹è½½æ‰€æœ‰æ–‡ä»¶å¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
for /L %%i in (0,1,16) do (
    for /f "tokens=1,2 delims=|" %%a in ("!urls_paths[%%i]!") do (
        if not exist "%%b" mkdir "%%b"
        echo æ­£åœ¨ä¸‹è½½ %%a åˆ° %%b
        curl -o "%%b\%%~nxa" %%a
    )
)

echo æ‰€æœ‰æ–‡ä»¶å·²æˆåŠŸä¸‹è½½åˆ°æŒ‡å®šç›®å½•
endlocal
pause


rem set HF_ENDPOINT=https://hf-mirror.com
streamlit run webui.py --browser.serverAddress="127.0.0.1" --server.enableCORS=True  --server.maxUploadSize=2048 --browser.gatherUsageStats=False

è¯·æ±‚0ï¼š
curl -X 'POST' \
  'http://127.0.0.1:8080/api/v2/youtube/download' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "url": "https://www.youtube.com/watch?v=Kenm35gdqtk",
  "resolution": "1080p",
  "output_format": "mp4",
  "rename": "2024-11-19"
}'
{
  "url": "https://www.youtube.com/watch?v=Kenm35gdqtk",
  "resolution": "1080p",
  "output_format": "mp4",
  "rename": "2024-11-19"
}

è¯·æ±‚1ï¼š
curl -X 'POST' \
  'http://127.0.0.1:8080/api/v2/scripts/generate' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "video_path": "E:\\projects\\NarratoAI\\resource\\videos\\test.mp4",
  "skip_seconds": 0,
  "threshold": 30,
  "vision_batch_size": 10,
  "vision_llm_provider": "gemini"
}'
{
  "video_path": "E:\\projects\\NarratoAI\\resource\\videos\\test.mp4",
  "skip_seconds": 0,
  "threshold": 30,
  "vision_batch_size": 10,
  "vision_llm_provider": "gemini"
}

è¯·æ±‚2ï¼š
curl -X 'POST' \
  'http://127.0.0.1:8080/api/v2/scripts/crop' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "video_origin_path": "E:\\projects\\NarratoAI\\resource\\videos\\test.mp4",
  "video_script": [
    {
      "timestamp": "00:10-01:01",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯è§†é¢‘ç”»é¢çš„å®¢è§‚æè¿°ï¼š\n\nè§†é¢‘å±•ç°ä¸€åç•™ç€èƒ¡é¡»çš„ç”·å­åœ¨æ£®æ—é‡ŒæŒ–æ˜ã€‚\n\nç”»é¢é¦–å…ˆå±•ç°ç”·å­ä»åæ–¹è§†è§’ï¼ŒèƒŒç€å†›ç»¿è‰²èƒŒåŒ…ï¼Œç©¿ç€å¡å…¶è‰²é•¿è£¤å’Œæ·±è‰²Tæ¤ï¼Œèµ°å‘ä¸€ä¸ªæ³¥åœŸæ–œå¡ã€‚èƒŒåŒ…ä¸Šä¼¼ä¹æœ‰ä¸€ä¸ªé•å¤´ã€‚\n\nä¸‹ä¸€ä¸ªé•œå¤´ç‰¹å†™å±•ç°äº†è¯¥èƒŒåŒ…ï¼Œä¸€ä¸ªé•å¤´ä»èƒŒåŒ…é‡Œä¼¸å‡ºæ¥ï¼ŒåŒ…é‡Œè¿˜æœ‰ä¸€äº›å…¶ä»–å·¥å…·ã€‚\n\nç„¶åï¼Œè§†é¢‘æ˜¾ç¤ºè¯¥ç”·å­ç”¨é•å¤´æŒ–æ˜æ³¥åœŸæ–œå¡ã€‚\n\næ¥ä¸‹æ¥æ˜¯ä¸€äº›è¿‘æ™¯é•œå¤´ï¼Œå±•ç°ç”·å­çš„é´å­åœ¨æ³¥åœŸä¸­è¡Œèµ°ï¼Œä»¥åŠç”·å­ç”¨æ‰‹æ¸…ç†æ³¥åœŸã€‚\n\nå…¶ä»–é•œå¤´ä»ä¸åŒè§’åº¦å±•ç°è¯¥ç”·å­åœ¨æŒ–æ˜ï¼ŒåŒ…æ‹¬ä»ä¾§é¢å’Œä¸Šæ–¹ã€‚\n\nå¯ä»¥çœ‹åˆ°ä»–ç”¨å·¥å…·æŒ–æ˜ï¼Œæ¸…ç†æ³¥åœŸï¼Œå¹¶æ£€æŸ¥æŒ–å‡ºçš„åœŸå£¤ã€‚\n\næœ€åï¼Œä¸€ä¸ªé•œå¤´å±•ç°äº†æŒ–å‡ºçš„åœŸå£¤çš„è´¨åœ°å’Œé¢œè‰²ã€‚",
      "narration": "å¥½çš„ï¼Œæ¥ä¸‹æ¥å°±æ˜¯æˆ‘ä»¬è¿™ä½â€œèƒ¡é¡»å¤§ä¾ â€çš„ç²¾å½©å†’é™©äº†ï¼åªè§ä»–èƒŒç€å†›ç»¿è‰²çš„èƒŒåŒ…ï¼Œè¿ˆç€æ¯”æˆ‘ä¸Šç­è¿˜ä¸æƒ…æ„¿çš„æ­¥ä¼èµ°å‘é‚£æ³¥åœŸæ–œå¡ã€‚å“å‘€ï¼Œè¿™ä¸ªèƒŒåŒ…å¯çœŸæ˜¯ä¸ªå®è´ï¼Œé‡Œé¢è—ç€ä¸€æŠŠé•å¤´å’Œä¸€äº›å·¥å…·ï¼Œç®€ç›´åƒæ˜¯ä¸ªéšèº«æºå¸¦çš„â€œå»ºç­‘å·¥å…·ç®±â€ï¼ \n\nçœ‹ä»–æŒ¥èˆç€é•å¤´ï¼ŒæŒ–æ˜æ³¥åœŸçš„å§¿åŠ¿ï¼Œä»¿ä½›åœ¨è¿›è¡Œä¸€åœºâ€œæŒ–åœŸå¤§èµ›â€ï¼Œç»“æœå´æ¯”æˆ‘åšé¥­è¿˜è¦ç³Ÿç³•ã€‚æ³¥åœŸé£æ‰¬ä¸­ï¼Œä»–çš„é´å­ä¹Ÿæˆäº†â€œæ³¥å·´è‰ºæœ¯å®¶â€ã€‚æœ€åï¼Œé‚£å †è‰²æ³½å„å¼‚çš„åœŸå£¤å°±åƒä»–å¿ƒæƒ…çš„å†™ç…§â€”â€”äº”å½©æ–‘æ–“åˆç•¥æ˜¾æ··ä¹±ï¼çœŸæ˜¯ä¸€æ¬¡è®©äººæ§è…¹çš„å»ºé€ ä¹‹æ—…ï¼",
      "OST": 2,
      "new_timestamp": "00:00-00:51"
    },
    {
      "timestamp": "01:07-01:53",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯è§†é¢‘ç”»é¢çš„å®¢è§‚æè¿°ï¼š\n\nè§†é¢‘ä»¥ä¸€ç³»åˆ—æ£®æ—ç¯å¢ƒçš„é•œå¤´å¼€å¤´ã€‚\n\nç¬¬ä¸€ä¸ªé•œå¤´æ˜¯ä¸€ä¸ªç‰¹å†™é•œå¤´ï¼Œé•œå¤´ä¸­æ˜¾ç¤ºçš„æ˜¯ä¸€äº›å¸¦æœ‰æ°´æ»´çš„ç»¿è‰²å¶å­ã€‚\n\nç¬¬äºŒä¸ªé•œå¤´æ˜¾ç¤ºä¸€ä¸ªç•™ç€èƒ¡é¡»çš„ç”·å­åœ¨æ£®æ—ä¸­æŒ–æ˜ä¸€ä¸ªæ´ã€‚ ä»–è·ªåœ¨åœ°ä¸Šï¼Œç”¨å·¥å…·æŒ–åœŸã€‚\n\nç¬¬ä¸‰ä¸ªé•œå¤´æ˜¯ä¸€ä¸ªä¸­ç­‰é•œå¤´ï¼Œæ˜¾ç¤ºåŒä¸€ä¸ªäººååœ¨ä»–æŒ–å¥½çš„æ´è¾¹ä¼‘æ¯ã€‚\n\nç¬¬å››ä¸ªé•œå¤´æ˜¾ç¤ºè¯¥æ´çš„å†…éƒ¨ç»“æ„ï¼Œè¯¥æ´åœ¨æ ‘æ ¹å’Œåœ°é¢ä¹‹é—´ã€‚\n\nç¬¬äº”ä¸ªé•œå¤´æ˜¾ç¤ºè¯¥ç”·å­ç”¨æ–§å¤´ç æ ‘æã€‚\n\nç¬¬å…­ä¸ªé•œå¤´æ˜¾ç¤ºä¸€å †æ ‘ææ¨ªè·¨ä¸€ä¸ªæ³¥æ³çš„å°æ°´å‘ã€‚\n\nç¬¬ä¸ƒä¸ªé•œå¤´æ˜¾ç¤ºæ›´å¤šèŒ‚ç››çš„æ ‘å¶å’Œæ ‘æåœ¨é˜³å…‰ä¸‹ã€‚\n\nç¬¬å…«ä¸ªé•œå¤´æ˜¾ç¤ºæ›´å¤šèŒ‚ç››çš„æ ‘å¶å’Œæ ‘æã€‚\n\n\n",
      "narration": "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çš„â€œæŒ–åœŸå¤§å¸ˆâ€åˆå¼€å§‹äº†ä»–çš„æ£®æ—æ¢é™©ã€‚çœ‹è¿™é•œå¤´ï¼Œæ°´æ»´åœ¨å¶å­ä¸Šé—ªçƒï¼Œä»¿ä½›åœ¨è¯´ï¼šâ€œå¿«æ¥ï¼Œå¿«æ¥ï¼Œè¿™é‡Œæœ‰æ•…äº‹ï¼â€ä»–ä¸€è¾¹æŒ–æ´ï¼Œä¸€è¾¹åƒä¸ªæ–°æ‰‹å¨å¸ˆè¯•å›¾åˆ‡æ´‹è‘±â€”â€”æ¯ä¸€ä¸‹éƒ½å°å¿ƒç¿¼ç¿¼ï¼Œç”Ÿæ€•è‡ªå·±ä¸å°å¿ƒæŒ–å‡ºä¸ªâ€œå†å²é—å€â€ã€‚åä¸‹ä¼‘æ¯çš„æ—¶å€™ï¼Œè„¸ä¸Šçš„è¡¨æƒ…å°±åƒå‘ç°æ–°å¤§é™†ä¸€æ ·ï¼ç„¶åï¼Œä»–æ‹¿èµ·æ–§å¤´ç æ ‘æï¼Œç®€ç›´æ˜¯ç°ä»£ç‰ˆçš„â€œç¥é›•ä¾ ä¾£â€ï¼Œåªä¸è¿‡å¯¹è±¡æ˜¯æ ‘æœ¨ã€‚æœ€åï¼Œé‚£å †æ ‘ææ¶è¿‡æ³¥æ³çš„å°æ°´å‘ï¼Œä»¿ä½›åœ¨è¯´ï¼šâ€œæˆ‘å°±æ˜¯ä¸æ€•æ¹¿è„šçš„å‹‡å£«ï¼â€è¿™å°±æ˜¯æˆ‘ä»¬çš„å»ºé€ ä¹‹æ—…ï¼",
      "OST": 2,
      "new_timestamp": "00:51-01:37"
    }
  ]
}'
{
  "video_origin_path": "E:\\projects\\NarratoAI\\resource\\videos\\test.mp4",
  "video_script": [
    {
      "timestamp": "00:10-01:01",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯è§†é¢‘ç”»é¢çš„å®¢è§‚æè¿°ï¼š\n\nè§†é¢‘å±•ç°ä¸€åç•™ç€èƒ¡é¡»çš„ç”·å­åœ¨æ£®æ—é‡ŒæŒ–æ˜ã€‚\n\nç”»é¢é¦–å…ˆå±•ç°ç”·å­ä»åæ–¹è§†è§’ï¼ŒèƒŒç€å†›ç»¿è‰²èƒŒåŒ…ï¼Œç©¿ç€å¡å…¶è‰²é•¿è£¤å’Œæ·±è‰²Tæ¤ï¼Œèµ°å‘ä¸€ä¸ªæ³¥åœŸæ–œå¡ã€‚èƒŒåŒ…ä¸Šä¼¼ä¹æœ‰ä¸€ä¸ªé•å¤´ã€‚\n\nä¸‹ä¸€ä¸ªé•œå¤´ç‰¹å†™å±•ç°äº†è¯¥èƒŒåŒ…ï¼Œä¸€ä¸ªé•å¤´ä»èƒŒåŒ…é‡Œä¼¸å‡ºæ¥ï¼ŒåŒ…é‡Œè¿˜æœ‰ä¸€äº›å…¶ä»–å·¥å…·ã€‚\n\nç„¶åï¼Œè§†é¢‘æ˜¾ç¤ºè¯¥ç”·å­ç”¨é•å¤´æŒ–æ˜æ³¥åœŸæ–œå¡ã€‚\n\næ¥ä¸‹æ¥æ˜¯ä¸€äº›è¿‘æ™¯é•œå¤´ï¼Œå±•ç°ç”·å­çš„é´å­åœ¨æ³¥åœŸä¸­è¡Œèµ°ï¼Œä»¥åŠç”·å­ç”¨æ‰‹æ¸…ç†æ³¥åœŸã€‚\n\nå…¶ä»–é•œå¤´ä»ä¸åŒè§’åº¦å±•ç°è¯¥ç”·å­åœ¨æŒ–æ˜ï¼ŒåŒ…æ‹¬ä»ä¾§é¢å’Œä¸Šæ–¹ã€‚\n\nå¯ä»¥çœ‹åˆ°ä»–ç”¨å·¥å…·æŒ–æ˜ï¼Œæ¸…ç†æ³¥åœŸï¼Œå¹¶æ£€æŸ¥æŒ–å‡ºçš„åœŸå£¤ã€‚\n\næœ€åï¼Œä¸€ä¸ªé•œå¤´å±•ç°äº†æŒ–å‡ºçš„åœŸå£¤çš„è´¨åœ°å’Œé¢œè‰²ã€‚",
      "narration": "å¥½çš„ï¼Œæ¥ä¸‹æ¥å°±æ˜¯æˆ‘ä»¬è¿™ä½â€œèƒ¡é¡»å¤§ä¾ â€çš„ç²¾å½©å†’é™©äº†ï¼åªè§ä»–èƒŒç€å†›ç»¿è‰²çš„èƒŒåŒ…ï¼Œè¿ˆç€æ¯”æˆ‘ä¸Šç­è¿˜ä¸æƒ…æ„¿çš„æ­¥ä¼èµ°å‘é‚£æ³¥åœŸæ–œå¡ã€‚å“å‘€ï¼Œè¿™ä¸ªèƒŒåŒ…å¯çœŸæ˜¯ä¸ªå®è´ï¼Œé‡Œé¢è—ç€ä¸€æŠŠé•å¤´å’Œä¸€äº›å·¥å…·ï¼Œç®€ç›´åƒæ˜¯ä¸ªéšèº«æºå¸¦çš„â€œå»ºç­‘å·¥å…·ç®±â€ï¼ \n\nçœ‹ä»–æŒ¥èˆç€é•å¤´ï¼ŒæŒ–æ˜æ³¥åœŸçš„å§¿åŠ¿ï¼Œä»¿ä½›åœ¨è¿›è¡Œä¸€åœºâ€œæŒ–åœŸå¤§èµ›â€ï¼Œç»“æœå´æ¯”æˆ‘åšé¥­è¿˜è¦ç³Ÿç³•ã€‚æ³¥åœŸé£æ‰¬ä¸­ï¼Œä»–çš„é´å­ä¹Ÿæˆäº†â€œæ³¥å·´è‰ºæœ¯å®¶â€ã€‚æœ€åï¼Œé‚£å †è‰²æ³½å„å¼‚çš„åœŸå£¤å°±åƒä»–å¿ƒæƒ…çš„å†™ç…§â€”â€”äº”å½©æ–‘æ–“åˆç•¥æ˜¾æ··ä¹±ï¼çœŸæ˜¯ä¸€æ¬¡è®©äººæ§è…¹çš„å»ºé€ ä¹‹æ—…ï¼",
      "OST": 2,
      "new_timestamp": "00:00-00:51"
    },
    {
      "timestamp": "01:07-01:53",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯è§†é¢‘ç”»é¢çš„å®¢è§‚æè¿°ï¼š\n\nè§†é¢‘ä»¥ä¸€ç³»åˆ—æ£®æ—ç¯å¢ƒçš„é•œå¤´å¼€å¤´ã€‚\n\nç¬¬ä¸€ä¸ªé•œå¤´æ˜¯ä¸€ä¸ªç‰¹å†™é•œå¤´ï¼Œé•œå¤´ä¸­æ˜¾ç¤ºçš„æ˜¯ä¸€äº›å¸¦æœ‰æ°´æ»´çš„ç»¿è‰²å¶å­ã€‚\n\nç¬¬äºŒä¸ªé•œå¤´æ˜¾ç¤ºä¸€ä¸ªç•™ç€èƒ¡é¡»çš„ç”·å­åœ¨æ£®æ—ä¸­æŒ–æ˜ä¸€ä¸ªæ´ã€‚ ä»–è·ªåœ¨åœ°ä¸Šï¼Œç”¨å·¥å…·æŒ–åœŸã€‚\n\nç¬¬ä¸‰ä¸ªé•œå¤´æ˜¯ä¸€ä¸ªä¸­ç­‰é•œå¤´ï¼Œæ˜¾ç¤ºåŒä¸€ä¸ªäººååœ¨ä»–æŒ–å¥½çš„æ´è¾¹ä¼‘æ¯ã€‚\n\nç¬¬å››ä¸ªé•œå¤´æ˜¾ç¤ºè¯¥æ´çš„å†…éƒ¨ç»“æ„ï¼Œè¯¥æ´åœ¨æ ‘æ ¹å’Œåœ°é¢ä¹‹é—´ã€‚\n\nç¬¬äº”ä¸ªé•œå¤´æ˜¾ç¤ºè¯¥ç”·å­ç”¨æ–§å¤´ç æ ‘æã€‚\n\nç¬¬å…­ä¸ªé•œå¤´æ˜¾ç¤ºä¸€å †æ ‘ææ¨ªè·¨ä¸€ä¸ªæ³¥æ³çš„å°æ°´å‘ã€‚\n\nç¬¬ä¸ƒä¸ªé•œå¤´æ˜¾ç¤ºæ›´å¤šèŒ‚ç››çš„æ ‘å¶å’Œæ ‘æåœ¨é˜³å…‰ä¸‹ã€‚\n\nç¬¬å…«ä¸ªé•œå¤´æ˜¾ç¤ºæ›´å¤šèŒ‚ç››çš„æ ‘å¶å’Œæ ‘æã€‚\n\n\n",
      "narration": "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çš„â€œæŒ–åœŸå¤§å¸ˆâ€åˆå¼€å§‹äº†ä»–çš„æ£®æ—æ¢é™©ã€‚çœ‹è¿™é•œå¤´ï¼Œæ°´æ»´åœ¨å¶å­ä¸Šé—ªçƒï¼Œä»¿ä½›åœ¨è¯´ï¼šâ€œå¿«æ¥ï¼Œå¿«æ¥ï¼Œè¿™é‡Œæœ‰æ•…äº‹ï¼â€ä»–ä¸€è¾¹æŒ–æ´ï¼Œä¸€è¾¹åƒä¸ªæ–°æ‰‹å¨å¸ˆè¯•å›¾åˆ‡æ´‹è‘±â€”â€”æ¯ä¸€ä¸‹éƒ½å°å¿ƒç¿¼ç¿¼ï¼Œç”Ÿæ€•è‡ªå·±ä¸å°å¿ƒæŒ–å‡ºä¸ªâ€œå†å²é—å€â€ã€‚åä¸‹ä¼‘æ¯çš„æ—¶å€™ï¼Œè„¸ä¸Šçš„è¡¨æƒ…å°±åƒå‘ç°æ–°å¤§é™†ä¸€æ ·ï¼ç„¶åï¼Œä»–æ‹¿èµ·æ–§å¤´ç æ ‘æï¼Œç®€ç›´æ˜¯ç°ä»£ç‰ˆçš„â€œç¥é›•ä¾ ä¾£â€ï¼Œåªä¸è¿‡å¯¹è±¡æ˜¯æ ‘æœ¨ã€‚æœ€åï¼Œé‚£å †æ ‘ææ¶è¿‡æ³¥æ³çš„å°æ°´å‘ï¼Œä»¿ä½›åœ¨è¯´ï¼šâ€œæˆ‘å°±æ˜¯ä¸æ€•æ¹¿è„šçš„å‹‡å£«ï¼â€è¿™å°±æ˜¯æˆ‘ä»¬çš„å»ºé€ ä¹‹æ—…ï¼",
      "OST": 2,
      "new_timestamp": "00:51-01:37"
    }
  ]
}

è¯·æ±‚3ï¼š
curl -X 'POST' \
  'http://127.0.0.1:8080/api/v2/scripts/start-subclip?task_id=12121' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "request": {
  "video_clip_json": [
    {
      "timestamp": "00:10-01:01",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯è§†é¢‘ç”»é¢çš„å®¢è§‚æè¿°ï¼š\n\nè§†é¢‘å±•ç°ä¸€åç•™ç€èƒ¡é¡»çš„ç”·å­åœ¨æ£®æ—é‡ŒæŒ–æ˜ã€‚\n\nç”»é¢é¦–å…ˆå±•ç°ç”·å­ä»åæ–¹è§†è§’ï¼ŒèƒŒç€å†›ç»¿è‰²èƒŒåŒ…ï¼Œç©¿ç€å¡å…¶è‰²é•¿è£¤å’Œæ·±è‰²Tæ¤ï¼Œèµ°å‘ä¸€ä¸ªæ³¥åœŸæ–œå¡ã€‚èƒŒåŒ…ä¸Šä¼¼ä¹æœ‰ä¸€ä¸ªé•å¤´ã€‚\n\nä¸‹ä¸€ä¸ªé•œå¤´ç‰¹å†™å±•ç°äº†è¯¥èƒŒåŒ…ï¼Œä¸€ä¸ªé•å¤´ä»èƒŒåŒ…é‡Œä¼¸å‡ºæ¥ï¼ŒåŒ…é‡Œè¿˜æœ‰ä¸€äº›å…¶ä»–å·¥å…·ã€‚\n\nç„¶åï¼Œè§†é¢‘æ˜¾ç¤ºè¯¥ç”·å­ç”¨é•å¤´æŒ–æ˜æ³¥åœŸæ–œå¡ã€‚\n\næ¥ä¸‹æ¥æ˜¯ä¸€äº›è¿‘æ™¯é•œå¤´ï¼Œå±•ç°ç”·å­çš„é´å­åœ¨æ³¥åœŸä¸­è¡Œèµ°ï¼Œä»¥åŠç”·å­ç”¨æ‰‹æ¸…ç†æ³¥åœŸã€‚\n\nå…¶ä»–é•œå¤´ä»ä¸åŒè§’åº¦å±•ç°è¯¥ç”·å­åœ¨æŒ–æ˜ï¼ŒåŒ…æ‹¬ä»ä¾§é¢å’Œä¸Šæ–¹ã€‚\n\nå¯ä»¥çœ‹åˆ°ä»–ç”¨å·¥å…·æŒ–æ˜ï¼Œæ¸…ç†æ³¥åœŸï¼Œå¹¶æ£€æŸ¥æŒ–å‡ºçš„åœŸå£¤ã€‚\n\næœ€åï¼Œä¸€ä¸ªé•œå¤´å±•ç°äº†æŒ–å‡ºçš„åœŸå£¤çš„è´¨åœ°å’Œé¢œè‰²ã€‚",
      "narration": "å¥½çš„ï¼Œæ¥ä¸‹æ¥å°±æ˜¯æˆ‘ä»¬è¿™ä½â€œèƒ¡é¡»å¤§ä¾ â€çš„ç²¾å½©å†’é™©äº†ï¼åªè§ä»–èƒŒç€å†›ç»¿è‰²çš„èƒŒåŒ…ï¼Œè¿ˆç€æ¯”æˆ‘ä¸Šç­è¿˜ä¸æƒ…æ„¿çš„æ­¥ä¼èµ°å‘é‚£æ³¥åœŸæ–œå¡ã€‚å“å‘€ï¼Œè¿™ä¸ªèƒŒåŒ…å¯çœŸæ˜¯ä¸ªå®è´ï¼Œé‡Œé¢è—ç€ä¸€æŠŠé•å¤´å’Œä¸€äº›å·¥å…·ï¼Œç®€ç›´åƒæ˜¯ä¸ªéšèº«æºå¸¦çš„â€œå»ºç­‘å·¥å…·ç®±â€ï¼ \n\nçœ‹ä»–æŒ¥èˆç€é•å¤´ï¼ŒæŒ–æ˜æ³¥åœŸçš„å§¿åŠ¿ï¼Œä»¿ä½›åœ¨è¿›è¡Œä¸€åœºâ€œæŒ–åœŸå¤§èµ›â€ï¼Œç»“æœå´æ¯”æˆ‘åšé¥­è¿˜è¦ç³Ÿç³•ã€‚æ³¥åœŸé£æ‰¬ä¸­ï¼Œä»–çš„é´å­ä¹Ÿæˆäº†â€œæ³¥å·´è‰ºæœ¯å®¶â€ã€‚æœ€åï¼Œé‚£å †è‰²æ³½å„å¼‚çš„åœŸå£¤å°±åƒä»–å¿ƒæƒ…çš„å†™ç…§â€”â€”äº”å½©æ–‘æ–“åˆç•¥æ˜¾æ··ä¹±ï¼çœŸæ˜¯ä¸€æ¬¡è®©äººæ§è…¹çš„å»ºé€ ä¹‹æ—…ï¼",
      "OST": 2,
      "new_timestamp": "00:00-00:51"
    },
    {
      "timestamp": "01:07-01:53",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯è§†é¢‘ç”»é¢çš„å®¢è§‚æè¿°ï¼š\n\nè§†é¢‘ä»¥ä¸€ç³»åˆ—æ£®æ—ç¯å¢ƒçš„é•œå¤´å¼€å¤´ã€‚\n\nç¬¬ä¸€ä¸ªé•œå¤´æ˜¯ä¸€ä¸ªç‰¹å†™é•œå¤´ï¼Œé•œå¤´ä¸­æ˜¾ç¤ºçš„æ˜¯ä¸€äº›å¸¦æœ‰æ°´æ»´çš„ç»¿è‰²å¶å­ã€‚\n\nç¬¬äºŒä¸ªé•œå¤´æ˜¾ç¤ºä¸€ä¸ªç•™ç€èƒ¡é¡»çš„ç”·å­åœ¨æ£®æ—ä¸­æŒ–æ˜ä¸€ä¸ªæ´ã€‚ ä»–è·ªåœ¨åœ°ä¸Šï¼Œç”¨å·¥å…·æŒ–åœŸã€‚\n\nç¬¬ä¸‰ä¸ªé•œå¤´æ˜¯ä¸€ä¸ªä¸­ç­‰é•œå¤´ï¼Œæ˜¾ç¤ºåŒä¸€ä¸ªäººååœ¨ä»–æŒ–å¥½çš„æ´è¾¹ä¼‘æ¯ã€‚\n\nç¬¬å››ä¸ªé•œå¤´æ˜¾ç¤ºè¯¥æ´çš„å†…éƒ¨ç»“æ„ï¼Œè¯¥æ´åœ¨æ ‘æ ¹å’Œåœ°é¢ä¹‹é—´ã€‚\n\nç¬¬äº”ä¸ªé•œå¤´æ˜¾ç¤ºè¯¥ç”·å­ç”¨æ–§å¤´ç æ ‘æã€‚\n\nç¬¬å…­ä¸ªé•œå¤´æ˜¾ç¤ºä¸€å †æ ‘ææ¨ªè·¨ä¸€ä¸ªæ³¥æ³çš„å°æ°´å‘ã€‚\n\nç¬¬ä¸ƒä¸ªé•œå¤´æ˜¾ç¤ºæ›´å¤šèŒ‚ç››çš„æ ‘å¶å’Œæ ‘æåœ¨é˜³å…‰ä¸‹ã€‚\n\nç¬¬å…«ä¸ªé•œå¤´æ˜¾ç¤ºæ›´å¤šèŒ‚ç››çš„æ ‘å¶å’Œæ ‘æã€‚\n\n\n",
      "narration": "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çš„â€œæŒ–åœŸå¤§å¸ˆâ€åˆå¼€å§‹äº†ä»–çš„æ£®æ—æ¢é™©ã€‚çœ‹è¿™é•œå¤´ï¼Œæ°´æ»´åœ¨å¶å­ä¸Šé—ªçƒï¼Œä»¿ä½›åœ¨è¯´ï¼šâ€œå¿«æ¥ï¼Œå¿«æ¥ï¼Œè¿™é‡Œæœ‰æ•…äº‹ï¼â€ä»–ä¸€è¾¹æŒ–æ´ï¼Œä¸€è¾¹åƒä¸ªæ–°æ‰‹å¨å¸ˆè¯•å›¾åˆ‡æ´‹è‘±â€”â€”æ¯ä¸€ä¸‹éƒ½å°å¿ƒç¿¼ç¿¼ï¼Œç”Ÿæ€•è‡ªå·±ä¸å°å¿ƒæŒ–å‡ºä¸ªâ€œå†å²é—å€â€ã€‚åä¸‹ä¼‘æ¯çš„æ—¶å€™ï¼Œè„¸ä¸Šçš„è¡¨æƒ…å°±åƒå‘ç°æ–°å¤§é™†ä¸€æ ·ï¼ç„¶åï¼Œä»–æ‹¿èµ·æ–§å¤´ç æ ‘æï¼Œç®€ç›´æ˜¯ç°ä»£ç‰ˆçš„â€œç¥é›•ä¾ ä¾£â€ï¼Œåªä¸è¿‡å¯¹è±¡æ˜¯æ ‘æœ¨ã€‚æœ€åï¼Œé‚£å †æ ‘ææ¶è¿‡æ³¥æ³çš„å°æ°´å‘ï¼Œä»¿ä½›åœ¨è¯´ï¼šâ€œæˆ‘å°±æ˜¯ä¸æ€•æ¹¿è„šçš„å‹‡å£«ï¼â€è¿™å°±æ˜¯æˆ‘ä»¬çš„å»ºé€ ä¹‹æ—…ï¼",
      "OST": 2,
      "new_timestamp": "00:51-01:37"
    }
  ],
  "video_clip_json_path": "E:\\projects\\NarratoAI\\resource\\scripts\\2024-1118-230421.json",
  "video_origin_path": "E:\\projects\\NarratoAI\\resource\\videos\\test.mp4",
  "video_aspect": "16:9",
  "video_language": "zh-CN",
  "voice_name": "zh-CN-YunjianNeural",
  "voice_volume": 1,
  "voice_rate": 1.2,
  "voice_pitch": 1,
  "bgm_name": "random",
  "bgm_type": "random",
  "bgm_file": "",
  "bgm_volume": 0.3,
  "subtitle_enabled": true,
  "subtitle_position": "bottom",
  "font_name": "STHeitiMedium.ttc",
  "text_fore_color": "#FFFFFF",
  "text_background_color": "transparent",
  "font_size": 75,
  "stroke_color": "#000000",
  "stroke_width": 1.5,
  "custom_position": 70,
  "n_threads": 8
  },
  "subclip_videos": {
    "00:10-01:01": "E:\\projects\\NarratoAI\\storage\\cache_videos/vid-00_10-01_01.mp4",
    "01:07-01:53": "E:\\projects\\NarratoAI\\storage\\cache_videos/vid-01_07-01_53.mp4"
  }
}'
{
  "request": {
  "video_clip_json": [
    {
      "timestamp": "00:10-01:01",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯è§†é¢‘ç”»é¢çš„å®¢è§‚æè¿°ï¼š\n\nè§†é¢‘å±•ç°ä¸€åç•™ç€èƒ¡é¡»çš„ç”·å­åœ¨æ£®æ—é‡ŒæŒ–æ˜ã€‚\n\nç”»é¢é¦–å…ˆå±•ç°ç”·å­ä»åæ–¹è§†è§’ï¼ŒèƒŒç€å†›ç»¿è‰²èƒŒåŒ…ï¼Œç©¿ç€å¡å…¶è‰²é•¿è£¤å’Œæ·±è‰²Tæ¤ï¼Œèµ°å‘ä¸€ä¸ªæ³¥åœŸæ–œå¡ã€‚èƒŒåŒ…ä¸Šä¼¼ä¹æœ‰ä¸€ä¸ªé•å¤´ã€‚\n\nä¸‹ä¸€ä¸ªé•œå¤´ç‰¹å†™å±•ç°äº†è¯¥èƒŒåŒ…ï¼Œä¸€ä¸ªé•å¤´ä»èƒŒåŒ…é‡Œä¼¸å‡ºæ¥ï¼ŒåŒ…é‡Œè¿˜æœ‰ä¸€äº›å…¶ä»–å·¥å…·ã€‚\n\nç„¶åï¼Œè§†é¢‘æ˜¾ç¤ºè¯¥ç”·å­ç”¨é•å¤´æŒ–æ˜æ³¥åœŸæ–œå¡ã€‚\n\næ¥ä¸‹æ¥æ˜¯ä¸€äº›è¿‘æ™¯é•œå¤´ï¼Œå±•ç°ç”·å­çš„é´å­åœ¨æ³¥åœŸä¸­è¡Œèµ°ï¼Œä»¥åŠç”·å­ç”¨æ‰‹æ¸…ç†æ³¥åœŸã€‚\n\nå…¶ä»–é•œå¤´ä»ä¸åŒè§’åº¦å±•ç°è¯¥ç”·å­åœ¨æŒ–æ˜ï¼ŒåŒ…æ‹¬ä»ä¾§é¢å’Œä¸Šæ–¹ã€‚\n\nå¯ä»¥çœ‹åˆ°ä»–ç”¨å·¥å…·æŒ–æ˜ï¼Œæ¸…ç†æ³¥åœŸï¼Œå¹¶æ£€æŸ¥æŒ–å‡ºçš„åœŸå£¤ã€‚\n\næœ€åï¼Œä¸€ä¸ªé•œå¤´å±•ç°äº†æŒ–å‡ºçš„åœŸå£¤çš„è´¨åœ°å’Œé¢œè‰²ã€‚",
      "narration": "å¥½çš„ï¼Œæ¥ä¸‹æ¥å°±æ˜¯æˆ‘ä»¬è¿™ä½â€œèƒ¡é¡»å¤§ä¾ â€çš„ç²¾å½©å†’é™©äº†ï¼åªè§ä»–èƒŒç€å†›ç»¿è‰²çš„èƒŒåŒ…ï¼Œè¿ˆç€æ¯”æˆ‘ä¸Šç­è¿˜ä¸æƒ…æ„¿çš„æ­¥ä¼èµ°å‘é‚£æ³¥åœŸæ–œå¡ã€‚å“å‘€ï¼Œè¿™ä¸ªèƒŒåŒ…å¯çœŸæ˜¯ä¸ªå®è´ï¼Œé‡Œé¢è—ç€ä¸€æŠŠé•å¤´å’Œä¸€äº›å·¥å…·ï¼Œç®€ç›´åƒæ˜¯ä¸ªéšèº«æºå¸¦çš„â€œå»ºç­‘å·¥å…·ç®±â€ï¼ \n\nçœ‹ä»–æŒ¥èˆç€é•å¤´ï¼ŒæŒ–æ˜æ³¥åœŸçš„å§¿åŠ¿ï¼Œä»¿ä½›åœ¨è¿›è¡Œä¸€åœºâ€œæŒ–åœŸå¤§èµ›â€ï¼Œç»“æœå´æ¯”æˆ‘åšé¥­è¿˜è¦ç³Ÿç³•ã€‚æ³¥åœŸé£æ‰¬ä¸­ï¼Œä»–çš„é´å­ä¹Ÿæˆäº†â€œæ³¥å·´è‰ºæœ¯å®¶â€ã€‚æœ€åï¼Œé‚£å †è‰²æ³½å„å¼‚çš„åœŸå£¤å°±åƒä»–å¿ƒæƒ…çš„å†™ç…§â€”â€”äº”å½©æ–‘æ–“åˆç•¥æ˜¾æ··ä¹±ï¼çœŸæ˜¯ä¸€æ¬¡è®©äººæ§è…¹çš„å»ºé€ ä¹‹æ—…ï¼",
      "OST": 2,
      "new_timestamp": "00:00-00:51"
    },
    {
      "timestamp": "01:07-01:53",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯è§†é¢‘ç”»é¢çš„å®¢è§‚æè¿°ï¼š\n\nè§†é¢‘ä»¥ä¸€ç³»åˆ—æ£®æ—ç¯å¢ƒçš„é•œå¤´å¼€å¤´ã€‚\n\nç¬¬ä¸€ä¸ªé•œå¤´æ˜¯ä¸€ä¸ªç‰¹å†™é•œå¤´ï¼Œé•œå¤´ä¸­æ˜¾ç¤ºçš„æ˜¯ä¸€äº›å¸¦æœ‰æ°´æ»´çš„ç»¿è‰²å¶å­ã€‚\n\nç¬¬äºŒä¸ªé•œå¤´æ˜¾ç¤ºä¸€ä¸ªç•™ç€èƒ¡é¡»çš„ç”·å­åœ¨æ£®æ—ä¸­æŒ–æ˜ä¸€ä¸ªæ´ã€‚ ä»–è·ªåœ¨åœ°ä¸Šï¼Œç”¨å·¥å…·æŒ–åœŸã€‚\n\nç¬¬ä¸‰ä¸ªé•œå¤´æ˜¯ä¸€ä¸ªä¸­ç­‰é•œå¤´ï¼Œæ˜¾ç¤ºåŒä¸€ä¸ªäººååœ¨ä»–æŒ–å¥½çš„æ´è¾¹ä¼‘æ¯ã€‚\n\nç¬¬å››ä¸ªé•œå¤´æ˜¾ç¤ºè¯¥æ´çš„å†…éƒ¨ç»“æ„ï¼Œè¯¥æ´åœ¨æ ‘æ ¹å’Œåœ°é¢ä¹‹é—´ã€‚\n\nç¬¬äº”ä¸ªé•œå¤´æ˜¾ç¤ºè¯¥ç”·å­ç”¨æ–§å¤´ç æ ‘æã€‚\n\nç¬¬å…­ä¸ªé•œå¤´æ˜¾ç¤ºä¸€å †æ ‘ææ¨ªè·¨ä¸€ä¸ªæ³¥æ³çš„å°æ°´å‘ã€‚\n\nç¬¬ä¸ƒä¸ªé•œå¤´æ˜¾ç¤ºæ›´å¤šèŒ‚ç››çš„æ ‘å¶å’Œæ ‘æåœ¨é˜³å…‰ä¸‹ã€‚\n\nç¬¬å…«ä¸ªé•œå¤´æ˜¾ç¤ºæ›´å¤šèŒ‚ç››çš„æ ‘å¶å’Œæ ‘æã€‚\n\n\n",
      "narration": "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çš„â€œæŒ–åœŸå¤§å¸ˆâ€åˆå¼€å§‹äº†ä»–çš„æ£®æ—æ¢é™©ã€‚çœ‹è¿™é•œå¤´ï¼Œæ°´æ»´åœ¨å¶å­ä¸Šé—ªçƒï¼Œä»¿ä½›åœ¨è¯´ï¼šâ€œå¿«æ¥ï¼Œå¿«æ¥ï¼Œè¿™é‡Œæœ‰æ•…äº‹ï¼â€ä»–ä¸€è¾¹æŒ–æ´ï¼Œä¸€è¾¹åƒä¸ªæ–°æ‰‹å¨å¸ˆè¯•å›¾åˆ‡æ´‹è‘±â€”â€”æ¯ä¸€ä¸‹éƒ½å°å¿ƒç¿¼ç¿¼ï¼Œç”Ÿæ€•è‡ªå·±ä¸å°å¿ƒæŒ–å‡ºä¸ªâ€œå†å²é—å€â€ã€‚åä¸‹ä¼‘æ¯çš„æ—¶å€™ï¼Œè„¸ä¸Šçš„è¡¨æƒ…å°±åƒå‘ç°æ–°å¤§é™†ä¸€æ ·ï¼ç„¶åï¼Œä»–æ‹¿èµ·æ–§å¤´ç æ ‘æï¼Œç®€ç›´æ˜¯ç°ä»£ç‰ˆçš„â€œç¥é›•ä¾ ä¾£â€ï¼Œåªä¸è¿‡å¯¹è±¡æ˜¯æ ‘æœ¨ã€‚æœ€åï¼Œé‚£å †æ ‘ææ¶è¿‡æ³¥æ³çš„å°æ°´å‘ï¼Œä»¿ä½›åœ¨è¯´ï¼šâ€œæˆ‘å°±æ˜¯ä¸æ€•æ¹¿è„šçš„å‹‡å£«ï¼â€è¿™å°±æ˜¯æˆ‘ä»¬çš„å»ºé€ ä¹‹æ—…ï¼",
      "OST": 2,
      "new_timestamp": "00:51-01:37"
    }
  ],
  "video_clip_json_path": "E:\\projects\\NarratoAI\\resource\\scripts\\2024-1118-230421.json",
  "video_origin_path": "E:\\projects\\NarratoAI\\resource\\videos\\test.mp4",
  "video_aspect": "16:9",
  "video_language": "zh-CN",
  "voice_name": "zh-CN-YunjianNeural",
  "voice_volume": 1,
  "voice_rate": 1.2,
  "voice_pitch": 1,
  "bgm_name": "random",
  "bgm_type": "random",
  "bgm_file": "",
  "bgm_volume": 0.3,
  "subtitle_enabled": true,
  "subtitle_position": "bottom",
  "font_name": "STHeitiMedium.ttc",
  "text_fore_color": "#FFFFFF",
  "text_background_color": "transparent",
  "font_size": 75,
  "stroke_color": "#000000",
  "stroke_width": 1.5,
  "custom_position": 70,
  "n_threads": 8
  },
  "subclip_videos": {
    "00:10-01:01": "E:\\projects\\NarratoAI\\storage\\cache_videos/vid-00_10-01_01.mp4",
    "01:07-01:53": "E:\\projects\\NarratoAI\\storage\\cache_videos/vid-01_07-01_53.mp4"
  }
}


è¯·åœ¨æœ€å¤–å±‚æ–°å»ºä¸€ä¸ªpipeline å·¥ä½œæµæ‰§è¡Œé€»è¾‘çš„ä»£ç ï¼›
ä»–ä¼šæŒ‰ç…§ä¸‹é¢çš„é¡ºåºè¯·æ±‚æ¥å£
1.ä¸‹è½½è§†é¢‘
curl -X 'POST' \
  'http://127.0.0.1:8080/api/v2/youtube/download' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "url": "https://www.youtube.com/watch?v=Kenm35gdqtk",
  "resolution": "1080p",
  "output_format": "mp4",
  "rename": "2024-11-19"
}'
2.ç”Ÿæˆè„šæœ¬
curl -X 'POST' \
  'http://127.0.0.1:8080/api/v2/scripts/generate' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "video_path": "E:\\projects\\NarratoAI\\resource\\videos\\test.mp4",
  "skip_seconds": 0,
  "threshold": 30,
  "vision_batch_size": 10,
  "vision_llm_provider": "gemini"
}'
3. å‰ªè¾‘è§†é¢‘
curl -X 'POST' \
  'http://127.0.0.1:8080/api/v2/scripts/crop' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "video_origin_path": "E:\\projects\\NarratoAI\\resource\\videos\\test.mp4",
  "video_script": [
    {
      "timestamp": "00:10-01:01",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯è§†é¢‘ç”»é¢çš„å®¢è§‚æè¿°ï¼š\n\nè§†é¢‘å±•ç°ä¸€åç•™ç€èƒ¡é¡»çš„ç”·å­åœ¨æ£®æ—é‡ŒæŒ–æ˜ã€‚\n\nç”»é¢é¦–å…ˆå±•ç°ç”·å­ä»åæ–¹è§†è§’ï¼ŒèƒŒç€å†›ç»¿è‰²èƒŒåŒ…ï¼Œç©¿ç€å¡å…¶è‰²é•¿è£¤å’Œæ·±è‰²Tæ¤ï¼Œèµ°å‘ä¸€ä¸ªæ³¥åœŸæ–œå¡ã€‚èƒŒåŒ…ä¸Šä¼¼ä¹æœ‰ä¸€ä¸ªé•å¤´ã€‚\n\nä¸‹ä¸€ä¸ªé•œå¤´ç‰¹å†™å±•ç°äº†è¯¥èƒŒåŒ…ï¼Œä¸€ä¸ªé•å¤´ä»èƒŒåŒ…é‡Œä¼¸å‡ºæ¥ï¼ŒåŒ…é‡Œè¿˜æœ‰ä¸€äº›å…¶ä»–å·¥å…·ã€‚\n\nç„¶åï¼Œè§†é¢‘æ˜¾ç¤ºè¯¥ç”·å­ç”¨é•å¤´æŒ–æ˜æ³¥åœŸæ–œå¡ã€‚\n\næ¥ä¸‹æ¥æ˜¯ä¸€äº›è¿‘æ™¯é•œå¤´ï¼Œå±•ç°ç”·å­çš„é´å­åœ¨æ³¥åœŸä¸­è¡Œèµ°ï¼Œä»¥åŠç”·å­ç”¨æ‰‹æ¸…ç†æ³¥åœŸã€‚\n\nå…¶ä»–é•œå¤´ä»ä¸åŒè§’åº¦å±•ç°è¯¥ç”·å­åœ¨æŒ–æ˜ï¼ŒåŒ…æ‹¬ä»ä¾§é¢å’Œä¸Šæ–¹ã€‚\n\nå¯ä»¥çœ‹åˆ°ä»–ç”¨å·¥å…·æŒ–æ˜ï¼Œæ¸…ç†æ³¥åœŸï¼Œå¹¶æ£€æŸ¥æŒ–å‡ºçš„åœŸå£¤ã€‚\n\næœ€åï¼Œä¸€ä¸ªé•œå¤´å±•ç°äº†æŒ–å‡ºçš„åœŸå£¤çš„è´¨åœ°å’Œé¢œè‰²ã€‚",
      "narration": "å¥½çš„ï¼Œæ¥ä¸‹æ¥å°±æ˜¯æˆ‘ä»¬è¿™ä½â€œèƒ¡é¡»å¤§ä¾ â€çš„ç²¾å½©å†’é™©äº†ï¼åªè§ä»–èƒŒç€å†›ç»¿è‰²çš„èƒŒåŒ…ï¼Œè¿ˆç€æ¯”æˆ‘ä¸Šç­è¿˜ä¸æƒ…æ„¿çš„æ­¥ä¼èµ°å‘é‚£æ³¥åœŸæ–œå¡ã€‚å“å‘€ï¼Œè¿™ä¸ªèƒŒåŒ…å¯çœŸæ˜¯ä¸ªå®è´ï¼Œé‡Œé¢è—ç€ä¸€æŠŠé•å¤´å’Œä¸€äº›å·¥å…·ï¼Œç®€ç›´åƒæ˜¯ä¸ªéšèº«æºå¸¦çš„â€œå»ºç­‘å·¥å…·ç®±â€ï¼ \n\nçœ‹ä»–æŒ¥èˆç€é•å¤´ï¼ŒæŒ–æ˜æ³¥åœŸçš„å§¿åŠ¿ï¼Œä»¿ä½›åœ¨è¿›è¡Œä¸€åœºâ€œæŒ–åœŸå¤§èµ›â€ï¼Œç»“æœå´æ¯”æˆ‘åšé¥­è¿˜è¦ç³Ÿç³•ã€‚æ³¥åœŸé£æ‰¬ä¸­ï¼Œä»–çš„é´å­ä¹Ÿæˆäº†â€œæ³¥å·´è‰ºæœ¯å®¶â€ã€‚æœ€åï¼Œé‚£å †è‰²æ³½å„å¼‚çš„åœŸå£¤å°±åƒä»–å¿ƒæƒ…çš„å†™ç…§â€”â€”äº”å½©æ–‘æ–“åˆç•¥æ˜¾æ··ä¹±ï¼çœŸæ˜¯ä¸€æ¬¡è®©äººæ§è…¹çš„å»ºé€ ä¹‹æ—…ï¼",
      "OST": 2,
      "new_timestamp": "00:00-00:51"
    },
    {
      "timestamp": "01:07-01:53",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯è§†é¢‘ç”»é¢çš„å®¢è§‚æè¿°ï¼š\n\nè§†é¢‘ä»¥ä¸€ç³»åˆ—æ£®æ—ç¯å¢ƒçš„é•œå¤´å¼€å¤´ã€‚\n\nç¬¬ä¸€ä¸ªé•œå¤´æ˜¯ä¸€ä¸ªç‰¹å†™é•œå¤´ï¼Œé•œå¤´ä¸­æ˜¾ç¤ºçš„æ˜¯ä¸€äº›å¸¦æœ‰æ°´æ»´çš„ç»¿è‰²å¶å­ã€‚\n\nç¬¬äºŒä¸ªé•œå¤´æ˜¾ç¤ºä¸€ä¸ªç•™ç€èƒ¡é¡»çš„ç”·å­åœ¨æ£®æ—ä¸­æŒ–æ˜ä¸€ä¸ªæ´ã€‚ ä»–è·ªåœ¨åœ°ä¸Šï¼Œç”¨å·¥å…·æŒ–åœŸã€‚\n\nç¬¬ä¸‰ä¸ªé•œå¤´æ˜¯ä¸€ä¸ªä¸­ç­‰é•œå¤´ï¼Œæ˜¾ç¤ºåŒä¸€ä¸ªäººååœ¨ä»–æŒ–å¥½çš„æ´è¾¹ä¼‘æ¯ã€‚\n\nç¬¬å››ä¸ªé•œå¤´æ˜¾ç¤ºè¯¥æ´çš„å†…éƒ¨ç»“æ„ï¼Œè¯¥æ´åœ¨æ ‘æ ¹å’Œåœ°é¢ä¹‹é—´ã€‚\n\nç¬¬äº”ä¸ªé•œå¤´æ˜¾ç¤ºè¯¥ç”·å­ç”¨æ–§å¤´ç æ ‘æã€‚\n\nç¬¬å…­ä¸ªé•œå¤´æ˜¾ç¤ºä¸€å †æ ‘ææ¨ªè·¨ä¸€ä¸ªæ³¥æ³çš„å°æ°´å‘ã€‚\n\nç¬¬ä¸ƒä¸ªé•œå¤´æ˜¾ç¤ºæ›´å¤šèŒ‚ç››çš„æ ‘å¶å’Œæ ‘æåœ¨é˜³å…‰ä¸‹ã€‚\n\nç¬¬å…«ä¸ªé•œå¤´æ˜¾ç¤ºæ›´å¤šèŒ‚ç››çš„æ ‘å¶å’Œæ ‘æã€‚\n\n\n",
      "narration": "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çš„â€œæŒ–åœŸå¤§å¸ˆâ€åˆå¼€å§‹äº†ä»–çš„æ£®æ—æ¢é™©ã€‚çœ‹è¿™é•œå¤´ï¼Œæ°´æ»´åœ¨å¶å­ä¸Šé—ªçƒï¼Œä»¿ä½›åœ¨è¯´ï¼šâ€œå¿«æ¥ï¼Œå¿«æ¥ï¼Œè¿™é‡Œæœ‰æ•…äº‹ï¼â€ä»–ä¸€è¾¹æŒ–æ´ï¼Œä¸€è¾¹åƒä¸ªæ–°æ‰‹å¨å¸ˆè¯•å›¾åˆ‡æ´‹è‘±â€”â€”æ¯ä¸€ä¸‹éƒ½å°å¿ƒç¿¼ç¿¼ï¼Œç”Ÿæ€•è‡ªå·±ä¸å°å¿ƒæŒ–å‡ºä¸ªâ€œå†å²é—å€â€ã€‚åä¸‹ä¼‘æ¯çš„æ—¶å€™ï¼Œè„¸ä¸Šçš„è¡¨æƒ…å°±åƒå‘ç°æ–°å¤§é™†ä¸€æ ·ï¼ç„¶åï¼Œä»–æ‹¿èµ·æ–§å¤´ç æ ‘æï¼Œç®€ç›´æ˜¯ç°ä»£ç‰ˆçš„â€œç¥é›•ä¾ ä¾£â€ï¼Œåªä¸è¿‡å¯¹è±¡æ˜¯æ ‘æœ¨ã€‚æœ€åï¼Œé‚£å †æ ‘ææ¶è¿‡æ³¥æ³çš„å°æ°´å‘ï¼Œä»¿ä½›åœ¨è¯´ï¼šâ€œæˆ‘å°±æ˜¯ä¸æ€•æ¹¿è„šçš„å‹‡å£«ï¼â€è¿™å°±æ˜¯æˆ‘ä»¬çš„å»ºé€ ä¹‹æ—…ï¼",
      "OST": 2,
      "new_timestamp": "00:51-01:37"
    }
  ]
}'
4.ç”Ÿæˆè§†é¢‘
curl -X 'POST' \
  'http://127.0.0.1:8080/api/v2/scripts/start-subclip?task_id=12121' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "request": {
  "video_clip_json": [
    {
      "timestamp": "00:10-01:01",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯è§†é¢‘ç”»é¢çš„å®¢è§‚æè¿°ï¼š\n\nè§†é¢‘å±•ç°ä¸€åç•™ç€èƒ¡é¡»çš„ç”·å­åœ¨æ£®æ—é‡ŒæŒ–æ˜ã€‚\n\nç”»é¢é¦–å…ˆå±•ç°ç”·å­ä»åæ–¹è§†è§’ï¼ŒèƒŒç€å†›ç»¿è‰²èƒŒåŒ…ï¼Œç©¿ç€å¡å…¶è‰²é•¿è£¤å’Œæ·±è‰²Tæ¤ï¼Œèµ°å‘ä¸€ä¸ªæ³¥åœŸæ–œå¡ã€‚èƒŒåŒ…ä¸Šä¼¼ä¹æœ‰ä¸€ä¸ªé•å¤´ã€‚\n\nä¸‹ä¸€ä¸ªé•œå¤´ç‰¹å†™å±•ç°äº†è¯¥èƒŒåŒ…ï¼Œä¸€ä¸ªé•å¤´ä»èƒŒåŒ…é‡Œä¼¸å‡ºæ¥ï¼ŒåŒ…é‡Œè¿˜æœ‰ä¸€äº›å…¶ä»–å·¥å…·ã€‚\n\nç„¶åï¼Œè§†é¢‘æ˜¾ç¤ºè¯¥ç”·å­ç”¨é•å¤´æŒ–æ˜æ³¥åœŸæ–œå¡ã€‚\n\næ¥ä¸‹æ¥æ˜¯ä¸€äº›è¿‘æ™¯é•œå¤´ï¼Œå±•ç°ç”·å­çš„é´å­åœ¨æ³¥åœŸä¸­è¡Œèµ°ï¼Œä»¥åŠç”·å­ç”¨æ‰‹æ¸…ç†æ³¥åœŸã€‚\n\nå…¶ä»–é•œå¤´ä»ä¸åŒè§’åº¦å±•ç°è¯¥ç”·å­åœ¨æŒ–æ˜ï¼ŒåŒ…æ‹¬ä»ä¾§é¢å’Œä¸Šæ–¹ã€‚\n\nå¯ä»¥çœ‹åˆ°ä»–ç”¨å·¥å…·æŒ–æ˜ï¼Œæ¸…ç†æ³¥åœŸï¼Œå¹¶æ£€æŸ¥æŒ–å‡ºçš„åœŸå£¤ã€‚\n\næœ€åï¼Œä¸€ä¸ªé•œå¤´å±•ç°äº†æŒ–å‡ºçš„åœŸå£¤çš„è´¨åœ°å’Œé¢œè‰²ã€‚",
      "narration": "å¥½çš„ï¼Œæ¥ä¸‹æ¥å°±æ˜¯æˆ‘ä»¬è¿™ä½â€œèƒ¡é¡»å¤§ä¾ â€çš„ç²¾å½©å†’é™©äº†ï¼åªè§ä»–èƒŒç€å†›ç»¿è‰²çš„èƒŒåŒ…ï¼Œè¿ˆç€æ¯”æˆ‘ä¸Šç­è¿˜ä¸æƒ…æ„¿çš„æ­¥ä¼èµ°å‘é‚£æ³¥åœŸæ–œå¡ã€‚å“å‘€ï¼Œè¿™ä¸ªèƒŒåŒ…å¯çœŸæ˜¯ä¸ªå®è´ï¼Œé‡Œé¢è—ç€ä¸€æŠŠé•å¤´å’Œä¸€äº›å·¥å…·ï¼Œç®€ç›´åƒæ˜¯ä¸ªéšèº«æºå¸¦çš„â€œå»ºç­‘å·¥å…·ç®±â€ï¼ \n\nçœ‹ä»–æŒ¥èˆç€é•å¤´ï¼ŒæŒ–æ˜æ³¥åœŸçš„å§¿åŠ¿ï¼Œä»¿ä½›åœ¨è¿›è¡Œä¸€åœºâ€œæŒ–åœŸå¤§èµ›â€ï¼Œç»“æœå´æ¯”æˆ‘åšé¥­è¿˜è¦ç³Ÿç³•ã€‚æ³¥åœŸé£æ‰¬ä¸­ï¼Œä»–çš„é´å­ä¹Ÿæˆäº†â€œæ³¥å·´è‰ºæœ¯å®¶â€ã€‚æœ€åï¼Œé‚£å †è‰²æ³½å„å¼‚çš„åœŸå£¤å°±åƒä»–å¿ƒæƒ…çš„å†™ç…§â€”â€”äº”å½©æ–‘æ–“åˆç•¥æ˜¾æ··ä¹±ï¼çœŸæ˜¯ä¸€æ¬¡è®©äººæ§è…¹çš„å»ºé€ ä¹‹æ—…ï¼",
      "OST": 2,
      "new_timestamp": "00:00-00:51"
    },
    {
      "timestamp": "01:07-01:53",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯è§†é¢‘ç”»é¢çš„å®¢è§‚æè¿°ï¼š\n\nè§†é¢‘ä»¥ä¸€ç³»åˆ—æ£®æ—ç¯å¢ƒçš„é•œå¤´å¼€å¤´ã€‚\n\nç¬¬ä¸€ä¸ªé•œå¤´æ˜¯ä¸€ä¸ªç‰¹å†™é•œå¤´ï¼Œé•œå¤´ä¸­æ˜¾ç¤ºçš„æ˜¯ä¸€äº›å¸¦æœ‰æ°´æ»´çš„ç»¿è‰²å¶å­ã€‚\n\nç¬¬äºŒä¸ªé•œå¤´æ˜¾ç¤ºä¸€ä¸ªç•™ç€èƒ¡é¡»çš„ç”·å­åœ¨æ£®æ—ä¸­æŒ–æ˜ä¸€ä¸ªæ´ã€‚ ä»–è·ªåœ¨åœ°ä¸Šï¼Œç”¨å·¥å…·æŒ–åœŸã€‚\n\nç¬¬ä¸‰ä¸ªé•œå¤´æ˜¯ä¸€ä¸ªä¸­ç­‰é•œå¤´ï¼Œæ˜¾ç¤ºåŒä¸€ä¸ªäººååœ¨ä»–æŒ–å¥½çš„æ´è¾¹ä¼‘æ¯ã€‚\n\nç¬¬å››ä¸ªé•œå¤´æ˜¾ç¤ºè¯¥æ´çš„å†…éƒ¨ç»“æ„ï¼Œè¯¥æ´åœ¨æ ‘æ ¹å’Œåœ°é¢ä¹‹é—´ã€‚\n\nç¬¬äº”ä¸ªé•œå¤´æ˜¾ç¤ºè¯¥ç”·å­ç”¨æ–§å¤´ç æ ‘æã€‚\n\nç¬¬å…­ä¸ªé•œå¤´æ˜¾ç¤ºä¸€å †æ ‘ææ¨ªè·¨ä¸€ä¸ªæ³¥æ³çš„å°æ°´å‘ã€‚\n\nç¬¬ä¸ƒä¸ªé•œå¤´æ˜¾ç¤ºæ›´å¤šèŒ‚ç››çš„æ ‘å¶å’Œæ ‘æåœ¨é˜³å…‰ä¸‹ã€‚\n\nç¬¬å…«ä¸ªé•œå¤´æ˜¾ç¤ºæ›´å¤šèŒ‚ç››çš„æ ‘å¶å’Œæ ‘æã€‚\n\n\n",
      "narration": "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çš„â€œæŒ–åœŸå¤§å¸ˆâ€åˆå¼€å§‹äº†ä»–çš„æ£®æ—æ¢é™©ã€‚çœ‹è¿™é•œå¤´ï¼Œæ°´æ»´åœ¨å¶å­ä¸Šé—ªçƒï¼Œä»¿ä½›åœ¨è¯´ï¼šâ€œå¿«æ¥ï¼Œå¿«æ¥ï¼Œè¿™é‡Œæœ‰æ•…äº‹ï¼â€ä»–ä¸€è¾¹æŒ–æ´ï¼Œä¸€è¾¹åƒä¸ªæ–°æ‰‹å¨å¸ˆè¯•å›¾åˆ‡æ´‹è‘±â€”â€”æ¯ä¸€ä¸‹éƒ½å°å¿ƒç¿¼ç¿¼ï¼Œç”Ÿæ€•è‡ªå·±ä¸å°å¿ƒæŒ–å‡ºä¸ªâ€œå†å²é—å€â€ã€‚åä¸‹ä¼‘æ¯çš„æ—¶å€™ï¼Œè„¸ä¸Šçš„è¡¨æƒ…å°±åƒå‘ç°æ–°å¤§é™†ä¸€æ ·ï¼ç„¶åï¼Œä»–æ‹¿èµ·æ–§å¤´ç æ ‘æï¼Œç®€ç›´æ˜¯ç°ä»£ç‰ˆçš„â€œç¥é›•ä¾ ä¾£â€ï¼Œåªä¸è¿‡å¯¹è±¡æ˜¯æ ‘æœ¨ã€‚æœ€åï¼Œé‚£å †æ ‘ææ¶è¿‡æ³¥æ³çš„å°æ°´å‘ï¼Œä»¿ä½›åœ¨è¯´ï¼šâ€œæˆ‘å°±æ˜¯ä¸æ€•æ¹¿è„šçš„å‹‡å£«ï¼â€è¿™å°±æ˜¯æˆ‘ä»¬çš„å»ºé€ ä¹‹æ—…ï¼",
      "OST": 2,
      "new_timestamp": "00:51-01:37"
    }
  ],
  "video_clip_json_path": "E:\\projects\\NarratoAI\\resource\\scripts\\2024-1118-230421.json",
  "video_origin_path": "E:\\projects\\NarratoAI\\resource\\videos\\test.mp4",
  "video_aspect": "16:9",
  "video_language": "zh-CN",
  "voice_name": "zh-CN-YunjianNeural",
  "voice_volume": 1,
  "voice_rate": 1.2,
  "voice_pitch": 1,
  "bgm_name": "random",
  "bgm_type": "random",
  "bgm_file": "",
  "bgm_volume": 0.3,
  "subtitle_enabled": true,
  "subtitle_position": "bottom",
  "font_name": "STHeitiMedium.ttc",
  "text_fore_color": "#FFFFFF",
  "text_background_color": "transparent",
  "font_size": 75,
  "stroke_color": "#000000",
  "stroke_width": 1.5,
  "custom_position": 70,
  "n_threads": 8
  },
  "subclip_videos": {
    "00:10-01:01": "E:\\projects\\NarratoAI\\storage\\cache_videos/vid-00_10-01_01.mp4",
    "01:07-01:53": "E:\\projects\\NarratoAI\\storage\\cache_videos/vid-01_07-01_53.mp4"
  }
}'

è¯·æ±‚1ï¼Œè¿”å›çš„å‚æ•°æ˜¯ï¼š
{
  "task_id": "4e9b575f-68c0-4ae1-b218-db42b67993d0",
  "output_path": "E:\\projects\\NarratoAI\\resource\\videos\\2024-11-19.mp4",
  "resolution": "1080p",
  "format": "mp4",
  "filename": "2024-11-19.mp4"
}
output_pathéœ€è¦ä¼ é€’ç»™è¯·æ±‚2
è¯·æ±‚2ï¼Œè¿”å›æ•°æ®ä¸ºï¼š
{
  "task_id": "04497017-953c-44b4-bf1d-9d8ed3ebbbce",
  "script": [
    {
      "timestamp": "00:10-01:01",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯å°å½±ç‰‡ç•«é¢çš„å®¢è§€æè¿°ï¼š\n\nå½±ç‰‡é¡¯ç¤ºä¸€åç•™è‘—é¬é¬šçš„ç”·å­åœ¨ä¸€è™•æ¨¹æ—èŒ‚å¯†çš„æ–œå¡ä¸ŠæŒ–æ˜ã€‚\n\nç•«é¢ä¸€ï¼šç”·å­å¾å¾Œæ–¹å‡ºç¾ï¼ŒèƒŒè‘—ä¸€å€‹è»ç¶ è‰²çš„èƒŒåŒ…ï¼ŒèƒŒåŒ…è£¡ä¼¼ä¹è£æœ‰å·¥å…·ã€‚ä»–ç©¿è‘—å¡å…¶è‰²çš„é•·è¤²å’Œæ·±è‰²çš„ç™»å±±é‹ã€‚\n\nç•«é¢äºŒï¼šç‰¹å¯«é¡é ­é¡¯ç¤ºç”·å­çš„èƒŒåŒ…ï¼Œä¸€å€‹èˆŠçš„é¬é ­å¾åŒ…è£¡éœ²å‡ºä¾†ï¼ŒåŒ…è£¡é‚„æœ‰å…¶ä»–å·¥å…·ï¼ŒåŒ…æ‹¬ä¸€å€‹éŸå­ã€‚\n\nç•«é¢ä¸‰ï¼šç”·å­ç”¨é¬é ­åœ¨æ–œå¡ä¸ŠæŒ–åœŸï¼ŒèƒŒåŒ…æ”¾åœ¨ä»–æ—é‚Šã€‚\n\nç•«é¢å››ï¼šç‰¹å¯«é¡é ­é¡¯ç¤ºç”·å­çš„ç™»å±±é‹åœ¨æ³¥åœŸä¸­ã€‚\n\nç•«é¢äº”ï¼šç”·å­ååœ¨æ–œå¡ä¸Šï¼Œç”¨æ‰‹æ¸…ç†æ¨¹æ ¹å’Œæ³¥åœŸã€‚\n\nç•«é¢å…­ï¼šåœ°ä¸Šæœ‰ä¸€äº›é¬†å‹•çš„æ³¥åœŸå’Œè½è‘‰ã€‚\n\nç•«é¢ä¸ƒï¼šç”·å­çš„èƒŒåŒ…è¿‘æ™¯é¡é ­ï¼Œä»–æ­£åœ¨æŒ–æ˜ã€‚\n\nç•«é¢å…«ï¼šç”·å­åœ¨æ–œå¡ä¸ŠæŒ–æ˜ï¼Œæšèµ·ä¸€é™£å¡µåœŸã€‚\n\nç•«é¢ä¹ï¼šç‰¹å¯«é¡é ­é¡¯ç¤ºç”·å­ç”¨æ‰‹æ¸…ç†æ³¥åœŸã€‚\n\nç•«é¢åï¼šç‰¹å¯«é¡é ­é¡¯ç¤ºæŒ–å‡ºçš„æ³¥åœŸå‰–é¢ï¼Œå¯ä»¥çœ‹åˆ°åœŸå£¤çš„å±¤æ¬¡ã€‚",
      "narration": "ä¸Šä¸€ä¸ªç”»é¢æ˜¯æˆ‘åœ¨ç»ç¾çš„è‡ªç„¶ä¸­ï¼Œå‡†å¤‡å¼€å¯æˆ‘çš„â€œåœŸè±ªâ€æŒ–æ˜ä¹‹æ—…ã€‚ç°åœ¨ï¼Œä½ ä»¬çœ‹åˆ°è¿™ä½ç•™ç€èƒ¡å­çš„â€œå¤§å“¥â€ï¼Œä»–èƒŒç€ä¸ªå†›ç»¿è‰²çš„åŒ…ï¼Œé‡Œé¢è£…çš„å¯ä¸ä»…ä»…æ˜¯å·¥å…·ï¼Œè¿˜æœ‰æˆ‘å¯¹ç”Ÿæ´»çš„æ— é™çƒ­çˆ±ï¼ˆä»¥åŠä¸€ä¸ä¸å®‰ï¼‰ã€‚çœ‹ï¼è¿™æŠŠæ—§é•å¤´å°±åƒæˆ‘çš„å‰ä»»â€”â€”ç”¨èµ·æ¥è´¹åŠ²ï¼Œä½†åˆèˆä¸å¾—æ‰”æ‰ã€‚\n\nä»–åœ¨æ–œå¡ä¸ŠæŒ–åœŸï¼Œæ³¥åœŸé£æ‰¬ï¼Œä»¿ä½›åœ¨è·Ÿå¤§åœ°è¿›è¡Œä¸€åœºâ€œæ³¥å·´å¤§æˆ˜â€ã€‚æ¯ä¸€é“²ä¸‹å»ï¼Œéƒ½èƒ½å¬åˆ°å¤§åœ°å¾®å¾®çš„å‘»åŸï¼šå“å‘€ï¼Œæˆ‘è¿™é¢—å°æ ‘æ ¹å¯æ¯”æˆ‘å½“å¹´çš„æƒ…æ„Ÿçº è‘›è¿˜éš¾å¤„ç†å‘¢ï¼åˆ«æ‹…å¿ƒï¼Œè¿™äº›æ³¥åœŸå±‚æ¬¡åˆ†æ˜ï¼Œç®€ç›´å¯ä»¥å¼€ä¸ªâ€œæ³¥åœŸåšç‰©é¦†â€ã€‚æ‰€ä»¥ï¼Œæœ‹å‹ä»¬ï¼Œè·Ÿç€æˆ‘ä¸€èµ·äº«å—è¿™åœºæ³¥æ³ä¸­çš„ä¹è¶£å§ï¼",
      "OST": 2,
      "new_timestamp": "00:00-00:51"
    },
    {
      "timestamp": "01:07-01:53",
      "picture": "å¥½çš„ï¼Œä»¥ä¸‹æ˜¯å°å½±ç‰‡ç•«é¢å…§å®¹çš„å®¢è§€æè¿°ï¼š\n\nå½±ç‰‡ä»¥ä¸€ç³»åˆ—æ£®æ—ç’°å¢ƒçš„é¡é ­é–‹å§‹ã€‚ç¬¬ä¸€å€‹é¡é ­å±•ç¤ºäº†ç¶ è‘‰æ¤ç‰©çš„ç‰¹å¯«é¡é ­ï¼Œè‘‰å­ä¸Šæœ‰ä¸€äº›æ°´ç ã€‚æ¥ä¸‹ä¾†çš„é¡é ­æ˜¯ä¸€å€‹ç”·äººåœ¨æ£®æ—è£¡æŒ–æ˜ä¸€å€‹å°å‘ï¼Œä»–è·ªåœ¨åœ°ä¸Šï¼Œç”¨éŸå­æŒ–åœŸã€‚\n\næ¥ä¸‹ä¾†çš„é¡é ­æ˜¯åŒä¸€å€‹ç”·äººååœ¨ä»–æŒ–çš„å‘æ—é‚Šï¼Œæœ›è‘—å‰æ–¹ã€‚ç„¶å¾Œï¼Œé¡é ­é¡¯ç¤ºè©²å‘çš„å»£è§’é¡é ­ï¼Œé¡¯ç¤ºå…¶çµæ§‹å’Œå¤§å°ã€‚\n\nä¹‹å¾Œçš„é¡é ­ï¼ŒåŒä¸€å€‹ç”·äººåœ¨æ¨¹æ—è£¡åŠˆæŸ´ã€‚é¡é ­æœ€å¾Œå‘ˆç¾å‡ºä¸€æ½­æ¸¾æ¿çš„æ°´ï¼Œå‘¨åœç’°ç¹è‘—æ¨¹æã€‚ç„¶å¾Œé¡é ­åˆå›åˆ°äº†æ£®æ—è£¡ç”Ÿé•·èŒ‚ç››çš„æ¤ç‰©ç‰¹å¯«é¡é ­ã€‚",
      "narration": "å¥½å˜ï¼Œæœ‹å‹ä»¬ï¼Œæˆ‘ä»¬å·²ç»åœ¨æ³¥åœŸåšç‰©é¦†é‡Œæ£é¼“äº†ä¸€é˜µå­ï¼Œç°åœ¨æ˜¯æ—¶å€™è·Ÿå¤§è‡ªç„¶äº²å¯†æ¥è§¦äº†ï¼çœ‹çœ‹è¿™ç‰‡æ£®æ—ï¼Œç»¿å¶ä¸Šæ°´ç é—ªé—ªå‘å…‰ï¼Œå°±åƒæˆ‘æ›¾ç»çš„çˆ±æƒ…ï¼Œè™½ç„¶çŸ­æš‚ï¼Œå´ç¾å¾—è®©äººå¿ƒç¢ã€‚\n\nç°åœ¨ï¼Œæˆ‘åœ¨è¿™é‡ŒæŒ–ä¸ªå°å‘ï¼Œæ„Ÿè§‰è‡ªå·±å°±åƒæ˜¯ä¸€ä½æ–°æ™‹â€œæŒ–åœŸå¤§ç‹â€ï¼Œä¸è¿‡è¯´å®è¯ï¼Œè¿™æ‰‹è‰ºçœŸä¸æ•¢æ­ç»´ï¼Œè¿é“²å­éƒ½å¿«å¯¹æˆ‘å´©æºƒäº†ã€‚å†è¯´åŠˆæŸ´ï¼Œè¿™åŠ¨ä½œç®€ç›´æ¯”æˆ‘å‰ä»»çš„æƒ…ç»ªæ³¢åŠ¨è¿˜è¦æ¿€çƒˆï¼æœ€åè¿™ä¸€æ½­æµ‘æµŠçš„æ°´ï¼Œåˆ«æ‹…å¿ƒï¼Œå®ƒåªæ˜¯å‘Šè¯‰æˆ‘ï¼šç”Ÿæ´»å°±åƒè¿™æ°´ï¼Œæ€»æœ‰äº›æ‚è´¨ï¼Œä½†ä¹Ÿåˆ«å¿˜äº†ï¼Œè¦å‹‡æ•¢é¢å¯¹å“¦ï¼",
      "OST": 2,
      "new_timestamp": "00:51-01:37"
    }
  ]
}
output_pathå’Œscriptå‚æ•°éœ€è¦ä¼ é€’ç»™è¯·æ±‚3
è¯·æ±‚3è¿”å›å‚æ•°æ˜¯
{
  "task_id": "b6f5a98a-b2e0-4e3d-89c5-64fb90db2ec1",
  "subclip_videos": {
    "00:10-01:01": "E:\\projects\\NarratoAI\\storage\\cache_videos/vid-00_10-01_01.mp4",
    "01:07-01:53": "E:\\projects\\NarratoAI\\storage\\cache_videos/vid-01_07-01_53.mp4"
  }
}
subclip_videoså’Œ output_pathå’Œscriptå‚æ•°éœ€è¦ä¼ é€’ç»™è¯·æ±‚4
æœ€åå®Œæˆå·¥ä½œæµ

0ä»£è¡¨åªæ’­æ”¾æ–‡æ¡ˆéŸ³é¢‘ï¼Œç¦ç”¨è§†é¢‘åŸå£°ï¼›1ä»£è¡¨åªæ’­æ”¾è§†é¢‘åŸå£°ï¼Œä¸éœ€è¦æ’­æ”¾æ–‡æ¡ˆéŸ³é¢‘å’Œå­—å¹•ï¼›2ä»£è¡¨å³æ’­æ”¾æ–‡æ¡ˆéŸ³é¢‘ä¹Ÿè¦æ’­æ”¾è§†é¢‘åŸå£°ï¼›
</file>

</files>
